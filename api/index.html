<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://github.com/StatMixedML/LightGBMLSS/api/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>API references - LightGBMLSS</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "API references";
        var mkdocs_page_input_path = "api.md";
        var mkdocs_page_url = "/StatMixedML/LightGBMLSS/api/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> LightGBMLSS
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../dgbm/">Distributional Modelling</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../distributions/">Available Distributions</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Examples</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../examples/Gaussian_Regression/">Basic Walkthrough - Gaussian Regression</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../examples/Expectile_Regression/">Expectile Regression</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../examples/Gamma_Regression_CaliforniaHousing/">Gamma Regression (California Housing Data)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../examples/GaussianMixture_Regression_CaliforniaHousing/">Gausssian-Mixture Regression (California Housing Data)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../examples/How_To_Select_A_Univariate_Distribution/">How to Select a Univariate Distribution</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../examples/SplineFlow_Regression/">Spline Flow Regression</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../examples/ZAGamma_Regression/">Zero-Adjusted Gamma Regression</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">API Docs</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">API references</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#lightgbmlss">lightgbmlss</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#lightgbmlss.datasets">datasets</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.datasets.data_loader">data_loader</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.datasets.data_loader.load_simulated_gaussian_data">load_simulated_gaussian_data</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.datasets.data_loader.load_simulated_studentT_data">load_simulated_studentT_data</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#lightgbmlss.distributions">distributions</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.Beta">Beta</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Beta.Beta">Beta</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Beta.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Beta.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Beta.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Beta.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Beta.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Beta.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Beta.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Beta.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Beta.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Beta.softplus_fn_df">softplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.Cauchy">Cauchy</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Cauchy.Cauchy">Cauchy</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Cauchy.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Cauchy.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Cauchy.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Cauchy.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Cauchy.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Cauchy.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Cauchy.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Cauchy.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Cauchy.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Cauchy.softplus_fn_df">softplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.Expectile">Expectile</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Expectile.Expectile">Expectile</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Expectile.Expectile_Torch">Expectile_Torch</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Expectile.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Expectile.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Expectile.expectile_norm">expectile_norm</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Expectile.expectile_pnorm">expectile_pnorm</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Expectile.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Expectile.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Expectile.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Expectile.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Expectile.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Expectile.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Expectile.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Expectile.softplus_fn_df">softplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.Gamma">Gamma</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gamma.Gamma">Gamma</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gamma.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gamma.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gamma.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gamma.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gamma.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gamma.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gamma.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gamma.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gamma.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gamma.softplus_fn_df">softplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.Gaussian">Gaussian</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gaussian.Gaussian">Gaussian</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gaussian.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gaussian.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gaussian.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gaussian.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gaussian.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gaussian.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gaussian.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gaussian.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gaussian.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gaussian.softplus_fn_df">softplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.Gumbel">Gumbel</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gumbel.Gumbel">Gumbel</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gumbel.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gumbel.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gumbel.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gumbel.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gumbel.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gumbel.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gumbel.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gumbel.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gumbel.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Gumbel.softplus_fn_df">softplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.Laplace">Laplace</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Laplace.Laplace">Laplace</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Laplace.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Laplace.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Laplace.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Laplace.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Laplace.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Laplace.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Laplace.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Laplace.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Laplace.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Laplace.softplus_fn_df">softplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.LogNormal">LogNormal</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.LogNormal.LogNormal">LogNormal</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.LogNormal.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.LogNormal.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.LogNormal.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.LogNormal.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.LogNormal.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.LogNormal.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.LogNormal.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.LogNormal.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.LogNormal.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.LogNormal.softplus_fn_df">softplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.Logistic">Logistic</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Logistic.Logistic">Logistic</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Logistic.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Logistic.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Logistic.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Logistic.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Logistic.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Logistic.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Logistic.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Logistic.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Logistic.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Logistic.softplus_fn_df">softplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.Mixture">Mixture</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Mixture.Mixture">Mixture</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Mixture.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Mixture.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Mixture.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Mixture.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Mixture.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Mixture.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Mixture.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Mixture.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Mixture.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Mixture.softplus_fn_df">softplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.NegativeBinomial">NegativeBinomial</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.NegativeBinomial.NegativeBinomial">NegativeBinomial</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.NegativeBinomial.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.NegativeBinomial.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.NegativeBinomial.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.NegativeBinomial.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.NegativeBinomial.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.NegativeBinomial.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.NegativeBinomial.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.NegativeBinomial.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.NegativeBinomial.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.NegativeBinomial.softplus_fn_df">softplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.Poisson">Poisson</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Poisson.Poisson">Poisson</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Poisson.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Poisson.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Poisson.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Poisson.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Poisson.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Poisson.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Poisson.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Poisson.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Poisson.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Poisson.softplus_fn_df">softplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.SplineFlow">SplineFlow</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.SplineFlow.SplineFlow">SplineFlow</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.StudentT">StudentT</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.StudentT.StudentT">StudentT</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.StudentT.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.StudentT.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.StudentT.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.StudentT.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.StudentT.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.StudentT.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.StudentT.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.StudentT.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.StudentT.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.StudentT.softplus_fn_df">softplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.Weibull">Weibull</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Weibull.Weibull">Weibull</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Weibull.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Weibull.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Weibull.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Weibull.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Weibull.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Weibull.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Weibull.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Weibull.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Weibull.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.Weibull.softplus_fn_df">softplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.ZABeta">ZABeta</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZABeta.ZABeta">ZABeta</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZABeta.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZABeta.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZABeta.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZABeta.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZABeta.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZABeta.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZABeta.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZABeta.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZABeta.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZABeta.softplus_fn_df">softplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.ZAGamma">ZAGamma</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZAGamma.ZAGamma">ZAGamma</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZAGamma.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZAGamma.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZAGamma.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZAGamma.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZAGamma.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZAGamma.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZAGamma.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZAGamma.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZAGamma.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZAGamma.softplus_fn_df">softplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.ZALN">ZALN</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZALN.ZALN">ZALN</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZALN.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZALN.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZALN.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZALN.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZALN.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZALN.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZALN.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZALN.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZALN.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZALN.softplus_fn_df">softplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.ZINB">ZINB</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZINB.ZINB">ZINB</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZINB.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZINB.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZINB.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZINB.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZINB.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZINB.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZINB.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZINB.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZINB.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZINB.softplus_fn_df">softplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.ZIPoisson">ZIPoisson</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZIPoisson.ZIPoisson">ZIPoisson</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZIPoisson.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZIPoisson.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZIPoisson.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZIPoisson.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZIPoisson.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZIPoisson.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZIPoisson.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZIPoisson.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZIPoisson.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.ZIPoisson.softplus_fn_df">softplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.distribution_utils">distribution_utils</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.distribution_utils.DistributionClass">DistributionClass</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.flow_utils">flow_utils</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.flow_utils.NormalizingFlowClass">NormalizingFlowClass</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.mixture_distribution_utils">mixture_distribution_utils</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass">MixtureDistributionClass</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.mixture_distribution_utils.get_component_distributions">get_component_distributions</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.distributions.zero_inflated">zero_inflated</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.zero_inflated.ZeroAdjustedBeta">ZeroAdjustedBeta</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.zero_inflated.ZeroAdjustedGamma">ZeroAdjustedGamma</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.zero_inflated.ZeroAdjustedLogNormal">ZeroAdjustedLogNormal</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.zero_inflated.ZeroInflatedDistribution">ZeroInflatedDistribution</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.zero_inflated.ZeroInflatedNegativeBinomial">ZeroInflatedNegativeBinomial</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.distributions.zero_inflated.ZeroInflatedPoisson">ZeroInflatedPoisson</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#lightgbmlss.model">model</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.model.LightGBMLSS">LightGBMLSS</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.LightGBMLSS--parameters">Parameters</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.LightGBMLSS.cv">cv</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.LightGBMLSS.expectile_plot">expectile_plot</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.LightGBMLSS.hyper_opt">hyper_opt</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.LightGBMLSS.load_model">load_model</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.LightGBMLSS.plot">plot</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.LightGBMLSS.predict">predict</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.LightGBMLSS.save_model">save_model</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.LightGBMLSS.set_init_score">set_init_score</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.LightGBMLSS.set_params">set_params</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.LightGBMLSS.set_valid_margin">set_valid_margin</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.LightGBMLSS.train">train</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.model.exp_fn">exp_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.exp_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.exp_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.model.exp_fn_df">exp_fn_df</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.exp_fn_df--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.exp_fn_df--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.model.gumbel_softmax_fn">gumbel_softmax_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.gumbel_softmax_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.gumbel_softmax_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.model.identity_fn">identity_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.identity_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.identity_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.model.nan_to_num">nan_to_num</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.nan_to_num--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.nan_to_num--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.model.relu_fn">relu_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.relu_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.relu_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.model.sigmoid_fn">sigmoid_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.sigmoid_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.sigmoid_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.model.softmax_fn">softmax_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.softmax_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.softmax_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.model.softplus_fn">softplus_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.softplus_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.softplus_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.model.softplus_fn_df">softplus_fn_df</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.softplus_fn_df--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.model.softplus_fn_df--returns">Returns</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#lightgbmlss.utils">utils</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.utils.exp_fn">exp_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.utils.exp_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.utils.exp_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.utils.exp_fn_df">exp_fn_df</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.utils.exp_fn_df--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.utils.exp_fn_df--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.utils.gumbel_softmax_fn">gumbel_softmax_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.utils.gumbel_softmax_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.utils.gumbel_softmax_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.utils.identity_fn">identity_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.utils.identity_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.utils.identity_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.utils.nan_to_num">nan_to_num</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.utils.nan_to_num--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.utils.nan_to_num--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.utils.relu_fn">relu_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.utils.relu_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.utils.relu_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.utils.sigmoid_fn">sigmoid_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.utils.sigmoid_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.utils.sigmoid_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.utils.softmax_fn">softmax_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.utils.softmax_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.utils.softmax_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.utils.softplus_fn">softplus_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.utils.softplus_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.utils.softplus_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lightgbmlss.utils.softplus_fn_df">softplus_fn_df</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.utils.softplus_fn_df--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#lightgbmlss.utils.softplus_fn_df--returns">Returns</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">LightGBMLSS</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">API Docs</li>
      <li class="breadcrumb-item active">API references</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/StatMixedML/LightGBMLSS/edit/master/docs/api.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="api-references">API references</h1>


<div class="doc doc-object doc-module">



<a id="lightgbmlss"></a>
    <div class="doc doc-contents first">

        <p>LightGBMLSS - An extension of LightGBM to probabilistic forecasting</p>










<div class="doc doc-children">











<div class="doc doc-object doc-module">



<h2 id="lightgbmlss.datasets" class="doc doc-heading">
            <code>datasets</code>


</h2>

    <div class="doc doc-contents ">

        <p>LightGBMLSS - An extension of LightGBM to probabilistic forecasting</p>










<div class="doc doc-children">











<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.datasets.data_loader" class="doc doc-heading">
            <code>data_loader</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.datasets.data_loader.load_simulated_gaussian_data" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">load_simulated_gaussian_data</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Returns train/test dataframe of a simulated example.</p>


<details class="contains-the-following-columns" open>
  <summary>Contains the following columns</summary>
  <p>y              int64: response
x              int64: x-feature
X1:X10         int64: random noise features</p>
</details>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/datasets/data_loader.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_simulated_gaussian_data</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns train/test dataframe of a simulated example.</span>

<span class="sd">    Contains the following columns:</span>
<span class="sd">        y              int64: response</span>
<span class="sd">        x              int64: x-feature</span>
<span class="sd">        X1:X10         int64: random noise features</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">train_path</span> <span class="o">=</span> <span class="n">pkg_resources</span><span class="o">.</span><span class="n">resource_stream</span><span class="p">(</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;gaussian_train_sim.csv&quot;</span><span class="p">)</span>
    <span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">train_path</span><span class="p">)</span>

    <span class="n">test_path</span> <span class="o">=</span> <span class="n">pkg_resources</span><span class="o">.</span><span class="n">resource_stream</span><span class="p">(</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;gaussian_test_sim.csv&quot;</span><span class="p">)</span>
    <span class="n">test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">test_path</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.datasets.data_loader.load_simulated_studentT_data" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">load_simulated_studentT_data</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Returns train/test dataframe of a simulated example.</p>


<details class="contains-the-following-columns" open>
  <summary>Contains the following columns</summary>
  <p>y              int64: response
x              int64: x-feature
X1:X10         int64: random noise features</p>
</details>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/datasets/data_loader.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_simulated_studentT_data</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns train/test dataframe of a simulated example.</span>

<span class="sd">    Contains the following columns:</span>
<span class="sd">        y              int64: response</span>
<span class="sd">        x              int64: x-feature</span>
<span class="sd">        X1:X10         int64: random noise features</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">train_path</span> <span class="o">=</span> <span class="n">pkg_resources</span><span class="o">.</span><span class="n">resource_stream</span><span class="p">(</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;studentT_train_sim.csv&quot;</span><span class="p">)</span>
    <span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">train_path</span><span class="p">)</span>

    <span class="n">test_path</span> <span class="o">=</span> <span class="n">pkg_resources</span><span class="o">.</span><span class="n">resource_stream</span><span class="p">(</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;studentT_test_sim.csv&quot;</span><span class="p">)</span>
    <span class="n">test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">test_path</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="lightgbmlss.distributions" class="doc doc-heading">
            <code>distributions</code>


</h2>

    <div class="doc doc-contents ">

        <p>LightGBMLSS - An extension of LightGBM to probabilistic forecasting</p>










<div class="doc doc-children">











<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.Beta" class="doc doc-heading">
            <code>Beta</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.Beta.Beta" class="doc doc-heading">
            <code>Beta</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (lightgbmlss.distributions.distribution_utils.DistributionClass)" href="#lightgbmlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Beta distribution class.</p>
<h6 id="lightgbmlss.distributions.Beta.Beta--distributional-parameters">Distributional Parameters</h6>
<p>concentration1: torch.Tensor
    1st concentration parameter of the distribution (often referred to as alpha).
concentration0: torch.Tensor
    2nd concentration parameter of the distribution (often referred to as beta).</p>
<h6 id="lightgbmlss.distributions.Beta.Beta--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#beta</p>
<h6 id="lightgbmlss.distributions.Beta.Beta--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/Beta.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Beta</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Beta distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    concentration1: torch.Tensor</span>
<span class="sd">        1st concentration parameter of the distribution (often referred to as alpha).</span>
<span class="sd">    concentration0: torch.Tensor</span>
<span class="sd">        2nd concentration parameter of the distribution (often referred to as beta).</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#beta</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please choose from &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Beta_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;concentration1&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">,</span> <span class="s2">&quot;concentration0&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Beta.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Beta.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Beta.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Beta.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Beta.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Beta.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Beta.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="lightgbmlss.distributions.Beta.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="lightgbmlss.distributions.Beta.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Beta.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="lightgbmlss.distributions.Beta.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Beta.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Beta.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="lightgbmlss.distributions.Beta.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Beta.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Beta.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="lightgbmlss.distributions.Beta.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Beta.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Beta.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="lightgbmlss.distributions.Beta.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Beta.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Beta.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="lightgbmlss.distributions.Beta.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Beta.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Beta.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Beta.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Beta.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Beta.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Beta.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Beta.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.Cauchy" class="doc doc-heading">
            <code>Cauchy</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.Cauchy.Cauchy" class="doc doc-heading">
            <code>Cauchy</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (lightgbmlss.distributions.distribution_utils.DistributionClass)" href="#lightgbmlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Cauchy distribution class.</p>
<h6 id="lightgbmlss.distributions.Cauchy.Cauchy--distributional-parameters">Distributional Parameters</h6>
<p>loc: torch.Tensor
    Mode or median of the distribution.
scale: torch.Tensor
    Half width at half maximum.</p>
<h6 id="lightgbmlss.distributions.Cauchy.Cauchy--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#cauchy</p>
<h6 id="lightgbmlss.distributions.Cauchy.Cauchy--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/Cauchy.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Cauchy</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Cauchy distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    loc: torch.Tensor</span>
<span class="sd">        Mode or median of the distribution.</span>
<span class="sd">    scale: torch.Tensor</span>
<span class="sd">        Half width at half maximum.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#cauchy</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please choose from &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Cauchy_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loc&quot;</span><span class="p">:</span> <span class="n">identity_fn</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Cauchy.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Cauchy.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Cauchy.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Cauchy.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Cauchy.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Cauchy.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Cauchy.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="lightgbmlss.distributions.Cauchy.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="lightgbmlss.distributions.Cauchy.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Cauchy.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="lightgbmlss.distributions.Cauchy.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Cauchy.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Cauchy.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="lightgbmlss.distributions.Cauchy.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Cauchy.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Cauchy.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="lightgbmlss.distributions.Cauchy.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Cauchy.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Cauchy.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="lightgbmlss.distributions.Cauchy.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Cauchy.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Cauchy.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="lightgbmlss.distributions.Cauchy.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Cauchy.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Cauchy.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Cauchy.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Cauchy.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Cauchy.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Cauchy.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Cauchy.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.Expectile" class="doc doc-heading">
            <code>Expectile</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.Expectile.Expectile" class="doc doc-heading">
            <code>Expectile</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (lightgbmlss.distributions.distribution_utils.DistributionClass)" href="#lightgbmlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Expectile distribution class.</p>
<h6 id="lightgbmlss.distributions.Expectile.Expectile--distributional-parameters">Distributional Parameters</h6>
<p>expectile: List
    List of specified expectiles.</p>
<h6 id="lightgbmlss.distributions.Expectile.Expectile--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
expectiles: List
    List of expectiles in increasing order.
penalize_crossing: bool
    Whether to include a penalty term to discourage crossing of expectiles.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/Expectile.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Expectile</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Expectile distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    expectile: List</span>
<span class="sd">        List of specified expectiles.</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    expectiles: List</span>
<span class="sd">        List of expectiles in increasing order.</span>
<span class="sd">    penalize_crossing: bool</span>
<span class="sd">        Whether to include a penalty term to discourage crossing of expectiles.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">expectiles</span><span class="p">:</span> <span class="n">List</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
                 <span class="n">penalize_crossing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">expectiles</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expectiles must be a list.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">([</span><span class="mi">0</span> <span class="o">&lt;</span> <span class="n">expectile</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">expectile</span> <span class="ow">in</span> <span class="n">expectiles</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expectiles must be between 0 and 1.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">penalize_crossing</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;penalize_crossing must be a boolean. Please choose from True or False.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Expectile_Torch</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">expectiles</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">expectile</span> <span class="ow">in</span> <span class="n">expectiles</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;expectile_</span><span class="si">{</span><span class="n">expectile</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">param_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">identity_fn</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                         <span class="n">tau</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">expectiles</span><span class="p">),</span>
                         <span class="n">penalize_crossing</span><span class="o">=</span><span class="n">penalize_crossing</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.Expectile.Expectile_Torch" class="doc doc-heading">
            <code>Expectile_Torch</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.distributions.Distribution">Distribution</span></code></p>



        <p>PyTorch implementation of expectiles.</p>
<h6 id="lightgbmlss.distributions.Expectile.Expectile_Torch--arguments">Arguments</h6>
<p>expectiles : List[torch.Tensor]
    List of expectiles.
penalize_crossing : bool
    Whether to include a penalty term to discourage crossing of expectiles.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/Expectile.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Expectile_Torch</span><span class="p">(</span><span class="n">Distribution</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    PyTorch implementation of expectiles.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    expectiles : List[torch.Tensor]</span>
<span class="sd">        List of expectiles.</span>
<span class="sd">    penalize_crossing : bool</span>
<span class="sd">        Whether to include a penalty term to discourage crossing of expectiles.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">expectiles</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
                 <span class="n">penalize_crossing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Expectile_Torch</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expectiles</span> <span class="o">=</span> <span class="n">expectiles</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">penalize_crossing</span> <span class="o">=</span> <span class="n">penalize_crossing</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="s2">&quot;Expectile&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tau</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the log of the probability density function evaluated at `value`.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        value : torch.Tensor</span>
<span class="sd">            Response for which log probability is to be calculated.</span>
<span class="sd">        tau : List[torch.Tensor]</span>
<span class="sd">            List of asymmetry parameters.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Log probability of `value`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">penalty</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1"># Calculate loss</span>
        <span class="n">predt_expectiles</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">expectile</span><span class="p">,</span> <span class="n">tau_value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">expectiles</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">value</span> <span class="o">-</span> <span class="n">expectile</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">tau_value</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">tau_value</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">value</span> <span class="o">-</span> <span class="n">expectile</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">predt_expectiles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">expectile</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Penalty term to discourage crossing of expectiles</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalize_crossing</span><span class="p">:</span>
            <span class="n">predt_expectiles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">predt_expectiles</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">penalty</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                <span class="p">(</span><span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">predt_expectiles</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">penalty</span><span class="p">))</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">expectiles</span><span class="p">)</span>

        <span class="k">return</span> <span class="o">-</span><span class="n">loss</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.Expectile.Expectile_Torch.log_prob" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">log_prob</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Returns the log of the probability density function evaluated at <code>value</code>.</p>
<h6 id="lightgbmlss.distributions.Expectile.Expectile_Torch.log_prob--arguments">Arguments</h6>
<p>value : torch.Tensor
    Response for which log probability is to be calculated.
tau : List[torch.Tensor]
    List of asymmetry parameters.</p>
<h6 id="lightgbmlss.distributions.Expectile.Expectile_Torch.log_prob--returns">Returns</h6>
<p>torch.Tensor
    Log probability of <code>value</code>.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/Expectile.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tau</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the log of the probability density function evaluated at `value`.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    value : torch.Tensor</span>
<span class="sd">        Response for which log probability is to be calculated.</span>
<span class="sd">    tau : List[torch.Tensor]</span>
<span class="sd">        List of asymmetry parameters.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        Log probability of `value`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">penalty</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Calculate loss</span>
    <span class="n">predt_expectiles</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">expectile</span><span class="p">,</span> <span class="n">tau_value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">expectiles</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">value</span> <span class="o">-</span> <span class="n">expectile</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">tau_value</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">tau_value</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">value</span> <span class="o">-</span> <span class="n">expectile</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">predt_expectiles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">expectile</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Penalty term to discourage crossing of expectiles</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalize_crossing</span><span class="p">:</span>
        <span class="n">predt_expectiles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">predt_expectiles</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">penalty</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
            <span class="p">(</span><span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">predt_expectiles</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">penalty</span><span class="p">))</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">expectiles</span><span class="p">)</span>

    <span class="k">return</span> <span class="o">-</span><span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Expectile.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Expectile.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Expectile.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Expectile.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Expectile.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Expectile.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Expectile.expectile_norm" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">expectile_norm</span><span class="p">(</span><span class="n">tau</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Calculates expectiles from Normal distribution for given tau values.
For more details and distributions see https://rdrr.io/cran/expectreg/man/enorm.html</p>
<p>Arguments</p>
<hr />
<p>tau : np.ndarray
    Vector of expectiles from the respective distribution.
m : np.ndarray
    Mean of the Normal distribution.
sd : np.ndarray
    Standard deviation of the Normal distribution.</p>
<p>Returns</p>
<hr />
<p>np.ndarray</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/Expectile.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">expectile_norm</span><span class="p">(</span><span class="n">tau</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
                   <span class="n">m</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                   <span class="n">sd</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates expectiles from Normal distribution for given tau values.</span>
<span class="sd">    For more details and distributions see https://rdrr.io/cran/expectreg/man/enorm.html</span>

<span class="sd">    Arguments</span>
<span class="sd">    _________</span>
<span class="sd">    tau : np.ndarray</span>
<span class="sd">        Vector of expectiles from the respective distribution.</span>
<span class="sd">    m : np.ndarray</span>
<span class="sd">        Mean of the Normal distribution.</span>
<span class="sd">    sd : np.ndarray</span>
<span class="sd">        Standard deviation of the Normal distribution.</span>

<span class="sd">    Returns</span>
<span class="sd">    _______</span>
<span class="sd">    np.ndarray</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tau</span><span class="p">[</span><span class="n">tau</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">tau</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
    <span class="n">zz</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">*</span> <span class="n">tau</span>
    <span class="n">lower</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">)</span>
    <span class="n">lower</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">lower</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="n">tau</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">upper</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">)</span>
    <span class="n">upper</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">upper</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="n">tau</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">diff</span> <span class="o">&gt;</span> <span class="mf">1e-10</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">index</span> <span class="o">&lt;</span> <span class="mi">1000</span><span class="p">):</span>
        <span class="n">root</span> <span class="o">=</span> <span class="n">expectile_pnorm</span><span class="p">(</span><span class="n">zz</span><span class="p">)</span> <span class="o">-</span> <span class="n">tau</span>
        <span class="n">root</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">root</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">lower</span><span class="p">[</span><span class="n">root</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">zz</span><span class="p">[</span><span class="n">root</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">upper</span><span class="p">[</span><span class="n">root</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">zz</span><span class="p">[</span><span class="n">root</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">zz</span> <span class="o">=</span> <span class="p">(</span><span class="n">upper</span> <span class="o">+</span> <span class="n">lower</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">root</span><span class="p">))</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">zz</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">tau</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>

    <span class="k">return</span> <span class="n">zz</span> <span class="o">*</span> <span class="n">sd</span> <span class="o">+</span> <span class="n">m</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Expectile.expectile_pnorm" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">expectile_pnorm</span><span class="p">(</span><span class="n">tau</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Normal Expectile Distribution Function.
For more details and distributions see https://rdrr.io/cran/expectreg/man/enorm.html</p>
<p>Arguments</p>
<hr />
<p>tau : np.ndarray
    Vector of expectiles from the respective distribution.
m : np.ndarray
    Mean of the Normal distribution.
sd : np.ndarray
    Standard deviation of the Normal distribution.</p>
<p>Returns</p>
<hr />
<p>tau : np.ndarray
    Expectiles from the Normal distribution.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/Expectile.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">expectile_pnorm</span><span class="p">(</span><span class="n">tau</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
                    <span class="n">m</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                    <span class="n">sd</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="mi">1</span>
                    <span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Normal Expectile Distribution Function.</span>
<span class="sd">    For more details and distributions see https://rdrr.io/cran/expectreg/man/enorm.html</span>

<span class="sd">    Arguments</span>
<span class="sd">    _________</span>
<span class="sd">    tau : np.ndarray</span>
<span class="sd">        Vector of expectiles from the respective distribution.</span>
<span class="sd">    m : np.ndarray</span>
<span class="sd">        Mean of the Normal distribution.</span>
<span class="sd">    sd : np.ndarray</span>
<span class="sd">        Standard deviation of the Normal distribution.</span>

<span class="sd">    Returns</span>
<span class="sd">    _______</span>
<span class="sd">    tau : np.ndarray</span>
<span class="sd">        Expectiles from the Normal distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">z</span> <span class="o">=</span> <span class="p">(</span><span class="n">tau</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span> <span class="o">/</span> <span class="n">sd</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">m</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sd</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">m</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sd</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="o">-</span><span class="n">d</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">p</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">u</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">u</span> <span class="o">+</span> <span class="n">z</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tau</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Expectile.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="lightgbmlss.distributions.Expectile.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="lightgbmlss.distributions.Expectile.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Expectile.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="lightgbmlss.distributions.Expectile.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Expectile.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Expectile.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="lightgbmlss.distributions.Expectile.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Expectile.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Expectile.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="lightgbmlss.distributions.Expectile.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Expectile.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Expectile.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="lightgbmlss.distributions.Expectile.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Expectile.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Expectile.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="lightgbmlss.distributions.Expectile.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Expectile.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Expectile.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Expectile.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Expectile.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Expectile.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Expectile.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Expectile.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.Gamma" class="doc doc-heading">
            <code>Gamma</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.Gamma.Gamma" class="doc doc-heading">
            <code>Gamma</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (lightgbmlss.distributions.distribution_utils.DistributionClass)" href="#lightgbmlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Gamma distribution class.</p>
<h6 id="lightgbmlss.distributions.Gamma.Gamma--distributional-parameters">Distributional Parameters</h6>
<p>concentration: torch.Tensor
    shape parameter of the distribution (often referred to as alpha)
rate: torch.Tensor
    rate = 1 / scale of the distribution (often referred to as beta)</p>
<h6 id="lightgbmlss.distributions.Gamma.Gamma--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#gamma</p>
<h6 id="lightgbmlss.distributions.Gamma.Gamma--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/Gamma.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Gamma</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gamma distribution class.</span>

<span class="sd">     Distributional Parameters</span>
<span class="sd">    --------------------------</span>
<span class="sd">    concentration: torch.Tensor</span>
<span class="sd">        shape parameter of the distribution (often referred to as alpha)</span>
<span class="sd">    rate: torch.Tensor</span>
<span class="sd">        rate = 1 / scale of the distribution (often referred to as beta)</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#gamma</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please choose from &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Gamma_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;concentration&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">,</span> <span class="s2">&quot;rate&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gamma.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Gamma.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gamma.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gamma.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Gamma.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gamma.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gamma.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="lightgbmlss.distributions.Gamma.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="lightgbmlss.distributions.Gamma.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gamma.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="lightgbmlss.distributions.Gamma.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gamma.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gamma.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="lightgbmlss.distributions.Gamma.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gamma.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gamma.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="lightgbmlss.distributions.Gamma.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gamma.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gamma.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="lightgbmlss.distributions.Gamma.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gamma.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gamma.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="lightgbmlss.distributions.Gamma.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gamma.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gamma.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Gamma.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gamma.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gamma.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Gamma.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gamma.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.Gaussian" class="doc doc-heading">
            <code>Gaussian</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.Gaussian.Gaussian" class="doc doc-heading">
            <code>Gaussian</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (lightgbmlss.distributions.distribution_utils.DistributionClass)" href="#lightgbmlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Gaussian distribution class.</p>
<h6 id="lightgbmlss.distributions.Gaussian.Gaussian--distributional-parameters">Distributional Parameters</h6>
<p>loc: torch.Tensor
    Mean of the distribution (often referred to as mu).
scale: torch.Tensor
    Standard deviation of the distribution (often referred to as sigma).</p>
<h6 id="lightgbmlss.distributions.Gaussian.Gaussian--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#normal</p>
<h6 id="lightgbmlss.distributions.Gaussian.Gaussian--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/Gaussian.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Gaussian</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gaussian distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    loc: torch.Tensor</span>
<span class="sd">        Mean of the distribution (often referred to as mu).</span>
<span class="sd">    scale: torch.Tensor</span>
<span class="sd">        Standard deviation of the distribution (often referred to as sigma).</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#normal</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please choose from &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Gaussian_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loc&quot;</span><span class="p">:</span> <span class="n">identity_fn</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gaussian.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Gaussian.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gaussian.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gaussian.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Gaussian.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gaussian.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gaussian.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="lightgbmlss.distributions.Gaussian.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="lightgbmlss.distributions.Gaussian.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gaussian.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="lightgbmlss.distributions.Gaussian.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gaussian.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gaussian.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="lightgbmlss.distributions.Gaussian.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gaussian.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gaussian.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="lightgbmlss.distributions.Gaussian.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gaussian.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gaussian.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="lightgbmlss.distributions.Gaussian.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gaussian.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gaussian.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="lightgbmlss.distributions.Gaussian.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gaussian.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gaussian.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Gaussian.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gaussian.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gaussian.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Gaussian.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gaussian.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.Gumbel" class="doc doc-heading">
            <code>Gumbel</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.Gumbel.Gumbel" class="doc doc-heading">
            <code>Gumbel</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (lightgbmlss.distributions.distribution_utils.DistributionClass)" href="#lightgbmlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Gumbel distribution class.</p>
<h6 id="lightgbmlss.distributions.Gumbel.Gumbel--distributional-parameters">Distributional Parameters</h6>
<p>loc: torch.Tensor
    Location parameter of the distribution.
scale: torch.Tensor
    Scale parameter of the distribution.</p>
<h6 id="lightgbmlss.distributions.Gumbel.Gumbel--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#gumbel</p>
<h6 id="lightgbmlss.distributions.Gumbel.Gumbel--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/Gumbel.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Gumbel</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    loc: torch.Tensor</span>
<span class="sd">        Location parameter of the distribution.</span>
<span class="sd">    scale: torch.Tensor</span>
<span class="sd">        Scale parameter of the distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#gumbel</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please choose from &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Gumbel_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loc&quot;</span><span class="p">:</span> <span class="n">identity_fn</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gumbel.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Gumbel.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gumbel.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gumbel.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Gumbel.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gumbel.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gumbel.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="lightgbmlss.distributions.Gumbel.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="lightgbmlss.distributions.Gumbel.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gumbel.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="lightgbmlss.distributions.Gumbel.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gumbel.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gumbel.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="lightgbmlss.distributions.Gumbel.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gumbel.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gumbel.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="lightgbmlss.distributions.Gumbel.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gumbel.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gumbel.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="lightgbmlss.distributions.Gumbel.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gumbel.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gumbel.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="lightgbmlss.distributions.Gumbel.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gumbel.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gumbel.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Gumbel.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gumbel.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Gumbel.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Gumbel.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Gumbel.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.Laplace" class="doc doc-heading">
            <code>Laplace</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.Laplace.Laplace" class="doc doc-heading">
            <code>Laplace</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (lightgbmlss.distributions.distribution_utils.DistributionClass)" href="#lightgbmlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Laplace distribution class.</p>
<h6 id="lightgbmlss.distributions.Laplace.Laplace--distributional-parameters">Distributional Parameters</h6>
<p>loc: torch.Tensor
    Mean of the distribution.
scale: torch.Tensor
    Scale of the distribution.</p>
<h6 id="lightgbmlss.distributions.Laplace.Laplace--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#laplace</p>
<h6 id="lightgbmlss.distributions.Laplace.Laplace--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/Laplace.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Laplace</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Laplace distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    loc: torch.Tensor</span>
<span class="sd">        Mean of the distribution.</span>
<span class="sd">    scale: torch.Tensor</span>
<span class="sd">        Scale of the distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#laplace</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please choose from &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Laplace_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loc&quot;</span><span class="p">:</span> <span class="n">identity_fn</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Laplace.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Laplace.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Laplace.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Laplace.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Laplace.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Laplace.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Laplace.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="lightgbmlss.distributions.Laplace.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="lightgbmlss.distributions.Laplace.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Laplace.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="lightgbmlss.distributions.Laplace.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Laplace.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Laplace.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="lightgbmlss.distributions.Laplace.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Laplace.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Laplace.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="lightgbmlss.distributions.Laplace.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Laplace.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Laplace.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="lightgbmlss.distributions.Laplace.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Laplace.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Laplace.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="lightgbmlss.distributions.Laplace.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Laplace.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Laplace.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Laplace.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Laplace.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Laplace.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Laplace.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Laplace.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.LogNormal" class="doc doc-heading">
            <code>LogNormal</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.LogNormal.LogNormal" class="doc doc-heading">
            <code>LogNormal</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (lightgbmlss.distributions.distribution_utils.DistributionClass)" href="#lightgbmlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>LogNormal distribution class.</p>
<h6 id="lightgbmlss.distributions.LogNormal.LogNormal--distributional-parameters">Distributional Parameters</h6>
<p>loc: torch.Tensor
    Mean of log of distribution.
scale: torch.Tensor
    Standard deviation of log of the distribution.</p>
<h6 id="lightgbmlss.distributions.LogNormal.LogNormal--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#lognormal</p>
<h6 id="lightgbmlss.distributions.LogNormal.LogNormal--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/LogNormal.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">LogNormal</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    LogNormal distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    loc: torch.Tensor</span>
<span class="sd">        Mean of log of distribution.</span>
<span class="sd">    scale: torch.Tensor</span>
<span class="sd">        Standard deviation of log of the distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#lognormal</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please choose from &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">LogNormal_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loc&quot;</span><span class="p">:</span> <span class="n">identity_fn</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.LogNormal.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.LogNormal.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.LogNormal.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.LogNormal.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.LogNormal.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.LogNormal.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.LogNormal.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="lightgbmlss.distributions.LogNormal.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="lightgbmlss.distributions.LogNormal.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.LogNormal.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="lightgbmlss.distributions.LogNormal.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.LogNormal.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.LogNormal.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="lightgbmlss.distributions.LogNormal.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.LogNormal.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.LogNormal.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="lightgbmlss.distributions.LogNormal.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.LogNormal.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.LogNormal.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="lightgbmlss.distributions.LogNormal.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.LogNormal.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.LogNormal.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="lightgbmlss.distributions.LogNormal.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.LogNormal.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.LogNormal.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.LogNormal.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.LogNormal.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.LogNormal.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.LogNormal.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.LogNormal.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.Logistic" class="doc doc-heading">
            <code>Logistic</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.Logistic.Logistic" class="doc doc-heading">
            <code>Logistic</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (lightgbmlss.distributions.distribution_utils.DistributionClass)" href="#lightgbmlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Logistic distribution class.</p>
<h6 id="lightgbmlss.distributions.Logistic.Logistic--distributional-parameters">Distributional Parameters</h6>
<p>loc: torch.Tensor
    Location parameter.
scale: torch.Tensor
    Scale parameter.</p>
<h6 id="lightgbmlss.distributions.Logistic.Logistic--source">Source</h6>
<p>https://docs.pyro.ai/en/dev/distributions.html#logistic</p>
<h6 id="lightgbmlss.distributions.Logistic.Logistic--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/Logistic.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Logistic</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Logistic distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    loc: torch.Tensor</span>
<span class="sd">        Location parameter.</span>
<span class="sd">    scale: torch.Tensor</span>
<span class="sd">        Scale parameter.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://docs.pyro.ai/en/dev/distributions.html#logistic</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please choose from &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Logistic_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loc&quot;</span><span class="p">:</span> <span class="n">identity_fn</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Logistic.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Logistic.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Logistic.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Logistic.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Logistic.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Logistic.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Logistic.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="lightgbmlss.distributions.Logistic.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="lightgbmlss.distributions.Logistic.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Logistic.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="lightgbmlss.distributions.Logistic.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Logistic.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Logistic.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="lightgbmlss.distributions.Logistic.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Logistic.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Logistic.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="lightgbmlss.distributions.Logistic.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Logistic.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Logistic.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="lightgbmlss.distributions.Logistic.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Logistic.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Logistic.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="lightgbmlss.distributions.Logistic.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Logistic.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Logistic.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Logistic.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Logistic.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Logistic.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Logistic.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Logistic.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.Mixture" class="doc doc-heading">
            <code>Mixture</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.Mixture.Mixture" class="doc doc-heading">
            <code>Mixture</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="MixtureDistributionClass (lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass)" href="#lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass">MixtureDistributionClass</a></code></p>



        <p>Mixture-Density distribution class.</p>
<p>Implements a mixture-density distribution for univariate targets, where all components are from different
parameterizations of the same distribution-type. A mixture-density distribution is a concept used to model a
complex distribution that arises from combining multiple simpler distributions. The Mixture-Density distribution
is parameterized by a categorical selecting distribution (over M components) and M-component distributions. For more
information on the Mixture-Density distribution, see:</p>
<pre><code>Bishop, C. M. (1994). Mixture density networks. Technical Report NCRG/4288, Aston University, Birmingham, UK.
</code></pre>
<h6 id="lightgbmlss.distributions.Mixture.Mixture--distributional-parameters">Distributional Parameters</h6>
<p>Inherits the distributional parameters from the component distributions.</p>
<h6 id="lightgbmlss.distributions.Mixture.Mixture--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#mixturesamefamily</p>
<h6 id="lightgbmlss.distributions.Mixture.Mixture--parameters">Parameters</h6>
<p>component_distribution: torch.distributions.Distribution
    Distribution class for the components of the mixture distribution. Has to be one of the available
    univariate distributions of the package.
M: int
    Number of components in the mixture distribution.
hessian_mode: str
    Mode for computing the Hessian. Must be one of the following:</p>
<pre><code>    - "individual": Each parameter is treated as a separate tensor. As a result, the Hessian corresponds to the
    second-order derivative with respect to that specific parameter only. The resulting Hessians capture the
    curvature of the loss w.r.t. each individual parameter. This is usually more runtime intensive, but can
    be more accurate.

    - "grouped": Each tensor contains all parameters for a specific parameter-type, e.g., for a Gaussian-Mixture
    with M=2, loc=[loc_1, loc_2], scale=[scale_1, scale_2], and mix_prob=[mix_prob_1, mix_prob_2]. When
    computing the Hessian, the derivatives for all parameters in the respective tensor are calculated jointly.
    The resulting Hessians capture the curvature of the loss w.r.t. the entire parameter-type. This is usually
    less runtime intensive, but can be less accurate.
</code></pre>
<p>tau: float, non-negative scalar temperature.
    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
    defined as:</p>
<pre><code>    s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}

where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to

    Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>


<details class="initialize" open>
  <summary>bool</summary>
  <p>Whether to initialize the distributional parameters with unconditional start values. Initialization can help
to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
solutions if the unconditional start values are far from the optimal values.</p>
</details>






              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/Mixture.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Mixture</span><span class="p">(</span><span class="n">MixtureDistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Mixture-Density distribution class.</span>

<span class="sd">    Implements a mixture-density distribution for univariate targets, where all components are from different</span>
<span class="sd">    parameterizations of the same distribution-type. A mixture-density distribution is a concept used to model a</span>
<span class="sd">    complex distribution that arises from combining multiple simpler distributions. The Mixture-Density distribution</span>
<span class="sd">    is parameterized by a categorical selecting distribution (over M components) and M-component distributions. For more</span>
<span class="sd">    information on the Mixture-Density distribution, see:</span>

<span class="sd">        Bishop, C. M. (1994). Mixture density networks. Technical Report NCRG/4288, Aston University, Birmingham, UK.</span>


<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    Inherits the distributional parameters from the component distributions.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#mixturesamefamily</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    component_distribution: torch.distributions.Distribution</span>
<span class="sd">        Distribution class for the components of the mixture distribution. Has to be one of the available</span>
<span class="sd">        univariate distributions of the package.</span>
<span class="sd">    M: int</span>
<span class="sd">        Number of components in the mixture distribution.</span>
<span class="sd">    hessian_mode: str</span>
<span class="sd">        Mode for computing the Hessian. Must be one of the following:</span>

<span class="sd">            - &quot;individual&quot;: Each parameter is treated as a separate tensor. As a result, the Hessian corresponds to the</span>
<span class="sd">            second-order derivative with respect to that specific parameter only. The resulting Hessians capture the</span>
<span class="sd">            curvature of the loss w.r.t. each individual parameter. This is usually more runtime intensive, but can</span>
<span class="sd">            be more accurate.</span>

<span class="sd">            - &quot;grouped&quot;: Each tensor contains all parameters for a specific parameter-type, e.g., for a Gaussian-Mixture</span>
<span class="sd">            with M=2, loc=[loc_1, loc_2], scale=[scale_1, scale_2], and mix_prob=[mix_prob_1, mix_prob_2]. When</span>
<span class="sd">            computing the Hessian, the derivatives for all parameters in the respective tensor are calculated jointly.</span>
<span class="sd">            The resulting Hessians capture the curvature of the loss w.r.t. the entire parameter-type. This is usually</span>
<span class="sd">            less runtime intensive, but can be less accurate.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">        version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">        differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">        categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">        Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">        Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">        defined as:</span>

<span class="sd">            s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">        where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">        of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">        approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">            Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">component_distribution</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="p">,</span>
                 <span class="n">M</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                 <span class="n">hessian_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;individual&quot;</span><span class="p">,</span>
                 <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="n">mixt_dist</span> <span class="o">=</span> <span class="n">get_component_distributions</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">str</span><span class="p">(</span><span class="n">component_distribution</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mixt_dist</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;component_distribution must be one of the following: </span><span class="si">{</span><span class="n">mixt_dist</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;M must be an integer.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">M</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;M must be greater than 1.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">component_distribution</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">!=</span> <span class="s2">&quot;nll&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Loss for component_distribution must be &#39;nll&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">hessian_mode</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;hessian_mode must be a string.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">hessian_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;individual&quot;</span><span class="p">,</span> <span class="s2">&quot;grouped&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;hessian_mode must be either &#39;individual&#39; or &#39;grouped&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;tau must be a float.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tau</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;tau must be greater than 0.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="n">component_distribution</span><span class="o">.</span><span class="n">param_dict</span>
        <span class="n">preset_gumbel_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">gumbel_softmax_fn</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">)</span>
        <span class="n">param_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;mix_prob&quot;</span><span class="p">:</span> <span class="n">preset_gumbel_fn</span><span class="p">})</span>
        <span class="n">distribution_arg_names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">param_dict</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">M</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">component_distribution</span><span class="p">,</span>
                         <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">,</span>
                         <span class="n">temperature</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span>
                         <span class="n">hessian_mode</span><span class="o">=</span><span class="n">hessian_mode</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="n">component_distribution</span><span class="o">.</span><span class="n">discrete</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">distribution_arg_names</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">component_distribution</span><span class="o">.</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="n">distribution_arg_names</span><span class="p">,</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">component_distribution</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Mixture.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Mixture.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Mixture.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Mixture.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Mixture.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Mixture.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Mixture.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="lightgbmlss.distributions.Mixture.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="lightgbmlss.distributions.Mixture.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Mixture.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="lightgbmlss.distributions.Mixture.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Mixture.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Mixture.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="lightgbmlss.distributions.Mixture.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Mixture.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Mixture.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="lightgbmlss.distributions.Mixture.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Mixture.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Mixture.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="lightgbmlss.distributions.Mixture.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Mixture.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Mixture.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="lightgbmlss.distributions.Mixture.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Mixture.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Mixture.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Mixture.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Mixture.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Mixture.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Mixture.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Mixture.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.NegativeBinomial" class="doc doc-heading">
            <code>NegativeBinomial</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.NegativeBinomial.NegativeBinomial" class="doc doc-heading">
            <code>NegativeBinomial</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (lightgbmlss.distributions.distribution_utils.DistributionClass)" href="#lightgbmlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>NegativeBinomial distribution class.</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.NegativeBinomial--distributional-parameters">Distributional Parameters</h6>
<p>total_count: torch.Tensor
    Non-negative number of negative Bernoulli trials to stop.
probs: torch.Tensor
    Event probabilities of success in the half open interval [0, 1).
logits: torch.Tensor
    Event log-odds for probabilities of success.</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.NegativeBinomial--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#negativebinomial</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.NegativeBinomial--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn_total_count: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential), "softplus" (softplus) or "relu" (rectified linear unit).
response_fn_probs: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "sigmoid" (sigmoid).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood).
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/NegativeBinomial.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">NegativeBinomial</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    NegativeBinomial distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    total_count: torch.Tensor</span>
<span class="sd">        Non-negative number of negative Bernoulli trials to stop.</span>
<span class="sd">    probs: torch.Tensor</span>
<span class="sd">        Event probabilities of success in the half open interval [0, 1).</span>
<span class="sd">    logits: torch.Tensor</span>
<span class="sd">        Event log-odds for probabilities of success.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#negativebinomial</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn_total_count: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential), &quot;softplus&quot; (softplus) or &quot;relu&quot; (rectified linear unit).</span>
<span class="sd">    response_fn_probs: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;sigmoid&quot; (sigmoid).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood).</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn_total_count</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
                 <span class="n">response_fn_probs</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sigmoid&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1">#  Specify Response Functions for total_count</span>
        <span class="n">response_functions_total_count</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">relu_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn_total_count</span> <span class="ow">in</span> <span class="n">response_functions_total_count</span><span class="p">:</span>
            <span class="n">response_fn_total_count</span> <span class="o">=</span> <span class="n">response_functions_total_count</span><span class="p">[</span><span class="n">response_fn_total_count</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function for total_count. Please choose from &#39;exp&#39;, &#39;softplus&#39; or &#39;relu&#39;.&quot;</span><span class="p">)</span>

        <span class="c1">#  Specify Response Functions for probs</span>
        <span class="n">response_functions_probs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">:</span> <span class="n">sigmoid_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn_probs</span> <span class="ow">in</span> <span class="n">response_functions_probs</span><span class="p">:</span>
            <span class="n">response_fn_probs</span> <span class="o">=</span> <span class="n">response_functions_probs</span><span class="p">[</span><span class="n">response_fn_probs</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function for probs. Please select &#39;sigmoid&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">NegativeBinomial_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;total_count&quot;</span><span class="p">:</span> <span class="n">response_fn_total_count</span><span class="p">,</span> <span class="s2">&quot;probs&quot;</span><span class="p">:</span> <span class="n">response_fn_probs</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.NegativeBinomial.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.NegativeBinomial.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.NegativeBinomial.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="lightgbmlss.distributions.NegativeBinomial.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.NegativeBinomial.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.NegativeBinomial.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.NegativeBinomial.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.NegativeBinomial.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.NegativeBinomial.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.NegativeBinomial.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.NegativeBinomial.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.NegativeBinomial.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.Poisson" class="doc doc-heading">
            <code>Poisson</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.Poisson.Poisson" class="doc doc-heading">
            <code>Poisson</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (lightgbmlss.distributions.distribution_utils.DistributionClass)" href="#lightgbmlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Poisson distribution class.</p>
<h6 id="lightgbmlss.distributions.Poisson.Poisson--distributional-parameters">Distributional Parameters</h6>
<p>rate: torch.Tensor
    Rate parameter of the distribution (often referred to as lambda).</p>
<h6 id="lightgbmlss.distributions.Poisson.Poisson--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#poisson</p>
<h6 id="lightgbmlss.distributions.Poisson.Poisson--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential), "softplus" (softplus) or "relu" (rectified linear unit).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood).
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/Poisson.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Poisson</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Poisson distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    rate: torch.Tensor</span>
<span class="sd">        Rate parameter of the distribution (often referred to as lambda).</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#poisson</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential), &quot;softplus&quot; (softplus) or &quot;relu&quot; (rectified linear unit).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood).</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">relu_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function for total_count. Please choose from &#39;exp&#39;, &#39;softplus&#39; or &#39;relu&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Poisson_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;rate&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Poisson.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Poisson.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Poisson.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Poisson.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Poisson.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Poisson.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Poisson.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="lightgbmlss.distributions.Poisson.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="lightgbmlss.distributions.Poisson.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Poisson.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="lightgbmlss.distributions.Poisson.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Poisson.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Poisson.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="lightgbmlss.distributions.Poisson.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Poisson.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Poisson.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="lightgbmlss.distributions.Poisson.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Poisson.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Poisson.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="lightgbmlss.distributions.Poisson.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Poisson.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Poisson.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="lightgbmlss.distributions.Poisson.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Poisson.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Poisson.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Poisson.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Poisson.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Poisson.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Poisson.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Poisson.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.SplineFlow" class="doc doc-heading">
            <code>SplineFlow</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.SplineFlow.SplineFlow" class="doc doc-heading">
            <code>SplineFlow</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="NormalizingFlowClass (lightgbmlss.distributions.flow_utils.NormalizingFlowClass)" href="#lightgbmlss.distributions.flow_utils.NormalizingFlowClass">NormalizingFlowClass</a></code></p>



        <p>Spline Flow class.</p>
<p>The spline flow is a normalizing flow based on element-wise rational spline bijections of linear and quadratic
order (Durkan et al., 2019; Dolatabadi et al., 2020). Rational splines are functions that are comprised of segments
that are the ratio of two polynomials. Rational splines offer an excellent combination of functional flexibility
whilst maintaining a numerically stable inverse.</p>
<p>For more details, see:
- Durkan, C., Bekasov, A., Murray, I. and Papamakarios, G. Neural Spline Flows. NeurIPS 2019.
- Dolatabadi, H. M., Erfani, S. and Leckie, C., Invertible Generative Modeling using Linear Rational Splines. AISTATS 2020.</p>
<h6 id="lightgbmlss.distributions.SplineFlow.SplineFlow--source">Source</h6>
<p>https://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.transforms.Spline</p>
<h6 id="lightgbmlss.distributions.SplineFlow.SplineFlow--arguments">Arguments</h6>
<p>target_support: str
    The target support. Options are
        - "real": [-inf, inf]
        - "positive": [0, inf]
        - "positive_integer": [0, 1, 2, 3, ...]
        - "unit_interval": [0, 1]
count_bins: int
    The number of segments comprising the spline.
bound: float
    The quantity "K" determining the bounding box, [-K,K] x [-K,K] of the spline. By adjusting the
    "K" value, you can control the size of the bounding box and consequently control the range of inputs that
    the spline transform operates on. Larger values of "K" will result in a wider valid range for the spline
    transformation, while smaller values will restrict the valid range to a smaller region. Should be chosen
    based on the range of the data.
order: str
    The order of the spline. Options are "linear" or "quadratic".
stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD" or "L2".
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/SplineFlow.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">SplineFlow</span><span class="p">(</span><span class="n">NormalizingFlowClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Spline Flow class.</span>

<span class="sd">    The spline flow is a normalizing flow based on element-wise rational spline bijections of linear and quadratic</span>
<span class="sd">    order (Durkan et al., 2019; Dolatabadi et al., 2020). Rational splines are functions that are comprised of segments</span>
<span class="sd">    that are the ratio of two polynomials. Rational splines offer an excellent combination of functional flexibility</span>
<span class="sd">    whilst maintaining a numerically stable inverse.</span>

<span class="sd">    For more details, see:</span>
<span class="sd">    - Durkan, C., Bekasov, A., Murray, I. and Papamakarios, G. Neural Spline Flows. NeurIPS 2019.</span>
<span class="sd">    - Dolatabadi, H. M., Erfani, S. and Leckie, C., Invertible Generative Modeling using Linear Rational Splines. AISTATS 2020.</span>


<span class="sd">    Source</span>
<span class="sd">    ---------</span>
<span class="sd">    https://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.transforms.Spline</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    target_support: str</span>
<span class="sd">        The target support. Options are</span>
<span class="sd">            - &quot;real&quot;: [-inf, inf]</span>
<span class="sd">            - &quot;positive&quot;: [0, inf]</span>
<span class="sd">            - &quot;positive_integer&quot;: [0, 1, 2, 3, ...]</span>
<span class="sd">            - &quot;unit_interval&quot;: [0, 1]</span>
<span class="sd">    count_bins: int</span>
<span class="sd">        The number of segments comprising the spline.</span>
<span class="sd">    bound: float</span>
<span class="sd">        The quantity &quot;K&quot; determining the bounding box, [-K,K] x [-K,K] of the spline. By adjusting the</span>
<span class="sd">        &quot;K&quot; value, you can control the size of the bounding box and consequently control the range of inputs that</span>
<span class="sd">        the spline transform operates on. Larger values of &quot;K&quot; will result in a wider valid range for the spline</span>
<span class="sd">        transformation, while smaller values will restrict the valid range to a smaller region. Should be chosen</span>
<span class="sd">        based on the range of the data.</span>
<span class="sd">    order: str</span>
<span class="sd">        The order of the spline. Options are &quot;linear&quot; or &quot;quadratic&quot;.</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot; or &quot;L2&quot;.</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">target_support</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;real&quot;</span><span class="p">,</span>
                 <span class="n">count_bins</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
                 <span class="n">bound</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">3.0</span><span class="p">,</span>
                 <span class="n">order</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;linear&quot;</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Specify Target Transform</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target_support</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;target_support must be a string.&quot;</span><span class="p">)</span>

        <span class="n">transforms</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;real&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">identity_transform</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
            <span class="s2">&quot;positive&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">SoftplusTransform</span><span class="p">(),</span> <span class="kc">False</span><span class="p">),</span>
            <span class="s2">&quot;positive_integer&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">SoftplusTransform</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
            <span class="s2">&quot;unit_interval&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">SigmoidTransform</span><span class="p">(),</span> <span class="kc">False</span><span class="p">)</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="n">target_support</span> <span class="ow">in</span> <span class="n">transforms</span><span class="p">:</span>
            <span class="n">target_transform</span><span class="p">,</span> <span class="n">discrete</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">[</span><span class="n">target_support</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid target_support. Options are &#39;real&#39;, &#39;positive&#39;, &#39;positive_integer&#39;, or &#39;unit_interval&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Check if count_bins is valid</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">count_bins</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;count_bins must be an integer.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">count_bins</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;count_bins must be a positive integer &gt; 0.&quot;</span><span class="p">)</span>

        <span class="c1"># Check if bound is float</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bound</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;bound must be a float.&quot;</span><span class="p">)</span>

        <span class="c1"># Number of parameters</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">order</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;order must be a string.&quot;</span><span class="p">)</span>

        <span class="n">order_params</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;quadratic&quot;</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">count_bins</span> <span class="o">+</span> <span class="p">(</span><span class="n">count_bins</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
            <span class="s2">&quot;linear&quot;</span><span class="p">:</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">count_bins</span> <span class="o">+</span> <span class="p">(</span><span class="n">count_bins</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="n">order</span> <span class="ow">in</span> <span class="n">order_params</span><span class="p">:</span>
            <span class="n">n_params</span> <span class="o">=</span> <span class="n">order_params</span><span class="p">[</span><span class="n">order</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid order specification. Options are &#39;linear&#39; or &#39;quadratic&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Check if stabilization method is valid.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stabilization</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;stabilization must be a string.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Options are &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Check if loss function is valid.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;loss_fn must be a string.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss_fn. Options are &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Check if initialize is valid.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify parameter dictionary</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="sa">f</span><span class="s2">&quot;param_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">identity_fn</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_params</span><span class="p">)}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Normalizing Flow Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">base_dist</span><span class="o">=</span><span class="n">Normal</span><span class="p">,</span>                     <span class="c1"># Base distribution, currently only Normal is supported.</span>
                         <span class="n">flow_transform</span><span class="o">=</span><span class="n">Spline</span><span class="p">,</span>
                         <span class="n">count_bins</span><span class="o">=</span><span class="n">count_bins</span><span class="p">,</span>
                         <span class="n">bound</span><span class="o">=</span><span class="n">bound</span><span class="p">,</span>
                         <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="n">n_params</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">target_transform</span><span class="o">=</span><span class="n">target_transform</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="n">discrete</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.StudentT" class="doc doc-heading">
            <code>StudentT</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.StudentT.StudentT" class="doc doc-heading">
            <code>StudentT</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (lightgbmlss.distributions.distribution_utils.DistributionClass)" href="#lightgbmlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Student-T Distribution Class</p>
<h6 id="lightgbmlss.distributions.StudentT.StudentT--distributional-parameters">Distributional Parameters</h6>
<p>df: torch.Tensor
    Degrees of freedom.
loc: torch.Tensor
    Mean of the distribution.
scale: torch.Tensor
    Scale of the distribution.</p>
<h6 id="lightgbmlss.distributions.StudentT.StudentT--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#studentt</p>
<h6 id="lightgbmlss.distributions.StudentT.StudentT--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/StudentT.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">StudentT</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Student-T Distribution Class</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    df: torch.Tensor</span>
<span class="sd">        Degrees of freedom.</span>
<span class="sd">    loc: torch.Tensor</span>
<span class="sd">        Mean of the distribution.</span>
<span class="sd">    scale: torch.Tensor</span>
<span class="sd">        Scale of the distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#studentt</span>

<span class="sd">     Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please choose from &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">exp_fn</span><span class="p">,</span> <span class="n">exp_fn_df</span><span class="p">),</span>
            <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">softplus_fn</span><span class="p">,</span> <span class="n">softplus_fn_df</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span><span class="p">,</span> <span class="n">response_fn_df</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">StudentT_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;df&quot;</span><span class="p">:</span> <span class="n">response_fn_df</span><span class="p">,</span> <span class="s2">&quot;loc&quot;</span><span class="p">:</span> <span class="n">identity_fn</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.StudentT.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.StudentT.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.StudentT.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.StudentT.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.StudentT.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.StudentT.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.StudentT.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="lightgbmlss.distributions.StudentT.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="lightgbmlss.distributions.StudentT.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.StudentT.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="lightgbmlss.distributions.StudentT.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.StudentT.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.StudentT.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="lightgbmlss.distributions.StudentT.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.StudentT.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.StudentT.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="lightgbmlss.distributions.StudentT.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.StudentT.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.StudentT.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="lightgbmlss.distributions.StudentT.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.StudentT.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.StudentT.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="lightgbmlss.distributions.StudentT.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.StudentT.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.StudentT.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.StudentT.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.StudentT.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.StudentT.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.StudentT.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.StudentT.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.Weibull" class="doc doc-heading">
            <code>Weibull</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.Weibull.Weibull" class="doc doc-heading">
            <code>Weibull</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (lightgbmlss.distributions.distribution_utils.DistributionClass)" href="#lightgbmlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Weibull distribution class.</p>
<h6 id="lightgbmlss.distributions.Weibull.Weibull--distributional-parameters">Distributional Parameters</h6>
<p>scale: torch.Tensor
    Scale parameter of distribution (lambda).
concentration: torch.Tensor
    Concentration parameter of distribution (k/shape).</p>
<h6 id="lightgbmlss.distributions.Weibull.Weibull--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#weibull</p>
<h6 id="lightgbmlss.distributions.Weibull.Weibull--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/Weibull.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Weibull</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Weibull distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    scale: torch.Tensor</span>
<span class="sd">        Scale parameter of distribution (lambda).</span>
<span class="sd">    concentration: torch.Tensor</span>
<span class="sd">        Concentration parameter of distribution (k/shape).</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#weibull</span>

<span class="sd">     Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please choose from &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Weibull_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">,</span> <span class="s2">&quot;concentration&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Weibull.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Weibull.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Weibull.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Weibull.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Weibull.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Weibull.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Weibull.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="lightgbmlss.distributions.Weibull.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="lightgbmlss.distributions.Weibull.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Weibull.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="lightgbmlss.distributions.Weibull.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Weibull.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Weibull.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="lightgbmlss.distributions.Weibull.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Weibull.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Weibull.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="lightgbmlss.distributions.Weibull.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Weibull.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Weibull.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="lightgbmlss.distributions.Weibull.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Weibull.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Weibull.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="lightgbmlss.distributions.Weibull.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Weibull.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Weibull.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.Weibull.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Weibull.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.Weibull.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.Weibull.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.Weibull.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.ZABeta" class="doc doc-heading">
            <code>ZABeta</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.ZABeta.ZABeta" class="doc doc-heading">
            <code>ZABeta</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (lightgbmlss.distributions.distribution_utils.DistributionClass)" href="#lightgbmlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Zero-Adjusted Beta distribution class.</p>
<p>The zero-adjusted Beta distribution is similar to the Beta distribution but allows zeros as y values.</p>
<h6 id="lightgbmlss.distributions.ZABeta.ZABeta--distributional-parameters">Distributional Parameters</h6>
<p>concentration1: torch.Tensor
    1st concentration parameter of the distribution (often referred to as alpha).
concentration0: torch.Tensor
    2nd concentration parameter of the distribution (often referred to as beta).
gate: torch.Tensor
    Probability of zeros given via a Bernoulli distribution.</p>
<h6 id="lightgbmlss.distributions.ZABeta.ZABeta--source">Source</h6>
<p>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py</p>
<h6 id="lightgbmlss.distributions.ZABeta.ZABeta--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood).
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/ZABeta.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZABeta</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Zero-Adjusted Beta distribution class.</span>

<span class="sd">    The zero-adjusted Beta distribution is similar to the Beta distribution but allows zeros as y values.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    concentration1: torch.Tensor</span>
<span class="sd">        1st concentration parameter of the distribution (often referred to as alpha).</span>
<span class="sd">    concentration0: torch.Tensor</span>
<span class="sd">        2nd concentration parameter of the distribution (often referred to as beta).</span>
<span class="sd">    gate: torch.Tensor</span>
<span class="sd">        Probability of zeros given via a Bernoulli distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood).</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">ZeroAdjustedBeta_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;concentration1&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">,</span> <span class="s2">&quot;concentration0&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">,</span> <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">sigmoid_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZABeta.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.ZABeta.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZABeta.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZABeta.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.ZABeta.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZABeta.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZABeta.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="lightgbmlss.distributions.ZABeta.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="lightgbmlss.distributions.ZABeta.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZABeta.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="lightgbmlss.distributions.ZABeta.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZABeta.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZABeta.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="lightgbmlss.distributions.ZABeta.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZABeta.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZABeta.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="lightgbmlss.distributions.ZABeta.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZABeta.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZABeta.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="lightgbmlss.distributions.ZABeta.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZABeta.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZABeta.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="lightgbmlss.distributions.ZABeta.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZABeta.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZABeta.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.ZABeta.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZABeta.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZABeta.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.ZABeta.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZABeta.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.ZAGamma" class="doc doc-heading">
            <code>ZAGamma</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.ZAGamma.ZAGamma" class="doc doc-heading">
            <code>ZAGamma</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (lightgbmlss.distributions.distribution_utils.DistributionClass)" href="#lightgbmlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Zero-Adjusted Gamma distribution class.</p>
<p>The zero-adjusted Gamma distribution is similar to the Gamma distribution but allows zeros as y values.</p>
<h6 id="lightgbmlss.distributions.ZAGamma.ZAGamma--distributional-parameters">Distributional Parameters</h6>
<p>concentration: torch.Tensor
    shape parameter of the distribution (often referred to as alpha)
rate: torch.Tensor
    rate = 1 / scale of the distribution (often referred to as beta)
gate: torch.Tensor
    Probability of zeros given via a Bernoulli distribution.</p>
<h6 id="lightgbmlss.distributions.ZAGamma.ZAGamma--source">Source</h6>
<p>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L150</p>
<h6 id="lightgbmlss.distributions.ZAGamma.ZAGamma--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood).
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/ZAGamma.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZAGamma</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Zero-Adjusted Gamma distribution class.</span>

<span class="sd">    The zero-adjusted Gamma distribution is similar to the Gamma distribution but allows zeros as y values.</span>

<span class="sd">     Distributional Parameters</span>
<span class="sd">    --------------------------</span>
<span class="sd">    concentration: torch.Tensor</span>
<span class="sd">        shape parameter of the distribution (often referred to as alpha)</span>
<span class="sd">    rate: torch.Tensor</span>
<span class="sd">        rate = 1 / scale of the distribution (often referred to as beta)</span>
<span class="sd">    gate: torch.Tensor</span>
<span class="sd">        Probability of zeros given via a Bernoulli distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L150</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood).</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">ZeroAdjustedGamma_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;concentration&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">,</span> <span class="s2">&quot;rate&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">,</span> <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">sigmoid_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZAGamma.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.ZAGamma.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZAGamma.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZAGamma.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.ZAGamma.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZAGamma.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZAGamma.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="lightgbmlss.distributions.ZAGamma.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="lightgbmlss.distributions.ZAGamma.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZAGamma.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="lightgbmlss.distributions.ZAGamma.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZAGamma.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZAGamma.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="lightgbmlss.distributions.ZAGamma.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZAGamma.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZAGamma.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="lightgbmlss.distributions.ZAGamma.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZAGamma.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZAGamma.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="lightgbmlss.distributions.ZAGamma.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZAGamma.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZAGamma.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="lightgbmlss.distributions.ZAGamma.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZAGamma.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZAGamma.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.ZAGamma.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZAGamma.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZAGamma.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.ZAGamma.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZAGamma.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.ZALN" class="doc doc-heading">
            <code>ZALN</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.ZALN.ZALN" class="doc doc-heading">
            <code>ZALN</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (lightgbmlss.distributions.distribution_utils.DistributionClass)" href="#lightgbmlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Zero-Adjusted LogNormal distribution class.</p>
<p>The zero-adjusted Log-Normal distribution is similar to the Log-Normal distribution but allows zeros as y values.</p>
<h6 id="lightgbmlss.distributions.ZALN.ZALN--distributional-parameters">Distributional Parameters</h6>
<p>loc: torch.Tensor
    Mean of log of distribution.
scale: torch.Tensor
    Standard deviation of log of the distribution.
gate: torch.Tensor
    Probability of zeros given via a Bernoulli distribution.</p>
<h6 id="lightgbmlss.distributions.ZALN.ZALN--source">Source</h6>
<p>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L150</p>
<h6 id="lightgbmlss.distributions.ZALN.ZALN--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood).
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/ZALN.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZALN</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Zero-Adjusted LogNormal distribution class.</span>

<span class="sd">    The zero-adjusted Log-Normal distribution is similar to the Log-Normal distribution but allows zeros as y values.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    loc: torch.Tensor</span>
<span class="sd">        Mean of log of distribution.</span>
<span class="sd">    scale: torch.Tensor</span>
<span class="sd">        Standard deviation of log of the distribution.</span>
<span class="sd">    gate: torch.Tensor</span>
<span class="sd">        Probability of zeros given via a Bernoulli distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L150</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood).</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">ZeroAdjustedLogNormal_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loc&quot;</span><span class="p">:</span> <span class="n">identity_fn</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">,</span>  <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">sigmoid_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZALN.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.ZALN.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZALN.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZALN.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.ZALN.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZALN.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZALN.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="lightgbmlss.distributions.ZALN.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="lightgbmlss.distributions.ZALN.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZALN.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="lightgbmlss.distributions.ZALN.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZALN.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZALN.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="lightgbmlss.distributions.ZALN.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZALN.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZALN.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="lightgbmlss.distributions.ZALN.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZALN.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZALN.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="lightgbmlss.distributions.ZALN.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZALN.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZALN.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="lightgbmlss.distributions.ZALN.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZALN.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZALN.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.ZALN.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZALN.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZALN.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.ZALN.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZALN.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.ZINB" class="doc doc-heading">
            <code>ZINB</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.ZINB.ZINB" class="doc doc-heading">
            <code>ZINB</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (lightgbmlss.distributions.distribution_utils.DistributionClass)" href="#lightgbmlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Zero-Inflated Negative Binomial distribution class.</p>
<h6 id="lightgbmlss.distributions.ZINB.ZINB--distributional-parameters">Distributional Parameters</h6>
<p>total_count: torch.Tensor
    Non-negative number of negative Bernoulli trials to stop.
probs: torch.Tensor
    Event probabilities of success in the half open interval [0, 1).
gate: torch.Tensor
    Probability of extra zeros given via a Bernoulli distribution.</p>
<h6 id="lightgbmlss.distributions.ZINB.ZINB--source">Source</h6>
<p>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L150</p>
<h6 id="lightgbmlss.distributions.ZINB.ZINB--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn_total_count: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential), "softplus" (softplus) or "relu" (rectified linear unit).
response_fn_probs: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "sigmoid" (sigmoid).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood).
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/ZINB.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZINB</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Zero-Inflated Negative Binomial distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    total_count: torch.Tensor</span>
<span class="sd">        Non-negative number of negative Bernoulli trials to stop.</span>
<span class="sd">    probs: torch.Tensor</span>
<span class="sd">        Event probabilities of success in the half open interval [0, 1).</span>
<span class="sd">    gate: torch.Tensor</span>
<span class="sd">        Probability of extra zeros given via a Bernoulli distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L150</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn_total_count: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential), &quot;softplus&quot; (softplus) or &quot;relu&quot; (rectified linear unit).</span>
<span class="sd">    response_fn_probs: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;sigmoid&quot; (sigmoid).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood).</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn_total_count</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
                 <span class="n">response_fn_probs</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sigmoid&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1">#  Specify Response Functions for total_count</span>
        <span class="n">response_functions_total_count</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">relu_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn_total_count</span> <span class="ow">in</span> <span class="n">response_functions_total_count</span><span class="p">:</span>
            <span class="n">response_fn_total_count</span> <span class="o">=</span> <span class="n">response_functions_total_count</span><span class="p">[</span><span class="n">response_fn_total_count</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function for total_count. Please choose from &#39;exp&#39;, &#39;softplus&#39; or &#39;relu&#39;.&quot;</span><span class="p">)</span>

        <span class="c1">#  Specify Response Functions for probs</span>
        <span class="n">response_functions_probs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">:</span> <span class="n">sigmoid_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn_probs</span> <span class="ow">in</span> <span class="n">response_functions_probs</span><span class="p">:</span>
            <span class="n">response_fn_probs</span> <span class="o">=</span> <span class="n">response_functions_probs</span><span class="p">[</span><span class="n">response_fn_probs</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function for probs. Please select &#39;sigmoid&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">ZeroInflatedNegativeBinomial_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;total_count&quot;</span><span class="p">:</span> <span class="n">response_fn_total_count</span><span class="p">,</span> <span class="s2">&quot;probs&quot;</span><span class="p">:</span> <span class="n">response_fn_probs</span><span class="p">,</span> <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">sigmoid_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZINB.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.ZINB.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZINB.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZINB.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.ZINB.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZINB.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZINB.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="lightgbmlss.distributions.ZINB.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="lightgbmlss.distributions.ZINB.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZINB.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="lightgbmlss.distributions.ZINB.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZINB.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZINB.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="lightgbmlss.distributions.ZINB.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZINB.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZINB.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="lightgbmlss.distributions.ZINB.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZINB.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZINB.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="lightgbmlss.distributions.ZINB.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZINB.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZINB.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="lightgbmlss.distributions.ZINB.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZINB.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZINB.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.ZINB.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZINB.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZINB.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.ZINB.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZINB.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.ZIPoisson" class="doc doc-heading">
            <code>ZIPoisson</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.ZIPoisson.ZIPoisson" class="doc doc-heading">
            <code>ZIPoisson</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (lightgbmlss.distributions.distribution_utils.DistributionClass)" href="#lightgbmlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Zero-Inflated Poisson distribution class.</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.ZIPoisson--distributional-parameters">Distributional Parameters</h6>
<p>rate: torch.Tensor
    Rate parameter of the distribution (often referred to as lambda).
gate: torch.Tensor
    Probability of extra zeros given via a Bernoulli distribution.</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.ZIPoisson--source">Source</h6>
<p>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L121</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.ZIPoisson--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential), "softplus" (softplus) or "relu" (rectified linear unit).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood).
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/ZIPoisson.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZIPoisson</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Zero-Inflated Poisson distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    rate: torch.Tensor</span>
<span class="sd">        Rate parameter of the distribution (often referred to as lambda).</span>
<span class="sd">    gate: torch.Tensor</span>
<span class="sd">        Probability of extra zeros given via a Bernoulli distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L121</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential), &quot;softplus&quot; (softplus) or &quot;relu&quot; (rectified linear unit).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood).</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">relu_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function for total_count. Please choose from &#39;exp&#39;, &#39;softplus&#39; or &#39;relu&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">ZeroInflatedPoisson_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;rate&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">,</span> <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">sigmoid_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZIPoisson.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZIPoisson.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZIPoisson.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="lightgbmlss.distributions.ZIPoisson.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZIPoisson.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZIPoisson.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZIPoisson.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZIPoisson.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZIPoisson.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZIPoisson.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.ZIPoisson.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="lightgbmlss.distributions.ZIPoisson.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.distribution_utils" class="doc doc-heading">
            <code>distribution_utils</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.distribution_utils.DistributionClass" class="doc doc-heading">
            <code>DistributionClass</code>


</h4>


    <div class="doc doc-contents ">



        <p>Generic class that contains general functions for univariate distributions.</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass--arguments">Arguments</h6>
<p>distribution: torch.distributions.Distribution
    PyTorch Distribution class.
univariate: bool
    Whether the distribution is univariate or multivariate.
discrete: bool
    Whether the support of the distribution is discrete or continuous.
n_dist_param: int
    Number of distributional parameters.
stabilization: str
    Stabilization method.
param_dict: Dict[str, Any]
    Dictionary that maps distributional parameters to their response scale.
distribution_arg_names: List
    List of distributional parameter names.
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.
tau: List
    List of expectiles. Only used for Expectile distributon.
penalize_crossing: bool
    Whether to include a penalty term to discourage crossing of expectiles. Only used for Expectile distribution.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/distribution_utils.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">DistributionClass</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generic class that contains general functions for univariate distributions.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    distribution: torch.distributions.Distribution</span>
<span class="sd">        PyTorch Distribution class.</span>
<span class="sd">    univariate: bool</span>
<span class="sd">        Whether the distribution is univariate or multivariate.</span>
<span class="sd">    discrete: bool</span>
<span class="sd">        Whether the support of the distribution is discrete or continuous.</span>
<span class="sd">    n_dist_param: int</span>
<span class="sd">        Number of distributional parameters.</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method.</span>
<span class="sd">    param_dict: Dict[str, Any]</span>
<span class="sd">        Dictionary that maps distributional parameters to their response scale.</span>
<span class="sd">    distribution_arg_names: List</span>
<span class="sd">        List of distributional parameter names.</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    tau: List</span>
<span class="sd">        List of expectiles. Only used for Expectile distributon.</span>
<span class="sd">    penalize_crossing: bool</span>
<span class="sd">        Whether to include a penalty term to discourage crossing of expectiles. Only used for Expectile distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">distribution</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">univariate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                 <span class="n">discrete</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">n_dist_param</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">param_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">distribution_arg_names</span><span class="p">:</span> <span class="n">List</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">tau</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">penalize_crossing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span> <span class="o">=</span> <span class="n">distribution</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">univariate</span> <span class="o">=</span> <span class="n">univariate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span> <span class="o">=</span> <span class="n">discrete</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span> <span class="o">=</span> <span class="n">n_dist_param</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span> <span class="o">=</span> <span class="n">stabilization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span> <span class="o">=</span> <span class="n">param_dict</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span> <span class="o">=</span> <span class="n">distribution_arg_names</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialize</span> <span class="o">=</span> <span class="n">initialize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">penalize_crossing</span> <span class="o">=</span> <span class="n">penalize_crossing</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">objective_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function to estimate gradients and hessians of distributional parameters.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt: np.ndarray</span>
<span class="sd">            Predicted values.</span>
<span class="sd">        data: lgb.DMatrix</span>
<span class="sd">            Data used for training.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        grad: np.ndarray</span>
<span class="sd">            Gradient.</span>
<span class="sd">        hess: np.ndarray</span>
<span class="sd">            Hessian.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Target</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Weights</span>
        <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Use 1 as weight if no weights are specified</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Start values (needed to replace NaNs in predt)</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_init_score</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="c1"># Calculate gradients and hessians</span>
        <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_gradients_and_hessians</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">predt</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">metric_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that evaluates the predictions using the negative log-likelihood.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt: np.ndarray</span>
<span class="sd">            Predicted values.</span>
<span class="sd">        data: lgb.Dataset</span>
<span class="sd">            Data used for training.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        name: str</span>
<span class="sd">            Name of the evaluation metric.</span>
<span class="sd">        nll: float</span>
<span class="sd">            Negative log-likelihood.</span>
<span class="sd">        is_higher_better: bool</span>
<span class="sd">            Whether a higher value of the metric is better or not.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Target</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">n_obs</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Start values (needed to replace NaNs in predt)</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_init_score</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="c1"># Calculate loss</span>
        <span class="n">is_higher_better</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">n_obs</span><span class="p">,</span> <span class="n">is_higher_better</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">loss_fn_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                             <span class="n">params</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that calculates the loss for a given set of distributional parameters. Only used for calculating</span>
<span class="sd">        the loss for the start values.</span>

<span class="sd">        Parameter</span>
<span class="sd">        ---------</span>
<span class="sd">        params: torch.Tensor</span>
<span class="sd">            Distributional parameters.</span>
<span class="sd">        target: torch.Tensor</span>
<span class="sd">            Target values.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        loss: torch.Tensor</span>
<span class="sd">            Loss value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Replace NaNs and infinity values with 0.5</span>
        <span class="n">nan_inf_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_idx</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>

        <span class="c1"># Transform parameters to response scale</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">response_fn</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="p">]</span>

        <span class="c1"># Specify Distribution and Loss</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">*</span><span class="n">params</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalize_crossing</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">calculate_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                               <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                               <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
                               <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that calculates the starting values for each distributional parameter.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        target: np.ndarray</span>
<span class="sd">            Data from which starting values are calculated.</span>
<span class="sd">        max_iter: int</span>
<span class="sd">            Maximum number of iterations.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        loss: float</span>
<span class="sd">            Loss value.</span>
<span class="sd">        start_values: np.ndarray</span>
<span class="sd">            Starting values for each distributional parameter.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Convert target to torch.tensor</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Initialize parameters</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)]</span>

        <span class="c1"># Specify optimizer</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">LBFGS</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">max_iter</span> <span class="o">/</span> <span class="mi">4</span><span class="p">),</span> <span class="mi">20</span><span class="p">]),</span> <span class="n">line_search_fn</span><span class="o">=</span><span class="s2">&quot;strong_wolfe&quot;</span><span class="p">)</span>

        <span class="c1"># Define learning rate scheduler</span>
        <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

        <span class="c1"># Define closure</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">():</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn_start_values</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">loss</span>

        <span class="c1"># Optimize parameters</span>
        <span class="n">loss_vals</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
            <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="n">loss_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="c1"># Get final loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss_vals</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Get start values</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)])</span>

        <span class="c1"># Replace any remaining NaNs or infinity values with 0.5</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">start_values</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_params_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                        <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                        <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">start_values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
                        <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that returns the predicted parameters and the loss.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt: np.ndarray</span>
<span class="sd">            Predicted values.</span>
<span class="sd">        target: torch.Tensor</span>
<span class="sd">            Target values.</span>
<span class="sd">        start_values: List</span>
<span class="sd">            Starting values for each distributional parameter.</span>
<span class="sd">        requires_grad: bool</span>
<span class="sd">            Whether to add to the computational graph or not.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        predt: torch.Tensor</span>
<span class="sd">            Predicted parameters.</span>
<span class="sd">        loss: torch.Tensor</span>
<span class="sd">            Loss value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Predicted Parameters</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="n">predt</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>

        <span class="c1"># Replace NaNs and infinity values with unconditional start values</span>
        <span class="n">nan_inf_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span>
        <span class="n">predt</span><span class="p">[</span><span class="n">nan_inf_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_mask</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Convert to torch.tensor</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># Predicted Parameters transformed to response scale</span>
        <span class="n">predt_transformed</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">response_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="p">]</span>

        <span class="c1"># Specify Distribution and Loss</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="n">predt_transformed</span><span class="p">))</span>
            <span class="n">dist_fit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">**</span><span class="n">dist_kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;nll&quot;</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist_fit</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;crps&quot;</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
                <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_fit</span><span class="o">.</span><span class="n">rsample</span><span class="p">((</span><span class="mi">30</span><span class="p">,))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">crps_score</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dist_samples</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dist_fit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="n">predt_transformed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalize_crossing</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist_fit</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">draw_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">predt_params</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                     <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                     <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">123</span>
                     <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that draws n_samples from a predicted distribution.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt_params: pd.DataFrame</span>
<span class="sd">            pd.DataFrame with predicted distributional parameters.</span>
<span class="sd">        n_samples: int</span>
<span class="sd">            Number of sample to draw from predicted response distribution.</span>
<span class="sd">        seed: int</span>
<span class="sd">            Manual seed.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        pred_dist: pd.DataFrame</span>
<span class="sd">            DataFrame with n_samples drawn from predicted response distribution.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pred_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt_params</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
            <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="n">arg_name</span><span class="p">:</span> <span class="n">param</span> <span class="k">for</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="n">pred_params</span><span class="o">.</span><span class="n">T</span><span class="p">)}</span>
            <span class="n">dist_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">**</span><span class="n">dist_kwargs</span><span class="p">)</span>
            <span class="c1"># Sample: shape is (n_samples, n_obs, *event_shape)</span>
            <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_pred</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="c1"># Flatten any event dimensions but keep (n_samples, n_obs) as outer structure</span>
            <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (n_samples, n_obs)</span>
            <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_samples</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># (n_obs, n_samples)</span>

            <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_samples</span><span class="p">)</span>
            <span class="n">dist_samples</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;y_sample&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dist_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dist_samples</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
            <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_samples</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">dist_samples</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">booster</span><span class="p">:</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Booster</span><span class="p">,</span>
                     <span class="n">data</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                     <span class="n">start_values</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                     <span class="n">pred_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;parameters&quot;</span><span class="p">,</span>
                     <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                     <span class="n">quantiles</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
                     <span class="n">seed</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="mi">123</span>
                     <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that predicts from the trained model.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        booster : lgb.Booster</span>
<span class="sd">            Trained model.</span>
<span class="sd">        data : pd.DataFrame</span>
<span class="sd">            Data to predict from.</span>
<span class="sd">        start_values : np.ndarray.</span>
<span class="sd">            Starting values for each distributional parameter.</span>
<span class="sd">        pred_type : str</span>
<span class="sd">            Type of prediction:</span>
<span class="sd">            - &quot;samples&quot; draws n_samples from the predicted distribution.</span>
<span class="sd">            - &quot;quantiles&quot; calculates the quantiles from the predicted distribution.</span>
<span class="sd">            - &quot;parameters&quot; returns the predicted distributional parameters.</span>
<span class="sd">            - &quot;expectiles&quot; returns the predicted expectiles.</span>
<span class="sd">        n_samples : int</span>
<span class="sd">            Number of samples to draw from the predicted distribution.</span>
<span class="sd">        quantiles : List[float]</span>
<span class="sd">            List of quantiles to calculate from the predicted distribution.</span>
<span class="sd">        seed : int</span>
<span class="sd">            Seed for random number generator used to draw samples from the predicted distribution.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        pred : pd.DataFrame</span>
<span class="sd">            Predictions.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="n">booster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">raw_score</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>

        <span class="c1"># Set init_score as starting point for each distributional parameter.</span>
        <span class="n">init_score_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="n">start_values</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>

        <span class="c1"># The predictions don&#39;t include the init_score specified in creating the train data.</span>
        <span class="c1"># Hence, it needs to be added manually with the corresponding transform for each distributional parameter.</span>
        <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">response_fun</span><span class="p">(</span>
                    <span class="n">predt</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">init_score_pred</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
            <span class="p">],</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_params_predt</span><span class="p">)</span>
        <span class="n">dist_params_predt</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>

        <span class="c1"># Draw samples from predicted response distribution</span>
        <span class="n">pred_samples_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">predt_params</span><span class="o">=</span><span class="n">dist_params_predt</span><span class="p">,</span>
                                            <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                            <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;parameters&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">dist_params_predt</span>

        <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;expectiles&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">dist_params_predt</span>

        <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;samples&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pred_samples_df</span>

        <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;quantiles&quot;</span><span class="p">:</span>
            <span class="c1"># Calculate quantiles from predicted response distribution</span>
            <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_samples_df</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">quantiles</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
            <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;quant_&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">quantiles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">quantiles</span><span class="p">))]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
                <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">pred_quant_df</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">compute_gradients_and_hessians</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                       <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                       <span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                       <span class="n">weights</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates gradients and hessians.</span>

<span class="sd">        Output gradients and hessians have shape (n_samples*n_outputs, 1).</span>

<span class="sd">        Arguments:</span>
<span class="sd">        ---------</span>
<span class="sd">        loss: torch.Tensor</span>
<span class="sd">            Loss.</span>
<span class="sd">        predt: torch.Tensor</span>
<span class="sd">            List of predicted parameters.</span>
<span class="sd">        weights: np.ndarray</span>
<span class="sd">            Weights.</span>

<span class="sd">        Returns:</span>
<span class="sd">        -------</span>
<span class="sd">        grad: torch.Tensor</span>
<span class="sd">            Gradients.</span>
<span class="sd">        hess: torch.Tensor</span>
<span class="sd">            Hessians.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;nll&quot;</span><span class="p">:</span>
            <span class="c1"># Gradient and Hessian</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">autograd</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">nansum</span><span class="p">(),</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;crps&quot;</span><span class="p">:</span>
            <span class="c1"># Gradient and Hessian</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>

            <span class="c1"># # Approximation of Hessian</span>
            <span class="c1"># step_size = 1e-6</span>
            <span class="c1"># predt_upper = [</span>
            <span class="c1">#     response_fn(predt[i] + step_size).reshape(-1, 1) for i, response_fn in</span>
            <span class="c1">#     enumerate(self.param_dict.values())</span>
            <span class="c1"># ]</span>
            <span class="c1"># dist_kwargs_upper = dict(zip(self.distribution_arg_names, predt_upper))</span>
            <span class="c1"># dist_fit_upper = self.distribution(**dist_kwargs_upper)</span>
            <span class="c1"># dist_samples_upper = dist_fit_upper.rsample((30,)).squeeze(-1)</span>
            <span class="c1"># loss_upper = torch.nansum(self.crps_score(self.target, dist_samples_upper))</span>
            <span class="c1">#</span>
            <span class="c1"># predt_lower = [</span>
            <span class="c1">#     response_fn(predt[i] - step_size).reshape(-1, 1) for i, response_fn in</span>
            <span class="c1">#     enumerate(self.param_dict.values())</span>
            <span class="c1"># ]</span>
            <span class="c1"># dist_kwargs_lower = dict(zip(self.distribution_arg_names, predt_lower))</span>
            <span class="c1"># dist_fit_lower = self.distribution(**dist_kwargs_lower)</span>
            <span class="c1"># dist_samples_lower = dist_fit_lower.rsample((30,)).squeeze(-1)</span>
            <span class="c1"># loss_lower = torch.nansum(self.crps_score(self.target, dist_samples_lower))</span>
            <span class="c1">#</span>
            <span class="c1"># grad_upper = autograd(loss_upper, inputs=predt_upper)</span>
            <span class="c1"># grad_lower = autograd(loss_lower, inputs=predt_lower)</span>
            <span class="c1"># hess = [(grad_upper[i] - grad_lower[i]) / (2 * step_size) for i in range(len(grad))]</span>

        <span class="c1"># Stabilization of Derivatives</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span> <span class="o">!=</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
            <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">hess</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hess</span><span class="p">))]</span>

        <span class="c1"># Reshape</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">hess</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="c1"># Weighting</span>
        <span class="n">grad</span> <span class="o">*=</span> <span class="n">weights</span>
        <span class="n">hess</span> <span class="o">*=</span> <span class="n">weights</span>

        <span class="c1"># Reshape</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="n">hess</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">stabilize_derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_der</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;MAD&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that stabilizes Gradients and Hessians.</span>

<span class="sd">        As LightGBMLSS updates the parameter estimates by optimizing Gradients and Hessians, it is important</span>
<span class="sd">        that these are comparable in magnitude for all distributional parameters. Due to imbalances regarding the ranges,</span>
<span class="sd">        the estimation might become unstable so that it does not converge (or converge very slowly) to the optimal solution.</span>
<span class="sd">        Another way to improve convergence might be to standardize the response variable. This is especially useful if the</span>
<span class="sd">        range of the response differs strongly from the range of the Gradients and Hessians. Both, the stabilization and</span>
<span class="sd">        the standardization of the response are not always advised but need to be carefully considered.</span>
<span class="sd">        Source: https://github.com/boost-R/gamboostLSS/blob/7792951d2984f289ed7e530befa42a2a4cb04d1d/R/helpers.R#L173</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_der : torch.Tensor</span>
<span class="sd">            Input derivative, either Gradient or Hessian.</span>
<span class="sd">        type: str</span>
<span class="sd">            Stabilization method. Can be either &quot;None&quot;, &quot;MAD&quot; or &quot;L2&quot;.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        stab_der : torch.Tensor</span>
<span class="sd">            Stabilized Gradient or Hessian.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;MAD&quot;</span><span class="p">:</span>
            <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">input_der</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
            <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;L2&quot;</span><span class="p">:</span>
            <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
            <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
            <span class="n">stab_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>

        <span class="k">return</span> <span class="n">stab_der</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">crps_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">yhat_dist</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that calculates the Continuous Ranked Probability Score (CRPS) for a given set of predicted samples.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y: torch.Tensor</span>
<span class="sd">            Response variable of shape (n_observations,1).</span>
<span class="sd">        yhat_dist: torch.Tensor</span>
<span class="sd">            Predicted samples of shape (n_samples, n_observations).</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        crps: torch.Tensor</span>
<span class="sd">            CRPS score.</span>

<span class="sd">        References</span>
<span class="sd">        ----------</span>
<span class="sd">        Gneiting, Tilmann &amp; Raftery, Adrian. (2007). Strictly Proper Scoring Rules, Prediction, and Estimation.</span>
<span class="sd">        Journal of the American Statistical Association. 102. 359-378.</span>

<span class="sd">        Source</span>
<span class="sd">        ------</span>
<span class="sd">        https://github.com/elephaint/pgbm/blob/main/pgbm/torch/pgbm_dist.py#L549</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Get the number of observations</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">yhat_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Sort the forecasts in ascending order</span>
        <span class="n">yhat_dist_sorted</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">yhat_dist</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Create temporary tensors</span>
        <span class="n">y_cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">yhat_cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">yhat_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">crps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="c1"># Loop over the predicted samples generated per observation</span>
        <span class="k">for</span> <span class="n">yhat</span> <span class="ow">in</span> <span class="n">yhat_dist_sorted</span><span class="p">:</span>
            <span class="n">yhat</span> <span class="o">=</span> <span class="n">yhat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">flag</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_cdf</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">&lt;</span> <span class="n">yhat</span><span class="p">)</span>
            <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat_prev</span><span class="p">)</span> <span class="o">*</span> <span class="n">yhat_cdf</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">((</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">yhat_cdf</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">crps</span> <span class="o">+=</span> <span class="p">(</span><span class="o">~</span><span class="n">flag</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">yhat_prev</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">yhat_cdf</span> <span class="o">-</span> <span class="n">y_cdf</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">y_cdf</span> <span class="o">+=</span> <span class="n">flag</span>
            <span class="n">yhat_cdf</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">n_samples</span>
            <span class="n">yhat_prev</span> <span class="o">=</span> <span class="n">yhat</span>

        <span class="c1"># In case y_cdf == 0 after the loop</span>
        <span class="n">flag</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_cdf</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">crps</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">dist_select</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                    <span class="n">candidate_distributions</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
                    <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
                    <span class="n">plot</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="n">figure_size</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that selects the most suitable distribution among the candidate_distributions for the target variable,</span>
<span class="sd">        based on the NegLogLikelihood (lower is better).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        target: np.ndarray</span>
<span class="sd">            Response variable.</span>
<span class="sd">        candidate_distributions: List</span>
<span class="sd">            List of candidate distributions.</span>
<span class="sd">        max_iter: int</span>
<span class="sd">            Maximum number of iterations for the optimization.</span>
<span class="sd">        plot: bool</span>
<span class="sd">            If True, a density plot of the actual and fitted distribution is created.</span>
<span class="sd">        figure_size: tuple</span>
<span class="sd">            Figure size of the density plot.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        fit_df: pd.DataFrame</span>
<span class="sd">            Dataframe with the loss values of the fitted candidate distributions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dist_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">total_iterations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">total_iterations</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Fitting candidate distributions&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">)):</span>
                <span class="n">dist_name</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span>
                <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting </span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2"> distribution&quot;</span><span class="p">)</span>
                <span class="n">dist_sel</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">dist_name</span><span class="p">)()</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">loss</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">dist_sel</span><span class="o">.</span><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">)</span>
                    <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
                        <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,),</span>
                         <span class="s2">&quot;distribution&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">dist_name</span><span class="p">),</span>
                         <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">params</span><span class="p">]</span>
                         <span class="p">}</span>
                    <span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error fitting </span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2"> distribution: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
                        <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
                         <span class="s2">&quot;distribution&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">dist_name</span><span class="p">),</span>
                         <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span>
                         <span class="p">}</span>
                    <span class="p">)</span>
                <span class="n">dist_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fit_df</span><span class="p">)</span>
                <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting of candidate distributions completed&quot;</span><span class="p">)</span>
            <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">dist_list</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="n">fit_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;dist_select with plot=True requires &#39;matplotlib&#39; and &#39;seaborn&#39; &quot;</span>
                <span class="s2">&quot;to be installed. Please install the packages to use this feature. &quot;</span>
                <span class="s2">&quot;Installing via pip install lightgbmlss[all_extras] also installs &quot;</span>
                <span class="s2">&quot;the required dependencies.&quot;</span>
            <span class="p">)</span>
            <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;matplotlib&quot;</span><span class="p">,</span> <span class="s2">&quot;seaborn&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

            <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

            <span class="c1"># Select best distribution</span>
            <span class="n">best_dist</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">dist</span> <span class="ow">in</span> <span class="n">candidate_distributions</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;distribution&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                    <span class="n">best_dist_sel</span> <span class="o">=</span> <span class="n">dist</span>
                    <span class="k">break</span>
            <span class="n">best_dist_sel</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">best_dist_sel</span><span class="p">,</span> <span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;distribution&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">])()</span>
            <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>

            <span class="c1"># Transform parameters to the response scale and draw samples</span>
            <span class="n">fitted_params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">response_fun</span><span class="p">(</span><span class="n">params</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
                <span class="p">],</span>
                <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">fitted_params</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">fitted_params</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="mi">10000</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
            <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">&gt;</span> <span class="mi">500000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
            <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">fitted_params</span><span class="p">,</span>
                                                      <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                                      <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>

            <span class="c1"># Plot actual and fitted distribution</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figure_size</span><span class="p">)</span>
            <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Actual&quot;</span><span class="p">)</span>
            <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">dist_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Best-Fit: </span><span class="si">{</span><span class="n">best_dist</span><span class="p">[</span><span class="s1">&#39;distribution&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Actual vs. Best-Fit Density&quot;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s2">&quot;bold&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

        <span class="n">fit_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">,</span> <span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">fit_df</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.distribution_utils.DistributionClass.calculate_start_values" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that calculates the starting values for each distributional parameter.</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.calculate_start_values--arguments">Arguments</h6>
<p>target: np.ndarray
    Data from which starting values are calculated.
max_iter: int
    Maximum number of iterations.</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.calculate_start_values--returns">Returns</h6>
<p>loss: float
    Loss value.
start_values: np.ndarray
    Starting values for each distributional parameter.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">calculate_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                           <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                           <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
                           <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that calculates the starting values for each distributional parameter.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    target: np.ndarray</span>
<span class="sd">        Data from which starting values are calculated.</span>
<span class="sd">    max_iter: int</span>
<span class="sd">        Maximum number of iterations.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss: float</span>
<span class="sd">        Loss value.</span>
<span class="sd">    start_values: np.ndarray</span>
<span class="sd">        Starting values for each distributional parameter.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Convert target to torch.tensor</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Initialize parameters</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)]</span>

    <span class="c1"># Specify optimizer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">LBFGS</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">max_iter</span> <span class="o">/</span> <span class="mi">4</span><span class="p">),</span> <span class="mi">20</span><span class="p">]),</span> <span class="n">line_search_fn</span><span class="o">=</span><span class="s2">&quot;strong_wolfe&quot;</span><span class="p">)</span>

    <span class="c1"># Define learning rate scheduler</span>
    <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="c1"># Define closure</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">():</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn_start_values</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="c1"># Optimize parameters</span>
    <span class="n">loss_vals</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
        <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">loss_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="c1"># Get final loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss_vals</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Get start values</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)])</span>

    <span class="c1"># Replace any remaining NaNs or infinity values with 0.5</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">start_values</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.distribution_utils.DistributionClass.compute_gradients_and_hessians" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute_gradients_and_hessians</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">predt</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Calculates gradients and hessians.</p>
<p>Output gradients and hessians have shape (n_samples*n_outputs, 1).</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.compute_gradients_and_hessians--arguments">Arguments:</h6>
<p>loss: torch.Tensor
    Loss.
predt: torch.Tensor
    List of predicted parameters.
weights: np.ndarray
    Weights.</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.compute_gradients_and_hessians--returns">Returns:</h6>
<p>grad: torch.Tensor
    Gradients.
hess: torch.Tensor
    Hessians.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">compute_gradients_and_hessians</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                   <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                   <span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                   <span class="n">weights</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates gradients and hessians.</span>

<span class="sd">    Output gradients and hessians have shape (n_samples*n_outputs, 1).</span>

<span class="sd">    Arguments:</span>
<span class="sd">    ---------</span>
<span class="sd">    loss: torch.Tensor</span>
<span class="sd">        Loss.</span>
<span class="sd">    predt: torch.Tensor</span>
<span class="sd">        List of predicted parameters.</span>
<span class="sd">    weights: np.ndarray</span>
<span class="sd">        Weights.</span>

<span class="sd">    Returns:</span>
<span class="sd">    -------</span>
<span class="sd">    grad: torch.Tensor</span>
<span class="sd">        Gradients.</span>
<span class="sd">    hess: torch.Tensor</span>
<span class="sd">        Hessians.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;nll&quot;</span><span class="p">:</span>
        <span class="c1"># Gradient and Hessian</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">autograd</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">nansum</span><span class="p">(),</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;crps&quot;</span><span class="p">:</span>
        <span class="c1"># Gradient and Hessian</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>

        <span class="c1"># # Approximation of Hessian</span>
        <span class="c1"># step_size = 1e-6</span>
        <span class="c1"># predt_upper = [</span>
        <span class="c1">#     response_fn(predt[i] + step_size).reshape(-1, 1) for i, response_fn in</span>
        <span class="c1">#     enumerate(self.param_dict.values())</span>
        <span class="c1"># ]</span>
        <span class="c1"># dist_kwargs_upper = dict(zip(self.distribution_arg_names, predt_upper))</span>
        <span class="c1"># dist_fit_upper = self.distribution(**dist_kwargs_upper)</span>
        <span class="c1"># dist_samples_upper = dist_fit_upper.rsample((30,)).squeeze(-1)</span>
        <span class="c1"># loss_upper = torch.nansum(self.crps_score(self.target, dist_samples_upper))</span>
        <span class="c1">#</span>
        <span class="c1"># predt_lower = [</span>
        <span class="c1">#     response_fn(predt[i] - step_size).reshape(-1, 1) for i, response_fn in</span>
        <span class="c1">#     enumerate(self.param_dict.values())</span>
        <span class="c1"># ]</span>
        <span class="c1"># dist_kwargs_lower = dict(zip(self.distribution_arg_names, predt_lower))</span>
        <span class="c1"># dist_fit_lower = self.distribution(**dist_kwargs_lower)</span>
        <span class="c1"># dist_samples_lower = dist_fit_lower.rsample((30,)).squeeze(-1)</span>
        <span class="c1"># loss_lower = torch.nansum(self.crps_score(self.target, dist_samples_lower))</span>
        <span class="c1">#</span>
        <span class="c1"># grad_upper = autograd(loss_upper, inputs=predt_upper)</span>
        <span class="c1"># grad_lower = autograd(loss_lower, inputs=predt_lower)</span>
        <span class="c1"># hess = [(grad_upper[i] - grad_lower[i]) / (2 * step_size) for i in range(len(grad))]</span>

    <span class="c1"># Stabilization of Derivatives</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span> <span class="o">!=</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">hess</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hess</span><span class="p">))]</span>

    <span class="c1"># Reshape</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">hess</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">hess</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="c1"># Weighting</span>
    <span class="n">grad</span> <span class="o">*=</span> <span class="n">weights</span>
    <span class="n">hess</span> <span class="o">*=</span> <span class="n">weights</span>

    <span class="c1"># Reshape</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>
    <span class="n">hess</span> <span class="o">=</span> <span class="n">hess</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.distribution_utils.DistributionClass.crps_score" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">crps_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat_dist</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that calculates the Continuous Ranked Probability Score (CRPS) for a given set of predicted samples.</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.crps_score--parameters">Parameters</h6>
<p>y: torch.Tensor
    Response variable of shape (n_observations,1).
yhat_dist: torch.Tensor
    Predicted samples of shape (n_samples, n_observations).</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.crps_score--returns">Returns</h6>
<p>crps: torch.Tensor
    CRPS score.</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.crps_score--references">References</h6>
<p>Gneiting, Tilmann &amp; Raftery, Adrian. (2007). Strictly Proper Scoring Rules, Prediction, and Estimation.
Journal of the American Statistical Association. 102. 359-378.</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.crps_score--source">Source</h6>
<p>https://github.com/elephaint/pgbm/blob/main/pgbm/torch/pgbm_dist.py#L549</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">crps_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">yhat_dist</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that calculates the Continuous Ranked Probability Score (CRPS) for a given set of predicted samples.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y: torch.Tensor</span>
<span class="sd">        Response variable of shape (n_observations,1).</span>
<span class="sd">    yhat_dist: torch.Tensor</span>
<span class="sd">        Predicted samples of shape (n_samples, n_observations).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    crps: torch.Tensor</span>
<span class="sd">        CRPS score.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Gneiting, Tilmann &amp; Raftery, Adrian. (2007). Strictly Proper Scoring Rules, Prediction, and Estimation.</span>
<span class="sd">    Journal of the American Statistical Association. 102. 359-378.</span>

<span class="sd">    Source</span>
<span class="sd">    ------</span>
<span class="sd">    https://github.com/elephaint/pgbm/blob/main/pgbm/torch/pgbm_dist.py#L549</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Get the number of observations</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="n">yhat_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Sort the forecasts in ascending order</span>
    <span class="n">yhat_dist_sorted</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">yhat_dist</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Create temporary tensors</span>
    <span class="n">y_cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">yhat_cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">yhat_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">crps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># Loop over the predicted samples generated per observation</span>
    <span class="k">for</span> <span class="n">yhat</span> <span class="ow">in</span> <span class="n">yhat_dist_sorted</span><span class="p">:</span>
        <span class="n">yhat</span> <span class="o">=</span> <span class="n">yhat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">flag</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_cdf</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">&lt;</span> <span class="n">yhat</span><span class="p">)</span>
        <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat_prev</span><span class="p">)</span> <span class="o">*</span> <span class="n">yhat_cdf</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">((</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">yhat_cdf</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">crps</span> <span class="o">+=</span> <span class="p">(</span><span class="o">~</span><span class="n">flag</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">yhat_prev</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">yhat_cdf</span> <span class="o">-</span> <span class="n">y_cdf</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">y_cdf</span> <span class="o">+=</span> <span class="n">flag</span>
        <span class="n">yhat_cdf</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">n_samples</span>
        <span class="n">yhat_prev</span> <span class="o">=</span> <span class="n">yhat</span>

    <span class="c1"># In case y_cdf == 0 after the loop</span>
    <span class="n">flag</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_cdf</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">crps</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.distribution_utils.DistributionClass.dist_select" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">dist_select</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">candidate_distributions</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">figure_size</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that selects the most suitable distribution among the candidate_distributions for the target variable,
based on the NegLogLikelihood (lower is better).</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.dist_select--parameters">Parameters</h6>
<p>target: np.ndarray
    Response variable.
candidate_distributions: List
    List of candidate distributions.
max_iter: int
    Maximum number of iterations for the optimization.
plot: bool
    If True, a density plot of the actual and fitted distribution is created.
figure_size: tuple
    Figure size of the density plot.</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.dist_select--returns">Returns</h6>
<p>fit_df: pd.DataFrame
    Dataframe with the loss values of the fitted candidate distributions.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">dist_select</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                <span class="n">candidate_distributions</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
                <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
                <span class="n">plot</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                <span class="n">figure_size</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that selects the most suitable distribution among the candidate_distributions for the target variable,</span>
<span class="sd">    based on the NegLogLikelihood (lower is better).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    target: np.ndarray</span>
<span class="sd">        Response variable.</span>
<span class="sd">    candidate_distributions: List</span>
<span class="sd">        List of candidate distributions.</span>
<span class="sd">    max_iter: int</span>
<span class="sd">        Maximum number of iterations for the optimization.</span>
<span class="sd">    plot: bool</span>
<span class="sd">        If True, a density plot of the actual and fitted distribution is created.</span>
<span class="sd">    figure_size: tuple</span>
<span class="sd">        Figure size of the density plot.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    fit_df: pd.DataFrame</span>
<span class="sd">        Dataframe with the loss values of the fitted candidate distributions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dist_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">total_iterations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">total_iterations</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Fitting candidate distributions&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">)):</span>
            <span class="n">dist_name</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting </span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2"> distribution&quot;</span><span class="p">)</span>
            <span class="n">dist_sel</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">dist_name</span><span class="p">)()</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">loss</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">dist_sel</span><span class="o">.</span><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">)</span>
                <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
                    <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,),</span>
                     <span class="s2">&quot;distribution&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">dist_name</span><span class="p">),</span>
                     <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">params</span><span class="p">]</span>
                     <span class="p">}</span>
                <span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error fitting </span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2"> distribution: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
                    <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
                     <span class="s2">&quot;distribution&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">dist_name</span><span class="p">),</span>
                     <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span>
                     <span class="p">}</span>
                <span class="p">)</span>
            <span class="n">dist_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fit_df</span><span class="p">)</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting of candidate distributions completed&quot;</span><span class="p">)</span>
        <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">dist_list</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">fit_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;dist_select with plot=True requires &#39;matplotlib&#39; and &#39;seaborn&#39; &quot;</span>
            <span class="s2">&quot;to be installed. Please install the packages to use this feature. &quot;</span>
            <span class="s2">&quot;Installing via pip install lightgbmlss[all_extras] also installs &quot;</span>
            <span class="s2">&quot;the required dependencies.&quot;</span>
        <span class="p">)</span>
        <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;matplotlib&quot;</span><span class="p">,</span> <span class="s2">&quot;seaborn&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

        <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

        <span class="c1"># Select best distribution</span>
        <span class="n">best_dist</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">dist</span> <span class="ow">in</span> <span class="n">candidate_distributions</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;distribution&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">best_dist_sel</span> <span class="o">=</span> <span class="n">dist</span>
                <span class="k">break</span>
        <span class="n">best_dist_sel</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">best_dist_sel</span><span class="p">,</span> <span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;distribution&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">])()</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>

        <span class="c1"># Transform parameters to the response scale and draw samples</span>
        <span class="n">fitted_params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">response_fun</span><span class="p">(</span><span class="n">params</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
            <span class="p">],</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">fitted_params</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">fitted_params</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="mi">10000</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">&gt;</span> <span class="mi">500000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">fitted_params</span><span class="p">,</span>
                                                  <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                                  <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>

        <span class="c1"># Plot actual and fitted distribution</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figure_size</span><span class="p">)</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Actual&quot;</span><span class="p">)</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">dist_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Best-Fit: </span><span class="si">{</span><span class="n">best_dist</span><span class="p">[</span><span class="s1">&#39;distribution&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Actual vs. Best-Fit Density&quot;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s2">&quot;bold&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="n">fit_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">,</span> <span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fit_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.distribution_utils.DistributionClass.draw_samples" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">draw_samples</span><span class="p">(</span><span class="n">predt_params</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that draws n_samples from a predicted distribution.</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.draw_samples--arguments">Arguments</h6>
<p>predt_params: pd.DataFrame
    pd.DataFrame with predicted distributional parameters.
n_samples: int
    Number of sample to draw from predicted response distribution.
seed: int
    Manual seed.</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.draw_samples--returns">Returns</h6>
<p>pred_dist: pd.DataFrame
    DataFrame with n_samples drawn from predicted response distribution.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">draw_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">predt_params</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                 <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                 <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">123</span>
                 <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that draws n_samples from a predicted distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt_params: pd.DataFrame</span>
<span class="sd">        pd.DataFrame with predicted distributional parameters.</span>
<span class="sd">    n_samples: int</span>
<span class="sd">        Number of sample to draw from predicted response distribution.</span>
<span class="sd">    seed: int</span>
<span class="sd">        Manual seed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pred_dist: pd.DataFrame</span>
<span class="sd">        DataFrame with n_samples drawn from predicted response distribution.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pred_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt_params</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="n">arg_name</span><span class="p">:</span> <span class="n">param</span> <span class="k">for</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="n">pred_params</span><span class="o">.</span><span class="n">T</span><span class="p">)}</span>
        <span class="n">dist_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">**</span><span class="n">dist_kwargs</span><span class="p">)</span>
        <span class="c1"># Sample: shape is (n_samples, n_obs, *event_shape)</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_pred</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="c1"># Flatten any event dimensions but keep (n_samples, n_obs) as outer structure</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (n_samples, n_obs)</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_samples</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># (n_obs, n_samples)</span>

        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_samples</span><span class="p">)</span>
        <span class="n">dist_samples</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;y_sample&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dist_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_samples</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dist_samples</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.distribution_utils.DistributionClass.get_params_loss" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that returns the predicted parameters and the loss.</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.get_params_loss--arguments">Arguments</h6>
<p>predt: np.ndarray
    Predicted values.
target: torch.Tensor
    Target values.
start_values: List
    Starting values for each distributional parameter.
requires_grad: bool
    Whether to add to the computational graph or not.</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.get_params_loss--returns">Returns</h6>
<p>predt: torch.Tensor
    Predicted parameters.
loss: torch.Tensor
    Loss value.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_params_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                    <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">start_values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
                    <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that returns the predicted parameters and the loss.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: np.ndarray</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    target: torch.Tensor</span>
<span class="sd">        Target values.</span>
<span class="sd">    start_values: List</span>
<span class="sd">        Starting values for each distributional parameter.</span>
<span class="sd">    requires_grad: bool</span>
<span class="sd">        Whether to add to the computational graph or not.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.Tensor</span>
<span class="sd">        Predicted parameters.</span>
<span class="sd">    loss: torch.Tensor</span>
<span class="sd">        Loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Predicted Parameters</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">predt</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>

    <span class="c1"># Replace NaNs and infinity values with unconditional start values</span>
    <span class="n">nan_inf_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span>
    <span class="n">predt</span><span class="p">[</span><span class="n">nan_inf_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_mask</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Convert to torch.tensor</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="c1"># Predicted Parameters transformed to response scale</span>
    <span class="n">predt_transformed</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">response_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
    <span class="p">]</span>

    <span class="c1"># Specify Distribution and Loss</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="n">predt_transformed</span><span class="p">))</span>
        <span class="n">dist_fit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">**</span><span class="n">dist_kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;nll&quot;</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist_fit</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;crps&quot;</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
            <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_fit</span><span class="o">.</span><span class="n">rsample</span><span class="p">((</span><span class="mi">30</span><span class="p">,))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">crps_score</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dist_samples</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dist_fit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="n">predt_transformed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalize_crossing</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist_fit</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.distribution_utils.DistributionClass.loss_fn_start_values" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">loss_fn_start_values</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that calculates the loss for a given set of distributional parameters. Only used for calculating
the loss for the start values.</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.loss_fn_start_values--parameter">Parameter</h6>
<p>params: torch.Tensor
    Distributional parameters.
target: torch.Tensor
    Target values.</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.loss_fn_start_values--returns">Returns</h6>
<p>loss: torch.Tensor
    Loss value.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">loss_fn_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                         <span class="n">params</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that calculates the loss for a given set of distributional parameters. Only used for calculating</span>
<span class="sd">    the loss for the start values.</span>

<span class="sd">    Parameter</span>
<span class="sd">    ---------</span>
<span class="sd">    params: torch.Tensor</span>
<span class="sd">        Distributional parameters.</span>
<span class="sd">    target: torch.Tensor</span>
<span class="sd">        Target values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss: torch.Tensor</span>
<span class="sd">        Loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Replace NaNs and infinity values with 0.5</span>
    <span class="n">nan_inf_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_idx</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>

    <span class="c1"># Transform parameters to response scale</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">response_fn</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
    <span class="p">]</span>

    <span class="c1"># Specify Distribution and Loss</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">*</span><span class="n">params</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalize_crossing</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.distribution_utils.DistributionClass.metric_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">metric_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that evaluates the predictions using the negative log-likelihood.</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.metric_fn--arguments">Arguments</h6>
<p>predt: np.ndarray
    Predicted values.
data: lgb.Dataset
    Data used for training.</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.metric_fn--returns">Returns</h6>
<p>name: str
    Name of the evaluation metric.
nll: float
    Negative log-likelihood.
is_higher_better: bool
    Whether a higher value of the metric is better or not.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">metric_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that evaluates the predictions using the negative log-likelihood.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: np.ndarray</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    data: lgb.Dataset</span>
<span class="sd">        Data used for training.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    name: str</span>
<span class="sd">        Name of the evaluation metric.</span>
<span class="sd">    nll: float</span>
<span class="sd">        Negative log-likelihood.</span>
<span class="sd">    is_higher_better: bool</span>
<span class="sd">        Whether a higher value of the metric is better or not.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Target</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">n_obs</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Start values (needed to replace NaNs in predt)</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_init_score</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="c1"># Calculate loss</span>
    <span class="n">is_higher_better</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">n_obs</span><span class="p">,</span> <span class="n">is_higher_better</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.distribution_utils.DistributionClass.objective_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">objective_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function to estimate gradients and hessians of distributional parameters.</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.objective_fn--arguments">Arguments</h6>
<p>predt: np.ndarray
    Predicted values.
data: lgb.DMatrix
    Data used for training.</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.objective_fn--returns">Returns</h6>
<p>grad: np.ndarray
    Gradient.
hess: np.ndarray
    Hessian.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">objective_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function to estimate gradients and hessians of distributional parameters.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: np.ndarray</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    data: lgb.DMatrix</span>
<span class="sd">        Data used for training.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    grad: np.ndarray</span>
<span class="sd">        Gradient.</span>
<span class="sd">    hess: np.ndarray</span>
<span class="sd">        Hessian.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Target</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Weights</span>
    <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Use 1 as weight if no weights are specified</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Start values (needed to replace NaNs in predt)</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_init_score</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="c1"># Calculate gradients and hessians</span>
    <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_gradients_and_hessians</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">predt</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.distribution_utils.DistributionClass.predict_dist" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">predict_dist</span><span class="p">(</span><span class="n">booster</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">pred_type</span><span class="o">=</span><span class="s1">&#39;parameters&#39;</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">quantiles</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that predicts from the trained model.</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.predict_dist--arguments">Arguments</h6>
<p>booster : lgb.Booster
    Trained model.
data : pd.DataFrame
    Data to predict from.
start_values : np.ndarray.
    Starting values for each distributional parameter.
pred_type : str
    Type of prediction:
    - "samples" draws n_samples from the predicted distribution.
    - "quantiles" calculates the quantiles from the predicted distribution.
    - "parameters" returns the predicted distributional parameters.
    - "expectiles" returns the predicted expectiles.
n_samples : int
    Number of samples to draw from the predicted distribution.
quantiles : List[float]
    List of quantiles to calculate from the predicted distribution.
seed : int
    Seed for random number generator used to draw samples from the predicted distribution.</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.predict_dist--returns">Returns</h6>
<p>pred : pd.DataFrame
    Predictions.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">predict_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">booster</span><span class="p">:</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Booster</span><span class="p">,</span>
                 <span class="n">data</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                 <span class="n">start_values</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                 <span class="n">pred_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;parameters&quot;</span><span class="p">,</span>
                 <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                 <span class="n">quantiles</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
                 <span class="n">seed</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="mi">123</span>
                 <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that predicts from the trained model.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    booster : lgb.Booster</span>
<span class="sd">        Trained model.</span>
<span class="sd">    data : pd.DataFrame</span>
<span class="sd">        Data to predict from.</span>
<span class="sd">    start_values : np.ndarray.</span>
<span class="sd">        Starting values for each distributional parameter.</span>
<span class="sd">    pred_type : str</span>
<span class="sd">        Type of prediction:</span>
<span class="sd">        - &quot;samples&quot; draws n_samples from the predicted distribution.</span>
<span class="sd">        - &quot;quantiles&quot; calculates the quantiles from the predicted distribution.</span>
<span class="sd">        - &quot;parameters&quot; returns the predicted distributional parameters.</span>
<span class="sd">        - &quot;expectiles&quot; returns the predicted expectiles.</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples to draw from the predicted distribution.</span>
<span class="sd">    quantiles : List[float]</span>
<span class="sd">        List of quantiles to calculate from the predicted distribution.</span>
<span class="sd">    seed : int</span>
<span class="sd">        Seed for random number generator used to draw samples from the predicted distribution.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pred : pd.DataFrame</span>
<span class="sd">        Predictions.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
        <span class="n">booster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">raw_score</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>

    <span class="c1"># Set init_score as starting point for each distributional parameter.</span>
    <span class="n">init_score_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="n">start_values</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="p">)</span>

    <span class="c1"># The predictions don&#39;t include the init_score specified in creating the train data.</span>
    <span class="c1"># Hence, it needs to be added manually with the corresponding transform for each distributional parameter.</span>
    <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">response_fun</span><span class="p">(</span>
                <span class="n">predt</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">init_score_pred</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
        <span class="p">],</span>
        <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_params_predt</span><span class="p">)</span>
    <span class="n">dist_params_predt</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>

    <span class="c1"># Draw samples from predicted response distribution</span>
    <span class="n">pred_samples_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">predt_params</span><span class="o">=</span><span class="n">dist_params_predt</span><span class="p">,</span>
                                        <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;parameters&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">dist_params_predt</span>

    <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;expectiles&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">dist_params_predt</span>

    <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;samples&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pred_samples_df</span>

    <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;quantiles&quot;</span><span class="p">:</span>
        <span class="c1"># Calculate quantiles from predicted response distribution</span>
        <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_samples_df</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">quantiles</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;quant_&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">quantiles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">quantiles</span><span class="p">))]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
            <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pred_quant_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.distribution_utils.DistributionClass.stabilize_derivative" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s1">&#39;MAD&#39;</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that stabilizes Gradients and Hessians.</p>
<p>As LightGBMLSS updates the parameter estimates by optimizing Gradients and Hessians, it is important
that these are comparable in magnitude for all distributional parameters. Due to imbalances regarding the ranges,
the estimation might become unstable so that it does not converge (or converge very slowly) to the optimal solution.
Another way to improve convergence might be to standardize the response variable. This is especially useful if the
range of the response differs strongly from the range of the Gradients and Hessians. Both, the stabilization and
the standardization of the response are not always advised but need to be carefully considered.
Source: https://github.com/boost-R/gamboostLSS/blob/7792951d2984f289ed7e530befa42a2a4cb04d1d/R/helpers.R#L173</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.stabilize_derivative--parameters">Parameters</h6>
<p>input_der : torch.Tensor
    Input derivative, either Gradient or Hessian.
type: str
    Stabilization method. Can be either "None", "MAD" or "L2".</p>
<h6 id="lightgbmlss.distributions.distribution_utils.DistributionClass.stabilize_derivative--returns">Returns</h6>
<p>stab_der : torch.Tensor
    Stabilized Gradient or Hessian.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">stabilize_derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_der</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;MAD&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that stabilizes Gradients and Hessians.</span>

<span class="sd">    As LightGBMLSS updates the parameter estimates by optimizing Gradients and Hessians, it is important</span>
<span class="sd">    that these are comparable in magnitude for all distributional parameters. Due to imbalances regarding the ranges,</span>
<span class="sd">    the estimation might become unstable so that it does not converge (or converge very slowly) to the optimal solution.</span>
<span class="sd">    Another way to improve convergence might be to standardize the response variable. This is especially useful if the</span>
<span class="sd">    range of the response differs strongly from the range of the Gradients and Hessians. Both, the stabilization and</span>
<span class="sd">    the standardization of the response are not always advised but need to be carefully considered.</span>
<span class="sd">    Source: https://github.com/boost-R/gamboostLSS/blob/7792951d2984f289ed7e530befa42a2a4cb04d1d/R/helpers.R#L173</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_der : torch.Tensor</span>
<span class="sd">        Input derivative, either Gradient or Hessian.</span>
<span class="sd">    type: str</span>
<span class="sd">        Stabilization method. Can be either &quot;None&quot;, &quot;MAD&quot; or &quot;L2&quot;.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    stab_der : torch.Tensor</span>
<span class="sd">        Stabilized Gradient or Hessian.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;MAD&quot;</span><span class="p">:</span>
        <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">input_der</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
        <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;L2&quot;</span><span class="p">:</span>
        <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
        <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
        <span class="n">stab_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>

    <span class="k">return</span> <span class="n">stab_der</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.flow_utils" class="doc doc-heading">
            <code>flow_utils</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass" class="doc doc-heading">
            <code>NormalizingFlowClass</code>


</h4>


    <div class="doc doc-contents ">



        <p>Generic class that contains general functions for normalizing flows.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass--arguments">Arguments</h6>
<p>base_dist: torch.distributions.Distribution
    PyTorch Distribution class. Currently only Normal is supported.
flow_transform: Transform
    Specify the normalizing flow transform.
count_bins: Optional[int]
    The number of segments comprising the spline. Only used if flow_transform is Spline.
bound: Optional[float]
    The quantity "K" determining the bounding box, [-K,K] x [-K,K] of the spline. By adjusting the
    "K" value, you can control the size of the bounding box and consequently control the range of inputs that
    the spline transform operates on. Larger values of "K" will result in a wider valid range for the spline
    transformation, while smaller values will restrict the valid range to a smaller region. Should be chosen
    based on the range of the data. Only used if flow_transform is Spline.
order: Optional[str]
    The order of the spline. Options are "linear" or "quadratic". Only used if flow_transform is Spline.
n_dist_param: int
    Number of parameters.
param_dict: Dict[str, Any]
    Dictionary that maps parameters to their response scale.
distribution_arg_names: List
    List of distributional parameter names.
target_transform: Transform
    Specify the target transform.
discrete: bool
    Whether the target is discrete or not.
univariate: bool
    Whether the distribution is univariate or multivariate.
stabilization: str
    Stabilization method. Options are "None", "MAD" or "L2".
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/flow_utils.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">NormalizingFlowClass</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generic class that contains general functions for normalizing flows.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    base_dist: torch.distributions.Distribution</span>
<span class="sd">        PyTorch Distribution class. Currently only Normal is supported.</span>
<span class="sd">    flow_transform: Transform</span>
<span class="sd">        Specify the normalizing flow transform.</span>
<span class="sd">    count_bins: Optional[int]</span>
<span class="sd">        The number of segments comprising the spline. Only used if flow_transform is Spline.</span>
<span class="sd">    bound: Optional[float]</span>
<span class="sd">        The quantity &quot;K&quot; determining the bounding box, [-K,K] x [-K,K] of the spline. By adjusting the</span>
<span class="sd">        &quot;K&quot; value, you can control the size of the bounding box and consequently control the range of inputs that</span>
<span class="sd">        the spline transform operates on. Larger values of &quot;K&quot; will result in a wider valid range for the spline</span>
<span class="sd">        transformation, while smaller values will restrict the valid range to a smaller region. Should be chosen</span>
<span class="sd">        based on the range of the data. Only used if flow_transform is Spline.</span>
<span class="sd">    order: Optional[str]</span>
<span class="sd">        The order of the spline. Options are &quot;linear&quot; or &quot;quadratic&quot;. Only used if flow_transform is Spline.</span>
<span class="sd">    n_dist_param: int</span>
<span class="sd">        Number of parameters.</span>
<span class="sd">    param_dict: Dict[str, Any]</span>
<span class="sd">        Dictionary that maps parameters to their response scale.</span>
<span class="sd">    distribution_arg_names: List</span>
<span class="sd">        List of distributional parameter names.</span>
<span class="sd">    target_transform: Transform</span>
<span class="sd">        Specify the target transform.</span>
<span class="sd">    discrete: bool</span>
<span class="sd">        Whether the target is discrete or not.</span>
<span class="sd">    univariate: bool</span>
<span class="sd">        Whether the distribution is univariate or multivariate.</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method. Options are &quot;None&quot;, &quot;MAD&quot; or &quot;L2&quot;.</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">base_dist</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">flow_transform</span><span class="p">:</span> <span class="n">Transform</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">count_bins</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
                 <span class="n">bound</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">3.0</span><span class="p">,</span>
                 <span class="n">order</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;quadratic&quot;</span><span class="p">,</span>
                 <span class="n">n_dist_param</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">param_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">distribution_arg_names</span><span class="p">:</span> <span class="n">List</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">target_transform</span><span class="p">:</span> <span class="n">Transform</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">discrete</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">univariate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span> <span class="o">=</span> <span class="n">base_dist</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flow_transform</span> <span class="o">=</span> <span class="n">flow_transform</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span> <span class="o">=</span> <span class="n">count_bins</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bound</span> <span class="o">=</span> <span class="n">bound</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">order</span> <span class="o">=</span> <span class="n">order</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span> <span class="o">=</span> <span class="n">n_dist_param</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span> <span class="o">=</span> <span class="n">param_dict</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span> <span class="o">=</span> <span class="n">distribution_arg_names</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_transform</span> <span class="o">=</span> <span class="n">target_transform</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span> <span class="o">=</span> <span class="n">discrete</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">univariate</span> <span class="o">=</span> <span class="n">univariate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span> <span class="o">=</span> <span class="n">stabilization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialize</span> <span class="o">=</span> <span class="n">initialize</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">objective_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function to estimate gradients and hessians of normalizing flow parameters.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt: np.ndarray</span>
<span class="sd">            Predicted values.</span>
<span class="sd">        data: lgb.Dataset</span>
<span class="sd">            Data used for training.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        grad: np.ndarray</span>
<span class="sd">            Gradient.</span>
<span class="sd">        hess: np.ndarray</span>
<span class="sd">            Hessian.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Target</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Weights</span>
        <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Use 1 as weight if no weights are specified</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Start values (needed to replace NaNs in predt)</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_init_score</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="c1"># Calculate gradients and hessians</span>
        <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">)</span>
        <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_gradients_and_hessians</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">predt</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">metric_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that evaluates the predictions using the specified loss function.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt: np.ndarray</span>
<span class="sd">            Predicted values.</span>
<span class="sd">        data: lgb.Dataset</span>
<span class="sd">            Data used for training.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        name: str</span>
<span class="sd">            Name of the evaluation metric.</span>
<span class="sd">        loss: float</span>
<span class="sd">            Loss value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Target</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">n_obs</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Start values (needed to replace NaNs in predt)</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_init_score</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="c1"># Calculate loss</span>
        <span class="n">is_higher_better</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">/</span> <span class="n">n_obs</span><span class="p">,</span> <span class="n">is_higher_better</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">calculate_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                               <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                               <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
                               <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that calculates starting values for each parameter.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        target: np.ndarray</span>
<span class="sd">            Data from which starting values are calculated.</span>
<span class="sd">        max_iter: int</span>
<span class="sd">            Maximum number of iterations.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        loss: float</span>
<span class="sd">            Loss value.</span>
<span class="sd">        start_values: np.ndarray</span>
<span class="sd">            Starting values for each parameter.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Convert target to torch.tensor</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Create Normalizing Flow</span>
        <span class="n">flow_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_spline_flow</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Specify optimizer</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">LBFGS</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                          <span class="n">lr</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
                          <span class="n">max_iter</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">max_iter</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span> <span class="mi">50</span><span class="p">]),</span>
                          <span class="n">line_search_fn</span><span class="o">=</span><span class="s2">&quot;strong_wolfe&quot;</span><span class="p">)</span>

        <span class="c1"># Define learning rate scheduler</span>
        <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

        <span class="c1"># Define closure</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">():</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">flow_dist</span><span class="o">.</span><span class="n">clear_cache</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">loss</span>

        <span class="c1"># Optimize parameters</span>
        <span class="n">loss_vals</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">tolerance</span> <span class="o">=</span> <span class="mf">1e-5</span>           <span class="c1"># Tolerance level for loss change</span>
        <span class="n">patience</span> <span class="o">=</span> <span class="mi">5</span>               <span class="c1"># Patience level for loss change</span>
        <span class="n">best_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
        <span class="n">epochs_without_change</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
            <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="n">loss_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

            <span class="c1"># Stopping criterion (no improvement in loss)</span>
            <span class="k">if</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">best_loss</span> <span class="o">-</span> <span class="n">tolerance</span><span class="p">:</span>
                <span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">epochs_without_change</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">epochs_without_change</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="k">if</span> <span class="n">epochs_without_change</span> <span class="o">&gt;=</span> <span class="n">patience</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="c1"># Get final loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss_vals</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Get start values</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">param</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">start_values</span><span class="p">])</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="c1"># Replace any remaining NaNs or infinity values with 0.5</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">start_values</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_params_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                        <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                        <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">start_values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
                        <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that returns the predicted parameters and the loss.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt: np.ndarray</span>
<span class="sd">            Predicted values.</span>
<span class="sd">        target: torch.Tensor</span>
<span class="sd">            Target values.</span>
<span class="sd">        start_values: List</span>
<span class="sd">            Starting values for each parameter.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        predt: torch.Tensor</span>
<span class="sd">            Predicted parameters.</span>
<span class="sd">        loss: torch.Tensor</span>
<span class="sd">            Loss value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Reshape Target</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Predicted Parameters</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="n">predt</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>

        <span class="c1"># Replace NaNs and infinity values with unconditional start values</span>
        <span class="n">nan_inf_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span>
        <span class="n">predt</span><span class="p">[</span><span class="n">nan_inf_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_mask</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Convert to torch.tensor</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1"># Specify Normalizing Flow</span>
        <span class="n">flow_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_spline_flow</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="c1"># Replace parameters with estimated ones</span>
        <span class="n">params</span><span class="p">,</span> <span class="n">flow_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replace_parameters</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">flow_dist</span><span class="p">)</span>

        <span class="c1"># Calculate loss</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;nll&quot;</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;crps&quot;</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
            <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">flow_dist</span><span class="o">.</span><span class="n">rsample</span><span class="p">((</span><span class="mi">30</span><span class="p">,))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">crps_score</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dist_samples</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">params</span><span class="p">,</span> <span class="n">loss</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">create_spline_flow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                           <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                           <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Transform</span><span class="p">:</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that constructs a Normalizing Flow.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        input_dim: int</span>
<span class="sd">            Input dimension.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        spline_flow: Transform</span>
<span class="sd">            Normalizing Flow.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Create flow distribution (currently only Normal)</span>
        <span class="n">loc</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_dim</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">input_dim</span><span class="p">)</span>
        <span class="n">flow_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>

        <span class="c1"># Create Spline Transform</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
        <span class="n">spline_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flow_transform</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span>
                                               <span class="n">count_bins</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">,</span>
                                               <span class="n">bound</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bound</span><span class="p">,</span>
                                               <span class="n">order</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">order</span><span class="p">)</span>

        <span class="c1"># Create Normalizing Flow</span>
        <span class="n">spline_flow</span> <span class="o">=</span> <span class="n">TransformedDistribution</span><span class="p">(</span><span class="n">flow_dist</span><span class="p">,</span> <span class="p">[</span><span class="n">spline_transform</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_transform</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">spline_flow</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">replace_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                           <span class="n">params</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                           <span class="n">flow_dist</span><span class="p">:</span> <span class="n">Transform</span><span class="p">,</span>
                           <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">Transform</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Replace parameters with estimated ones.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        params: torch.Tensor</span>
<span class="sd">            Estimated parameters.</span>
<span class="sd">        flow_dist: Transform</span>
<span class="sd">            Normalizing Flow.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        params_list: List</span>
<span class="sd">            List of estimated parameters.</span>
<span class="sd">        flow_dist: Transform</span>
<span class="sd">            Normalizing Flow with estimated parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Split parameters into list</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">order</span> <span class="o">==</span> <span class="s2">&quot;quadratic&quot;</span><span class="p">:</span>
            <span class="n">params_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
                <span class="n">params</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">order</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
            <span class="n">params_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
                <span class="n">params</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">],</span>
                <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Replace parameters</span>
        <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">new_value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">params_list</span><span class="p">):</span>
            <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">new_value</span>

        <span class="c1"># Get parameters (including require_grad=True)</span>
        <span class="n">params_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

        <span class="k">return</span> <span class="n">params_list</span><span class="p">,</span> <span class="n">flow_dist</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">draw_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">predt_params</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                     <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                     <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">123</span>
                     <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that draws n_samples from a predicted distribution.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt_params: pd.DataFrame</span>
<span class="sd">            pd.DataFrame with predicted distributional parameters.</span>
<span class="sd">        n_samples: int</span>
<span class="sd">            Number of sample to draw from predicted response distribution.</span>
<span class="sd">        seed: int</span>
<span class="sd">            Manual seed.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        pred_dist: pd.DataFrame</span>
<span class="sd">            DataFrame with n_samples drawn from predicted response distribution.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

        <span class="c1"># Specify Normalizing Flow</span>
        <span class="n">pred_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt_params</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="n">flow_dist_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_spline_flow</span><span class="p">(</span><span class="n">pred_params</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="c1"># Replace parameters with estimated ones</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">flow_dist_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replace_parameters</span><span class="p">(</span><span class="n">pred_params</span><span class="p">,</span> <span class="n">flow_dist_pred</span><span class="p">)</span>

        <span class="c1"># Draw samples (n_samples, n_obs, *event_shape)</span>
        <span class="n">flow_samples</span> <span class="o">=</span> <span class="n">flow_dist_pred</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="c1"># Flatten event dims, keep (n_samples, n_obs) as outer shape</span>
        <span class="n">flow_samples</span> <span class="o">=</span> <span class="n">flow_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (n_samples, n_obs)</span>
        <span class="n">flow_samples</span> <span class="o">=</span> <span class="n">flow_samples</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># (n_obs, n_samples)</span>

        <span class="n">flow_samples</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">flow_samples</span><span class="p">)</span>
        <span class="n">flow_samples</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;y_sample&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">flow_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
            <span class="n">flow_samples</span> <span class="o">=</span> <span class="n">flow_samples</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">flow_samples</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">booster</span><span class="p">:</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Booster</span><span class="p">,</span>
                     <span class="n">data</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                     <span class="n">start_values</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                     <span class="n">pred_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;parameters&quot;</span><span class="p">,</span>
                     <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                     <span class="n">quantiles</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
                     <span class="n">seed</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="mi">123</span>
                     <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that predicts from the trained model.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        booster : lgb.Booster</span>
<span class="sd">            Trained model.</span>
<span class="sd">        start_values : np.ndarray</span>
<span class="sd">            Starting values for each distributional parameter.</span>
<span class="sd">        data : pd.DataFrame</span>
<span class="sd">            Data to predict from.</span>
<span class="sd">        pred_type : str</span>
<span class="sd">            Type of prediction:</span>
<span class="sd">            - &quot;samples&quot; draws n_samples from the predicted distribution.</span>
<span class="sd">            - &quot;quantiles&quot; calculates the quantiles from the predicted distribution.</span>
<span class="sd">            - &quot;parameters&quot; returns the predicted distributional parameters.</span>
<span class="sd">            - &quot;expectiles&quot; returns the predicted expectiles.</span>
<span class="sd">        n_samples : int</span>
<span class="sd">            Number of samples to draw from the predicted distribution.</span>
<span class="sd">        quantiles : List[float]</span>
<span class="sd">            List of quantiles to calculate from the predicted distribution.</span>
<span class="sd">        seed : int</span>
<span class="sd">            Seed for random number generator used to draw samples from the predicted distribution.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        pred : pd.DataFrame</span>
<span class="sd">            Predictions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Predict raw scores</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="n">booster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">raw_score</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>

        <span class="c1"># Set init_score as starting point for each distributional parameter.</span>
        <span class="n">init_score_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">start_values</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>

        <span class="c1"># The predictions don&#39;t include the init_score specified in creating the train data. Hence, it needs to be</span>
        <span class="c1"># added manually.</span>
        <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                <span class="p">[</span><span class="n">predt</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">init_score_pred</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)],</span>
                <span class="n">axis</span><span class="o">=</span><span class="mi">1</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="n">dist_params_predt</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>

        <span class="c1"># Draw samples from predicted response distribution</span>
        <span class="n">pred_samples_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">predt_params</span><span class="o">=</span><span class="n">dist_params_predt</span><span class="p">,</span>
                                            <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                            <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;parameters&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">dist_params_predt</span>

        <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;samples&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pred_samples_df</span>

        <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;quantiles&quot;</span><span class="p">:</span>
            <span class="c1"># Calculate quantiles from predicted response distribution</span>
            <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_samples_df</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">quantiles</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
            <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;quant_&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">quantiles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">quantiles</span><span class="p">))]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
                <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">pred_quant_df</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">compute_gradients_and_hessians</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                       <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                       <span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                       <span class="n">weights</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates gradients and hessians.</span>

<span class="sd">        Output gradients and hessians have shape (n_samples*n_outputs, 1).</span>

<span class="sd">        Arguments:</span>
<span class="sd">        ---------</span>
<span class="sd">        loss: torch.Tensor</span>
<span class="sd">            Loss.</span>
<span class="sd">        predt: torch.Tensor</span>
<span class="sd">            List of predicted parameters.</span>
<span class="sd">        weights: np.ndarray</span>
<span class="sd">            Weights.</span>

<span class="sd">        Returns:</span>
<span class="sd">        -------</span>
<span class="sd">        grad: torch.Tensor</span>
<span class="sd">            Gradients.</span>
<span class="sd">        hess: torch.Tensor</span>
<span class="sd">            Hessians.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;nll&quot;</span><span class="p">:</span>
            <span class="c1"># Gradient and Hessian</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">autograd</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">nansum</span><span class="p">(),</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;crps&quot;</span><span class="p">:</span>
            <span class="c1"># Gradient and Hessian</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>

        <span class="c1"># Stabilization of Derivatives</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span> <span class="o">!=</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
            <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">hess</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hess</span><span class="p">))]</span>

        <span class="c1"># Reshape</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">hess</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="c1"># Weighting</span>
        <span class="n">grad</span> <span class="o">*=</span> <span class="n">weights</span>
        <span class="n">hess</span> <span class="o">*=</span> <span class="n">weights</span>

        <span class="c1"># Reshape</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="n">hess</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">stabilize_derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_der</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;MAD&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that stabilizes Gradients and Hessians.</span>

<span class="sd">        Since parameters are estimated by optimizing Gradients and Hessians, it is important that these are comparable</span>
<span class="sd">        in magnitude for all parameters. Due to imbalances regarding the ranges, the estimation might become unstable</span>
<span class="sd">        so that it does not converge (or converge very slowly) to the optimal solution. Another way to improve</span>
<span class="sd">        convergence might be to standardize the response variable. This is especially useful if the range of the</span>
<span class="sd">        response differs strongly from the range of the Gradients and Hessians. Both, the stabilization and the</span>
<span class="sd">        standardization of the response are not always advised but need to be carefully considered.</span>

<span class="sd">        Source</span>
<span class="sd">        ---------</span>
<span class="sd">        https://github.com/boost-R/gamboostLSS/blob/7792951d2984f289ed7e530befa42a2a4cb04d1d/R/helpers.R#L173</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        input_der : torch.Tensor</span>
<span class="sd">            Input derivative, either Gradient or Hessian.</span>
<span class="sd">        type: str</span>
<span class="sd">            Stabilization method. Can be either &quot;None&quot;, &quot;MAD&quot; or &quot;L2&quot;.</span>

<span class="sd">        Returns</span>
<span class="sd">        ---------</span>
<span class="sd">        stab_der : torch.Tensor</span>
<span class="sd">            Stabilized Gradient or Hessian.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;MAD&quot;</span><span class="p">:</span>
            <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">input_der</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
            <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;L2&quot;</span><span class="p">:</span>
            <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
            <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
            <span class="n">stab_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>

        <span class="k">return</span> <span class="n">stab_der</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">crps_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">yhat_dist</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that calculates the Continuous Ranked Probability Score (CRPS) for a given set of predicted samples.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        y: torch.Tensor</span>
<span class="sd">            Response variable of shape (n_observations,1).</span>
<span class="sd">        yhat_dist: torch.Tensor</span>
<span class="sd">            Predicted samples of shape (n_samples, n_observations).</span>

<span class="sd">        Returns</span>
<span class="sd">        ---------</span>
<span class="sd">        crps: torch.Tensor</span>
<span class="sd">            CRPS score.</span>

<span class="sd">        References</span>
<span class="sd">        ---------</span>
<span class="sd">        Gneiting, Tilmann &amp; Raftery, Adrian. (2007). Strictly Proper Scoring Rules, Prediction, and Estimation.</span>
<span class="sd">        Journal of the American Statistical Association. 102. 359-378.</span>

<span class="sd">        Source</span>
<span class="sd">        ---------</span>
<span class="sd">        https://github.com/elephaint/pgbm/blob/main/pgbm/torch/pgbm_dist.py#L549</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Get the number of observations</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">yhat_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Sort the forecasts in ascending order</span>
        <span class="n">yhat_dist_sorted</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">yhat_dist</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Create temporary tensors</span>
        <span class="n">y_cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">yhat_cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">yhat_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">crps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="c1"># Loop over the predicted samples generated per observation</span>
        <span class="k">for</span> <span class="n">yhat</span> <span class="ow">in</span> <span class="n">yhat_dist_sorted</span><span class="p">:</span>
            <span class="n">yhat</span> <span class="o">=</span> <span class="n">yhat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">flag</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_cdf</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">&lt;</span> <span class="n">yhat</span><span class="p">)</span>
            <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat_prev</span><span class="p">)</span> <span class="o">*</span> <span class="n">yhat_cdf</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">((</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">yhat_cdf</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">crps</span> <span class="o">+=</span> <span class="p">(</span><span class="o">~</span><span class="n">flag</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">yhat_prev</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">yhat_cdf</span> <span class="o">-</span> <span class="n">y_cdf</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">y_cdf</span> <span class="o">+=</span> <span class="n">flag</span>
            <span class="n">yhat_cdf</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">n_samples</span>
            <span class="n">yhat_prev</span> <span class="o">=</span> <span class="n">yhat</span>

        <span class="c1"># In case y_cdf == 0 after the loop</span>
        <span class="n">flag</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_cdf</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">crps</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">flow_select</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                    <span class="n">candidate_flows</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
                    <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
                    <span class="n">plot</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="n">figure_size</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that selects the most suitable normalizing flow specification among the candidate_flow for the</span>
<span class="sd">        target variable, based on the NegLogLikelihood (lower is better).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        target: np.ndarray</span>
<span class="sd">            Response variable.</span>
<span class="sd">        candidate_flows: List</span>
<span class="sd">            List of candidate normalizing flow specifications.</span>
<span class="sd">        max_iter: int</span>
<span class="sd">            Maximum number of iterations for the optimization.</span>
<span class="sd">        plot: bool</span>
<span class="sd">            If True, a density plot of the actual and fitted distribution is created.</span>
<span class="sd">        figure_size: tuple</span>
<span class="sd">            Figure size of the density plot.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        fit_df: pd.DataFrame</span>
<span class="sd">            Dataframe with the loss values of the fitted normalizing flow.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">flow_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">total_iterations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">candidate_flows</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">total_iterations</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Fitting candidate normalizing flows&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">flow</span> <span class="ow">in</span> <span class="n">candidate_flows</span><span class="p">:</span>
                <span class="n">flow_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">flow</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;&#39;&gt;&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">flow_spec</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;(count_bins: </span><span class="si">{</span><span class="n">flow</span><span class="o">.</span><span class="n">count_bins</span><span class="si">}</span><span class="s2">, order: </span><span class="si">{</span><span class="n">flow</span><span class="o">.</span><span class="n">order</span><span class="si">}</span><span class="s2">)&quot;</span>
                <span class="n">flow_name</span> <span class="o">=</span> <span class="n">flow_name</span> <span class="o">+</span> <span class="n">flow_spec</span>
                <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting </span><span class="si">{</span><span class="n">flow_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">flow_sel</span> <span class="o">=</span> <span class="n">flow</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">loss</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">flow_sel</span><span class="o">.</span><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">)</span>
                    <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
                        <span class="p">{</span><span class="n">flow_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span>
                         <span class="s2">&quot;NormFlow&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">flow_name</span><span class="p">),</span>
                         <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">params</span><span class="p">]</span>
                         <span class="p">}</span>
                    <span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error fitting </span><span class="si">{</span><span class="n">flow_sel</span><span class="si">}</span><span class="s2"> NormFlow: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
                        <span class="p">{</span><span class="n">flow_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
                         <span class="s2">&quot;NormFlow&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">flow_sel</span><span class="p">),</span>
                         <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span> <span class="o">*</span> <span class="n">flow_sel</span><span class="o">.</span><span class="n">n_dist_param</span>
                         <span class="p">}</span>
                    <span class="p">)</span>
                <span class="n">flow_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fit_df</span><span class="p">)</span>
                <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting of candidate normalizing flows completed&quot;</span><span class="p">)</span>
            <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">flow_list</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="n">flow_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">flow_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="n">fit_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;dist_select with plot=True requires &#39;matplotlib&#39; and &#39;seaborn&#39; &quot;</span>
                <span class="s2">&quot;to be installed. Please install the packages to use this feature. &quot;</span>
                <span class="s2">&quot;Installing via pip install lightgbmlss[all_extras] also installs &quot;</span>
                <span class="s2">&quot;the required dependencies.&quot;</span>
            <span class="p">)</span>
            <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;matplotlib&quot;</span><span class="p">,</span> <span class="s2">&quot;seaborn&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

            <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

            <span class="c1"># Select normalizing flow with the lowest loss</span>
            <span class="n">best_flow</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">flow</span> <span class="ow">in</span> <span class="n">candidate_flows</span><span class="p">:</span>
                <span class="n">flow_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">flow</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;&#39;&gt;&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">flow_spec</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;(count_bins: </span><span class="si">{</span><span class="n">flow</span><span class="o">.</span><span class="n">count_bins</span><span class="si">}</span><span class="s2">, order: </span><span class="si">{</span><span class="n">flow</span><span class="o">.</span><span class="n">order</span><span class="si">}</span><span class="s2">)&quot;</span>
                <span class="n">flow_name</span> <span class="o">=</span> <span class="n">flow_name</span> <span class="o">+</span> <span class="n">flow_spec</span>
                <span class="k">if</span> <span class="n">flow_name</span> <span class="o">==</span> <span class="n">best_flow</span><span class="p">[</span><span class="s2">&quot;NormFlow&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                    <span class="n">best_flow_sel</span> <span class="o">=</span> <span class="n">flow</span>
                    <span class="k">break</span>

            <span class="c1"># Draw samples from distribution</span>
            <span class="n">flow_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">best_flow</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">flow_dist_sel</span> <span class="o">=</span> <span class="n">best_flow_sel</span><span class="o">.</span><span class="n">create_spline_flow</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">flow_dist_sel</span> <span class="o">=</span> <span class="n">best_flow_sel</span><span class="o">.</span><span class="n">replace_parameters</span><span class="p">(</span><span class="n">flow_params</span><span class="p">,</span> <span class="n">flow_dist_sel</span><span class="p">)</span>
            <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="mi">10000</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
            <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">&gt;</span> <span class="mi">500000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
            <span class="n">flow_samples</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">flow_dist_sel</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>

            <span class="c1"># Plot actual and fitted distribution</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figure_size</span><span class="p">)</span>
            <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Actual&quot;</span><span class="p">)</span>
            <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">flow_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Best-Fit: </span><span class="si">{</span><span class="n">best_flow</span><span class="p">[</span><span class="s1">&#39;NormFlow&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Actual vs. Best-Fit Density&quot;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s2">&quot;bold&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

        <span class="n">fit_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">,</span> <span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">fit_df</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.calculate_start_values" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that calculates starting values for each parameter.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.calculate_start_values--arguments">Arguments</h6>
<p>target: np.ndarray
    Data from which starting values are calculated.
max_iter: int
    Maximum number of iterations.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.calculate_start_values--returns">Returns</h6>
<p>loss: float
    Loss value.
start_values: np.ndarray
    Starting values for each parameter.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">calculate_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                           <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                           <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
                           <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that calculates starting values for each parameter.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    target: np.ndarray</span>
<span class="sd">        Data from which starting values are calculated.</span>
<span class="sd">    max_iter: int</span>
<span class="sd">        Maximum number of iterations.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss: float</span>
<span class="sd">        Loss value.</span>
<span class="sd">    start_values: np.ndarray</span>
<span class="sd">        Starting values for each parameter.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Convert target to torch.tensor</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Create Normalizing Flow</span>
    <span class="n">flow_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_spline_flow</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Specify optimizer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">LBFGS</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                      <span class="n">lr</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
                      <span class="n">max_iter</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">max_iter</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span> <span class="mi">50</span><span class="p">]),</span>
                      <span class="n">line_search_fn</span><span class="o">=</span><span class="s2">&quot;strong_wolfe&quot;</span><span class="p">)</span>

    <span class="c1"># Define learning rate scheduler</span>
    <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

    <span class="c1"># Define closure</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">():</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">flow_dist</span><span class="o">.</span><span class="n">clear_cache</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="c1"># Optimize parameters</span>
    <span class="n">loss_vals</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tolerance</span> <span class="o">=</span> <span class="mf">1e-5</span>           <span class="c1"># Tolerance level for loss change</span>
    <span class="n">patience</span> <span class="o">=</span> <span class="mi">5</span>               <span class="c1"># Patience level for loss change</span>
    <span class="n">best_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
    <span class="n">epochs_without_change</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
        <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">loss_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="c1"># Stopping criterion (no improvement in loss)</span>
        <span class="k">if</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">best_loss</span> <span class="o">-</span> <span class="n">tolerance</span><span class="p">:</span>
            <span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">epochs_without_change</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">epochs_without_change</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="n">epochs_without_change</span> <span class="o">&gt;=</span> <span class="n">patience</span><span class="p">:</span>
            <span class="k">break</span>

    <span class="c1"># Get final loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss_vals</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Get start values</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">param</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">start_values</span><span class="p">])</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="c1"># Replace any remaining NaNs or infinity values with 0.5</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">start_values</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.compute_gradients_and_hessians" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute_gradients_and_hessians</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">predt</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Calculates gradients and hessians.</p>
<p>Output gradients and hessians have shape (n_samples*n_outputs, 1).</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.compute_gradients_and_hessians--arguments">Arguments:</h6>
<p>loss: torch.Tensor
    Loss.
predt: torch.Tensor
    List of predicted parameters.
weights: np.ndarray
    Weights.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.compute_gradients_and_hessians--returns">Returns:</h6>
<p>grad: torch.Tensor
    Gradients.
hess: torch.Tensor
    Hessians.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">compute_gradients_and_hessians</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                   <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                   <span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                   <span class="n">weights</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates gradients and hessians.</span>

<span class="sd">    Output gradients and hessians have shape (n_samples*n_outputs, 1).</span>

<span class="sd">    Arguments:</span>
<span class="sd">    ---------</span>
<span class="sd">    loss: torch.Tensor</span>
<span class="sd">        Loss.</span>
<span class="sd">    predt: torch.Tensor</span>
<span class="sd">        List of predicted parameters.</span>
<span class="sd">    weights: np.ndarray</span>
<span class="sd">        Weights.</span>

<span class="sd">    Returns:</span>
<span class="sd">    -------</span>
<span class="sd">    grad: torch.Tensor</span>
<span class="sd">        Gradients.</span>
<span class="sd">    hess: torch.Tensor</span>
<span class="sd">        Hessians.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;nll&quot;</span><span class="p">:</span>
        <span class="c1"># Gradient and Hessian</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">autograd</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">nansum</span><span class="p">(),</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;crps&quot;</span><span class="p">:</span>
        <span class="c1"># Gradient and Hessian</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>

    <span class="c1"># Stabilization of Derivatives</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span> <span class="o">!=</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">hess</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hess</span><span class="p">))]</span>

    <span class="c1"># Reshape</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">hess</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">hess</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="c1"># Weighting</span>
    <span class="n">grad</span> <span class="o">*=</span> <span class="n">weights</span>
    <span class="n">hess</span> <span class="o">*=</span> <span class="n">weights</span>

    <span class="c1"># Reshape</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>
    <span class="n">hess</span> <span class="o">=</span> <span class="n">hess</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.create_spline_flow" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">create_spline_flow</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that constructs a Normalizing Flow.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.create_spline_flow--arguments">Arguments</h6>
<p>input_dim: int
    Input dimension.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.create_spline_flow--returns">Returns</h6>
<p>spline_flow: Transform
    Normalizing Flow.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">create_spline_flow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                       <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                       <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Transform</span><span class="p">:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that constructs a Normalizing Flow.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    input_dim: int</span>
<span class="sd">        Input dimension.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    spline_flow: Transform</span>
<span class="sd">        Normalizing Flow.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Create flow distribution (currently only Normal)</span>
    <span class="n">loc</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_dim</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">input_dim</span><span class="p">)</span>
    <span class="n">flow_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>

    <span class="c1"># Create Spline Transform</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">spline_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flow_transform</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span>
                                           <span class="n">count_bins</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">,</span>
                                           <span class="n">bound</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bound</span><span class="p">,</span>
                                           <span class="n">order</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">order</span><span class="p">)</span>

    <span class="c1"># Create Normalizing Flow</span>
    <span class="n">spline_flow</span> <span class="o">=</span> <span class="n">TransformedDistribution</span><span class="p">(</span><span class="n">flow_dist</span><span class="p">,</span> <span class="p">[</span><span class="n">spline_transform</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_transform</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">spline_flow</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.crps_score" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">crps_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat_dist</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that calculates the Continuous Ranked Probability Score (CRPS) for a given set of predicted samples.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.crps_score--arguments">Arguments</h6>
<p>y: torch.Tensor
    Response variable of shape (n_observations,1).
yhat_dist: torch.Tensor
    Predicted samples of shape (n_samples, n_observations).</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.crps_score--returns">Returns</h6>
<p>crps: torch.Tensor
    CRPS score.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.crps_score--references">References</h6>
<p>Gneiting, Tilmann &amp; Raftery, Adrian. (2007). Strictly Proper Scoring Rules, Prediction, and Estimation.
Journal of the American Statistical Association. 102. 359-378.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.crps_score--source">Source</h6>
<p>https://github.com/elephaint/pgbm/blob/main/pgbm/torch/pgbm_dist.py#L549</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">crps_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">yhat_dist</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that calculates the Continuous Ranked Probability Score (CRPS) for a given set of predicted samples.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    y: torch.Tensor</span>
<span class="sd">        Response variable of shape (n_observations,1).</span>
<span class="sd">    yhat_dist: torch.Tensor</span>
<span class="sd">        Predicted samples of shape (n_samples, n_observations).</span>

<span class="sd">    Returns</span>
<span class="sd">    ---------</span>
<span class="sd">    crps: torch.Tensor</span>
<span class="sd">        CRPS score.</span>

<span class="sd">    References</span>
<span class="sd">    ---------</span>
<span class="sd">    Gneiting, Tilmann &amp; Raftery, Adrian. (2007). Strictly Proper Scoring Rules, Prediction, and Estimation.</span>
<span class="sd">    Journal of the American Statistical Association. 102. 359-378.</span>

<span class="sd">    Source</span>
<span class="sd">    ---------</span>
<span class="sd">    https://github.com/elephaint/pgbm/blob/main/pgbm/torch/pgbm_dist.py#L549</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Get the number of observations</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="n">yhat_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Sort the forecasts in ascending order</span>
    <span class="n">yhat_dist_sorted</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">yhat_dist</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Create temporary tensors</span>
    <span class="n">y_cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">yhat_cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">yhat_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">crps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># Loop over the predicted samples generated per observation</span>
    <span class="k">for</span> <span class="n">yhat</span> <span class="ow">in</span> <span class="n">yhat_dist_sorted</span><span class="p">:</span>
        <span class="n">yhat</span> <span class="o">=</span> <span class="n">yhat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">flag</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_cdf</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">&lt;</span> <span class="n">yhat</span><span class="p">)</span>
        <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat_prev</span><span class="p">)</span> <span class="o">*</span> <span class="n">yhat_cdf</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">((</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">yhat_cdf</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">crps</span> <span class="o">+=</span> <span class="p">(</span><span class="o">~</span><span class="n">flag</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">yhat_prev</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">yhat_cdf</span> <span class="o">-</span> <span class="n">y_cdf</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">y_cdf</span> <span class="o">+=</span> <span class="n">flag</span>
        <span class="n">yhat_cdf</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">n_samples</span>
        <span class="n">yhat_prev</span> <span class="o">=</span> <span class="n">yhat</span>

    <span class="c1"># In case y_cdf == 0 after the loop</span>
    <span class="n">flag</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_cdf</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">crps</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.draw_samples" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">draw_samples</span><span class="p">(</span><span class="n">predt_params</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that draws n_samples from a predicted distribution.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.draw_samples--arguments">Arguments</h6>
<p>predt_params: pd.DataFrame
    pd.DataFrame with predicted distributional parameters.
n_samples: int
    Number of sample to draw from predicted response distribution.
seed: int
    Manual seed.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.draw_samples--returns">Returns</h6>
<p>pred_dist: pd.DataFrame
    DataFrame with n_samples drawn from predicted response distribution.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">draw_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">predt_params</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                 <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                 <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">123</span>
                 <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that draws n_samples from a predicted distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt_params: pd.DataFrame</span>
<span class="sd">        pd.DataFrame with predicted distributional parameters.</span>
<span class="sd">    n_samples: int</span>
<span class="sd">        Number of sample to draw from predicted response distribution.</span>
<span class="sd">    seed: int</span>
<span class="sd">        Manual seed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pred_dist: pd.DataFrame</span>
<span class="sd">        DataFrame with n_samples drawn from predicted response distribution.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Specify Normalizing Flow</span>
    <span class="n">pred_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt_params</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
    <span class="n">flow_dist_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_spline_flow</span><span class="p">(</span><span class="n">pred_params</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># Replace parameters with estimated ones</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">flow_dist_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replace_parameters</span><span class="p">(</span><span class="n">pred_params</span><span class="p">,</span> <span class="n">flow_dist_pred</span><span class="p">)</span>

    <span class="c1"># Draw samples (n_samples, n_obs, *event_shape)</span>
    <span class="n">flow_samples</span> <span class="o">=</span> <span class="n">flow_dist_pred</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="c1"># Flatten event dims, keep (n_samples, n_obs) as outer shape</span>
    <span class="n">flow_samples</span> <span class="o">=</span> <span class="n">flow_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (n_samples, n_obs)</span>
    <span class="n">flow_samples</span> <span class="o">=</span> <span class="n">flow_samples</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># (n_obs, n_samples)</span>

    <span class="n">flow_samples</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">flow_samples</span><span class="p">)</span>
    <span class="n">flow_samples</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;y_sample&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">flow_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
        <span class="n">flow_samples</span> <span class="o">=</span> <span class="n">flow_samples</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">flow_samples</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.flow_select" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">flow_select</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">candidate_flows</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">figure_size</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that selects the most suitable normalizing flow specification among the candidate_flow for the
target variable, based on the NegLogLikelihood (lower is better).</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.flow_select--parameters">Parameters</h6>
<p>target: np.ndarray
    Response variable.
candidate_flows: List
    List of candidate normalizing flow specifications.
max_iter: int
    Maximum number of iterations for the optimization.
plot: bool
    If True, a density plot of the actual and fitted distribution is created.
figure_size: tuple
    Figure size of the density plot.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.flow_select--returns">Returns</h6>
<p>fit_df: pd.DataFrame
    Dataframe with the loss values of the fitted normalizing flow.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">flow_select</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                <span class="n">candidate_flows</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
                <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
                <span class="n">plot</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                <span class="n">figure_size</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that selects the most suitable normalizing flow specification among the candidate_flow for the</span>
<span class="sd">    target variable, based on the NegLogLikelihood (lower is better).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    target: np.ndarray</span>
<span class="sd">        Response variable.</span>
<span class="sd">    candidate_flows: List</span>
<span class="sd">        List of candidate normalizing flow specifications.</span>
<span class="sd">    max_iter: int</span>
<span class="sd">        Maximum number of iterations for the optimization.</span>
<span class="sd">    plot: bool</span>
<span class="sd">        If True, a density plot of the actual and fitted distribution is created.</span>
<span class="sd">    figure_size: tuple</span>
<span class="sd">        Figure size of the density plot.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    fit_df: pd.DataFrame</span>
<span class="sd">        Dataframe with the loss values of the fitted normalizing flow.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">flow_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">total_iterations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">candidate_flows</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">total_iterations</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Fitting candidate normalizing flows&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">flow</span> <span class="ow">in</span> <span class="n">candidate_flows</span><span class="p">:</span>
            <span class="n">flow_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">flow</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;&#39;&gt;&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">flow_spec</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;(count_bins: </span><span class="si">{</span><span class="n">flow</span><span class="o">.</span><span class="n">count_bins</span><span class="si">}</span><span class="s2">, order: </span><span class="si">{</span><span class="n">flow</span><span class="o">.</span><span class="n">order</span><span class="si">}</span><span class="s2">)&quot;</span>
            <span class="n">flow_name</span> <span class="o">=</span> <span class="n">flow_name</span> <span class="o">+</span> <span class="n">flow_spec</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting </span><span class="si">{</span><span class="n">flow_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">flow_sel</span> <span class="o">=</span> <span class="n">flow</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">loss</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">flow_sel</span><span class="o">.</span><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">)</span>
                <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
                    <span class="p">{</span><span class="n">flow_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span>
                     <span class="s2">&quot;NormFlow&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">flow_name</span><span class="p">),</span>
                     <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">params</span><span class="p">]</span>
                     <span class="p">}</span>
                <span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error fitting </span><span class="si">{</span><span class="n">flow_sel</span><span class="si">}</span><span class="s2"> NormFlow: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
                    <span class="p">{</span><span class="n">flow_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
                     <span class="s2">&quot;NormFlow&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">flow_sel</span><span class="p">),</span>
                     <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span> <span class="o">*</span> <span class="n">flow_sel</span><span class="o">.</span><span class="n">n_dist_param</span>
                     <span class="p">}</span>
                <span class="p">)</span>
            <span class="n">flow_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fit_df</span><span class="p">)</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting of candidate normalizing flows completed&quot;</span><span class="p">)</span>
        <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">flow_list</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="n">flow_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">flow_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">fit_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;dist_select with plot=True requires &#39;matplotlib&#39; and &#39;seaborn&#39; &quot;</span>
            <span class="s2">&quot;to be installed. Please install the packages to use this feature. &quot;</span>
            <span class="s2">&quot;Installing via pip install lightgbmlss[all_extras] also installs &quot;</span>
            <span class="s2">&quot;the required dependencies.&quot;</span>
        <span class="p">)</span>
        <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;matplotlib&quot;</span><span class="p">,</span> <span class="s2">&quot;seaborn&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

        <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

        <span class="c1"># Select normalizing flow with the lowest loss</span>
        <span class="n">best_flow</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">flow</span> <span class="ow">in</span> <span class="n">candidate_flows</span><span class="p">:</span>
            <span class="n">flow_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">flow</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;&#39;&gt;&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">flow_spec</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;(count_bins: </span><span class="si">{</span><span class="n">flow</span><span class="o">.</span><span class="n">count_bins</span><span class="si">}</span><span class="s2">, order: </span><span class="si">{</span><span class="n">flow</span><span class="o">.</span><span class="n">order</span><span class="si">}</span><span class="s2">)&quot;</span>
            <span class="n">flow_name</span> <span class="o">=</span> <span class="n">flow_name</span> <span class="o">+</span> <span class="n">flow_spec</span>
            <span class="k">if</span> <span class="n">flow_name</span> <span class="o">==</span> <span class="n">best_flow</span><span class="p">[</span><span class="s2">&quot;NormFlow&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">best_flow_sel</span> <span class="o">=</span> <span class="n">flow</span>
                <span class="k">break</span>

        <span class="c1"># Draw samples from distribution</span>
        <span class="n">flow_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">best_flow</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">flow_dist_sel</span> <span class="o">=</span> <span class="n">best_flow_sel</span><span class="o">.</span><span class="n">create_spline_flow</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">flow_dist_sel</span> <span class="o">=</span> <span class="n">best_flow_sel</span><span class="o">.</span><span class="n">replace_parameters</span><span class="p">(</span><span class="n">flow_params</span><span class="p">,</span> <span class="n">flow_dist_sel</span><span class="p">)</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="mi">10000</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">&gt;</span> <span class="mi">500000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
        <span class="n">flow_samples</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">flow_dist_sel</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>

        <span class="c1"># Plot actual and fitted distribution</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figure_size</span><span class="p">)</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Actual&quot;</span><span class="p">)</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">flow_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Best-Fit: </span><span class="si">{</span><span class="n">best_flow</span><span class="p">[</span><span class="s1">&#39;NormFlow&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Actual vs. Best-Fit Density&quot;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s2">&quot;bold&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="n">fit_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">,</span> <span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fit_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.get_params_loss" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that returns the predicted parameters and the loss.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.get_params_loss--arguments">Arguments</h6>
<p>predt: np.ndarray
    Predicted values.
target: torch.Tensor
    Target values.
start_values: List
    Starting values for each parameter.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.get_params_loss--returns">Returns</h6>
<p>predt: torch.Tensor
    Predicted parameters.
loss: torch.Tensor
    Loss value.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_params_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                    <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">start_values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
                    <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that returns the predicted parameters and the loss.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: np.ndarray</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    target: torch.Tensor</span>
<span class="sd">        Target values.</span>
<span class="sd">    start_values: List</span>
<span class="sd">        Starting values for each parameter.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.Tensor</span>
<span class="sd">        Predicted parameters.</span>
<span class="sd">    loss: torch.Tensor</span>
<span class="sd">        Loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Reshape Target</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Predicted Parameters</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">predt</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>

    <span class="c1"># Replace NaNs and infinity values with unconditional start values</span>
    <span class="n">nan_inf_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span>
    <span class="n">predt</span><span class="p">[</span><span class="n">nan_inf_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_mask</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Convert to torch.tensor</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Specify Normalizing Flow</span>
    <span class="n">flow_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_spline_flow</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># Replace parameters with estimated ones</span>
    <span class="n">params</span><span class="p">,</span> <span class="n">flow_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replace_parameters</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">flow_dist</span><span class="p">)</span>

    <span class="c1"># Calculate loss</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;nll&quot;</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;crps&quot;</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">flow_dist</span><span class="o">.</span><span class="n">rsample</span><span class="p">((</span><span class="mi">30</span><span class="p">,))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">crps_score</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dist_samples</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">params</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.metric_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">metric_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that evaluates the predictions using the specified loss function.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.metric_fn--arguments">Arguments</h6>
<p>predt: np.ndarray
    Predicted values.
data: lgb.Dataset
    Data used for training.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.metric_fn--returns">Returns</h6>
<p>name: str
    Name of the evaluation metric.
loss: float
    Loss value.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">metric_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that evaluates the predictions using the specified loss function.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: np.ndarray</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    data: lgb.Dataset</span>
<span class="sd">        Data used for training.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    name: str</span>
<span class="sd">        Name of the evaluation metric.</span>
<span class="sd">    loss: float</span>
<span class="sd">        Loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Target</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">n_obs</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Start values (needed to replace NaNs in predt)</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_init_score</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="c1"># Calculate loss</span>
    <span class="n">is_higher_better</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">/</span> <span class="n">n_obs</span><span class="p">,</span> <span class="n">is_higher_better</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.objective_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">objective_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function to estimate gradients and hessians of normalizing flow parameters.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.objective_fn--arguments">Arguments</h6>
<p>predt: np.ndarray
    Predicted values.
data: lgb.Dataset
    Data used for training.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.objective_fn--returns">Returns</h6>
<p>grad: np.ndarray
    Gradient.
hess: np.ndarray
    Hessian.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">objective_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function to estimate gradients and hessians of normalizing flow parameters.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: np.ndarray</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    data: lgb.Dataset</span>
<span class="sd">        Data used for training.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    grad: np.ndarray</span>
<span class="sd">        Gradient.</span>
<span class="sd">    hess: np.ndarray</span>
<span class="sd">        Hessian.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Target</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Weights</span>
    <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Use 1 as weight if no weights are specified</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Start values (needed to replace NaNs in predt)</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_init_score</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="c1"># Calculate gradients and hessians</span>
    <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">)</span>
    <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_gradients_and_hessians</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">predt</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.predict_dist" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">predict_dist</span><span class="p">(</span><span class="n">booster</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">pred_type</span><span class="o">=</span><span class="s1">&#39;parameters&#39;</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">quantiles</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that predicts from the trained model.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.predict_dist--arguments">Arguments</h6>
<p>booster : lgb.Booster
    Trained model.
start_values : np.ndarray
    Starting values for each distributional parameter.
data : pd.DataFrame
    Data to predict from.
pred_type : str
    Type of prediction:
    - "samples" draws n_samples from the predicted distribution.
    - "quantiles" calculates the quantiles from the predicted distribution.
    - "parameters" returns the predicted distributional parameters.
    - "expectiles" returns the predicted expectiles.
n_samples : int
    Number of samples to draw from the predicted distribution.
quantiles : List[float]
    List of quantiles to calculate from the predicted distribution.
seed : int
    Seed for random number generator used to draw samples from the predicted distribution.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.predict_dist--returns">Returns</h6>
<p>pred : pd.DataFrame
    Predictions.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">predict_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">booster</span><span class="p">:</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Booster</span><span class="p">,</span>
                 <span class="n">data</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                 <span class="n">start_values</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                 <span class="n">pred_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;parameters&quot;</span><span class="p">,</span>
                 <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                 <span class="n">quantiles</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
                 <span class="n">seed</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="mi">123</span>
                 <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that predicts from the trained model.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    booster : lgb.Booster</span>
<span class="sd">        Trained model.</span>
<span class="sd">    start_values : np.ndarray</span>
<span class="sd">        Starting values for each distributional parameter.</span>
<span class="sd">    data : pd.DataFrame</span>
<span class="sd">        Data to predict from.</span>
<span class="sd">    pred_type : str</span>
<span class="sd">        Type of prediction:</span>
<span class="sd">        - &quot;samples&quot; draws n_samples from the predicted distribution.</span>
<span class="sd">        - &quot;quantiles&quot; calculates the quantiles from the predicted distribution.</span>
<span class="sd">        - &quot;parameters&quot; returns the predicted distributional parameters.</span>
<span class="sd">        - &quot;expectiles&quot; returns the predicted expectiles.</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples to draw from the predicted distribution.</span>
<span class="sd">    quantiles : List[float]</span>
<span class="sd">        List of quantiles to calculate from the predicted distribution.</span>
<span class="sd">    seed : int</span>
<span class="sd">        Seed for random number generator used to draw samples from the predicted distribution.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pred : pd.DataFrame</span>
<span class="sd">        Predictions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Predict raw scores</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
        <span class="n">booster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">raw_score</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>

    <span class="c1"># Set init_score as starting point for each distributional parameter.</span>
    <span class="n">init_score_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">start_values</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="p">)</span>

    <span class="c1"># The predictions don&#39;t include the init_score specified in creating the train data. Hence, it needs to be</span>
    <span class="c1"># added manually.</span>
    <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
            <span class="p">[</span><span class="n">predt</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">init_score_pred</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)],</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="n">dist_params_predt</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>

    <span class="c1"># Draw samples from predicted response distribution</span>
    <span class="n">pred_samples_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">predt_params</span><span class="o">=</span><span class="n">dist_params_predt</span><span class="p">,</span>
                                        <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;parameters&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">dist_params_predt</span>

    <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;samples&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pred_samples_df</span>

    <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;quantiles&quot;</span><span class="p">:</span>
        <span class="c1"># Calculate quantiles from predicted response distribution</span>
        <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_samples_df</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">quantiles</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;quant_&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">quantiles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">quantiles</span><span class="p">))]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
            <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pred_quant_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.replace_parameters" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">replace_parameters</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">flow_dist</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Replace parameters with estimated ones.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.replace_parameters--arguments">Arguments</h6>
<p>params: torch.Tensor
    Estimated parameters.
flow_dist: Transform
    Normalizing Flow.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.replace_parameters--returns">Returns</h6>
<p>params_list: List
    List of estimated parameters.
flow_dist: Transform
    Normalizing Flow with estimated parameters.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">replace_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                       <span class="n">params</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">flow_dist</span><span class="p">:</span> <span class="n">Transform</span><span class="p">,</span>
                       <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">Transform</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace parameters with estimated ones.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    params: torch.Tensor</span>
<span class="sd">        Estimated parameters.</span>
<span class="sd">    flow_dist: Transform</span>
<span class="sd">        Normalizing Flow.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    params_list: List</span>
<span class="sd">        List of estimated parameters.</span>
<span class="sd">    flow_dist: Transform</span>
<span class="sd">        Normalizing Flow with estimated parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Split parameters into list</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">order</span> <span class="o">==</span> <span class="s2">&quot;quadratic&quot;</span><span class="p">:</span>
        <span class="n">params_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">params</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">order</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
        <span class="n">params_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">params</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">],</span>
            <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Replace parameters</span>
    <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">new_value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">params_list</span><span class="p">):</span>
        <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">new_value</span>

    <span class="c1"># Get parameters (including require_grad=True)</span>
    <span class="n">params_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">params_list</span><span class="p">,</span> <span class="n">flow_dist</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.stabilize_derivative" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s1">&#39;MAD&#39;</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that stabilizes Gradients and Hessians.</p>
<p>Since parameters are estimated by optimizing Gradients and Hessians, it is important that these are comparable
in magnitude for all parameters. Due to imbalances regarding the ranges, the estimation might become unstable
so that it does not converge (or converge very slowly) to the optimal solution. Another way to improve
convergence might be to standardize the response variable. This is especially useful if the range of the
response differs strongly from the range of the Gradients and Hessians. Both, the stabilization and the
standardization of the response are not always advised but need to be carefully considered.</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.stabilize_derivative--source">Source</h6>
<p>https://github.com/boost-R/gamboostLSS/blob/7792951d2984f289ed7e530befa42a2a4cb04d1d/R/helpers.R#L173</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.stabilize_derivative--arguments">Arguments</h6>
<p>input_der : torch.Tensor
    Input derivative, either Gradient or Hessian.
type: str
    Stabilization method. Can be either "None", "MAD" or "L2".</p>
<h6 id="lightgbmlss.distributions.flow_utils.NormalizingFlowClass.stabilize_derivative--returns">Returns</h6>
<p>stab_der : torch.Tensor
    Stabilized Gradient or Hessian.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">stabilize_derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_der</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;MAD&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that stabilizes Gradients and Hessians.</span>

<span class="sd">    Since parameters are estimated by optimizing Gradients and Hessians, it is important that these are comparable</span>
<span class="sd">    in magnitude for all parameters. Due to imbalances regarding the ranges, the estimation might become unstable</span>
<span class="sd">    so that it does not converge (or converge very slowly) to the optimal solution. Another way to improve</span>
<span class="sd">    convergence might be to standardize the response variable. This is especially useful if the range of the</span>
<span class="sd">    response differs strongly from the range of the Gradients and Hessians. Both, the stabilization and the</span>
<span class="sd">    standardization of the response are not always advised but need to be carefully considered.</span>

<span class="sd">    Source</span>
<span class="sd">    ---------</span>
<span class="sd">    https://github.com/boost-R/gamboostLSS/blob/7792951d2984f289ed7e530befa42a2a4cb04d1d/R/helpers.R#L173</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    input_der : torch.Tensor</span>
<span class="sd">        Input derivative, either Gradient or Hessian.</span>
<span class="sd">    type: str</span>
<span class="sd">        Stabilization method. Can be either &quot;None&quot;, &quot;MAD&quot; or &quot;L2&quot;.</span>

<span class="sd">    Returns</span>
<span class="sd">    ---------</span>
<span class="sd">    stab_der : torch.Tensor</span>
<span class="sd">        Stabilized Gradient or Hessian.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;MAD&quot;</span><span class="p">:</span>
        <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">input_der</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
        <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;L2&quot;</span><span class="p">:</span>
        <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
        <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
        <span class="n">stab_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>

    <span class="k">return</span> <span class="n">stab_der</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.mixture_distribution_utils" class="doc doc-heading">
            <code>mixture_distribution_utils</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass" class="doc doc-heading">
            <code>MixtureDistributionClass</code>


</h4>


    <div class="doc doc-contents ">



        <p>Generic class that contains general functions for mixed-density distributions.</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass--arguments">Arguments</h6>
<p>distribution: torch.distributions.Distribution
    PyTorch Distribution class.
M: int
    Number of components in the mixture distribution.
temperature: float
    Temperature for the Gumbel-Softmax distribution.
hessian_mode: str
    Mode for computing the Hessian. Must be one of the following:</p>
<pre><code>    - "individual": Each parameter is treated as a separate tensor. As a result, when the Hessian is calculated
    for each gradient element, this corresponds to the second derivative with respect to that specific tensor
    element only. This means the resulting Hessians capture the curvature of the loss w.r.t. each individual
    parameter. This is usually more runtime intensive, but can also be more accurate.

    - "grouped": Each parameter is a tensor containing all values for a specific parameter type,
    e.g., loc, scale, or mixture probabilities for a Gaussian Mixture. When computing the Hessian for each
    gradient element, the Hessian matrix for all the values in the respective tensor are calculated together.
    The resulting Hessians capture the curvature of the loss w.r.t. the entire parameter type tensor. This is
    usually less runtime intensive, but can be less accurate.
</code></pre>
<p>univariate: bool
    Whether the distribution is univariate or multivariate.
discrete: bool
    Whether the support of the distribution is discrete or continuous.
n_dist_param: int
    Number of distributional parameters.
stabilization: str
    Stabilization method.
param_dict: Dict[str, Any]
    Dictionary that maps distributional parameters to their response scale.
distribution_arg_names: List
    List of distributional parameter names.
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood).
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/mixture_distribution_utils.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MixtureDistributionClass</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generic class that contains general functions for mixed-density distributions.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    distribution: torch.distributions.Distribution</span>
<span class="sd">        PyTorch Distribution class.</span>
<span class="sd">    M: int</span>
<span class="sd">        Number of components in the mixture distribution.</span>
<span class="sd">    temperature: float</span>
<span class="sd">        Temperature for the Gumbel-Softmax distribution.</span>
<span class="sd">    hessian_mode: str</span>
<span class="sd">        Mode for computing the Hessian. Must be one of the following:</span>

<span class="sd">            - &quot;individual&quot;: Each parameter is treated as a separate tensor. As a result, when the Hessian is calculated</span>
<span class="sd">            for each gradient element, this corresponds to the second derivative with respect to that specific tensor</span>
<span class="sd">            element only. This means the resulting Hessians capture the curvature of the loss w.r.t. each individual</span>
<span class="sd">            parameter. This is usually more runtime intensive, but can also be more accurate.</span>

<span class="sd">            - &quot;grouped&quot;: Each parameter is a tensor containing all values for a specific parameter type,</span>
<span class="sd">            e.g., loc, scale, or mixture probabilities for a Gaussian Mixture. When computing the Hessian for each</span>
<span class="sd">            gradient element, the Hessian matrix for all the values in the respective tensor are calculated together.</span>
<span class="sd">            The resulting Hessians capture the curvature of the loss w.r.t. the entire parameter type tensor. This is</span>
<span class="sd">            usually less runtime intensive, but can be less accurate.</span>
<span class="sd">    univariate: bool</span>
<span class="sd">        Whether the distribution is univariate or multivariate.</span>
<span class="sd">    discrete: bool</span>
<span class="sd">        Whether the support of the distribution is discrete or continuous.</span>
<span class="sd">    n_dist_param: int</span>
<span class="sd">        Number of distributional parameters.</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method.</span>
<span class="sd">    param_dict: Dict[str, Any]</span>
<span class="sd">        Dictionary that maps distributional parameters to their response scale.</span>
<span class="sd">    distribution_arg_names: List</span>
<span class="sd">        List of distributional parameter names.</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood).</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">distribution</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">M</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                 <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">hessian_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;individual&quot;</span><span class="p">,</span>
                 <span class="n">univariate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                 <span class="n">discrete</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">n_dist_param</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">param_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">distribution_arg_names</span><span class="p">:</span> <span class="n">List</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span> <span class="o">=</span> <span class="n">distribution</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">M</span> <span class="o">=</span> <span class="n">M</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="o">=</span> <span class="n">temperature</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hessian_mode</span> <span class="o">=</span> <span class="n">hessian_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">univariate</span> <span class="o">=</span> <span class="n">univariate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span> <span class="o">=</span> <span class="n">discrete</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span> <span class="o">=</span> <span class="n">n_dist_param</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span> <span class="o">=</span> <span class="n">stabilization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span> <span class="o">=</span> <span class="n">param_dict</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span> <span class="o">=</span> <span class="n">distribution_arg_names</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialize</span> <span class="o">=</span> <span class="n">initialize</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">objective_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function to estimate gradients and hessians of distributional parameters.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt: np.ndarray</span>
<span class="sd">            Predicted values.</span>
<span class="sd">        data: lgb.DMatrix</span>
<span class="sd">            Data used for training.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        grad: np.ndarray</span>
<span class="sd">            Gradient.</span>
<span class="sd">        hess: np.ndarray</span>
<span class="sd">            Hessian.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Target</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1"># Weights</span>
        <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Use 1 as weight if no weights are specified</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Start values (needed to replace NaNs in predt)</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_init_score</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="c1"># Calculate gradients and hessians</span>
        <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_gradients_and_hessians</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">predt</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">metric_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that evaluates the predictions using the negative log-likelihood.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt: np.ndarray</span>
<span class="sd">            Predicted values.</span>
<span class="sd">        data: lgb.Dataset</span>
<span class="sd">            Data used for training.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        name: str</span>
<span class="sd">            Name of the evaluation metric.</span>
<span class="sd">        nll: float</span>
<span class="sd">            Negative log-likelihood.</span>
<span class="sd">        is_higher_better: bool</span>
<span class="sd">            Whether a higher value of the metric is better or not.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Target</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">n_obs</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Start values (needed to replace NaNs in predt)</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_init_score</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="c1"># Calculate loss</span>
        <span class="n">is_higher_better</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">n_obs</span><span class="p">,</span> <span class="n">is_higher_better</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">create_mixture_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                    <span class="n">params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
                                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that creates a mixture distribution.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        params: torch.Tensor</span>
<span class="sd">            Distributional parameters.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        dist: torch.distributions.Distribution</span>
<span class="sd">            Mixture distribution.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Create Mixture Distribution</span>
        <span class="n">mixture_cat</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">mixture_comp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">*</span><span class="n">params</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">mixture_dist</span> <span class="o">=</span> <span class="n">MixtureSameFamily</span><span class="p">(</span><span class="n">mixture_cat</span><span class="p">,</span> <span class="n">mixture_comp</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">mixture_dist</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">loss_fn_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                             <span class="n">params</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that calculates the loss for a given set of distributional parameters. Only used for calculating</span>
<span class="sd">        the loss for the start values.</span>

<span class="sd">        Parameter</span>
<span class="sd">        ---------</span>
<span class="sd">        params: torch.Tensor</span>
<span class="sd">            Distributional parameters.</span>
<span class="sd">        target: torch.Tensor</span>
<span class="sd">            Target values.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        loss: torch.Tensor</span>
<span class="sd">            Loss value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Replace NaNs and infinity values with 0.5</span>
        <span class="n">nan_inf_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_idx</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Transform parameters to response scale</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">response_fn</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())]</span>

        <span class="c1"># Specify Distribution and Loss</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_mixture_distribution</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">calculate_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                               <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                               <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
                               <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that calculates the starting values for each distributional parameter.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        target: np.ndarray</span>
<span class="sd">            Data from which starting values are calculated.</span>
<span class="sd">        max_iter: int</span>
<span class="sd">            Maximum number of iterations.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        loss: float</span>
<span class="sd">            Loss value.</span>
<span class="sd">        start_values: np.ndarray</span>
<span class="sd">            Starting values for each distributional parameter.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Convert target to torch.tensor</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

        <span class="c1"># Initialize parameters</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)]</span>

        <span class="c1"># Specify optimizer</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">LBFGS</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">max_iter</span> <span class="o">/</span> <span class="mi">4</span><span class="p">),</span> <span class="mi">20</span><span class="p">]),</span> <span class="n">line_search_fn</span><span class="o">=</span><span class="s2">&quot;strong_wolfe&quot;</span><span class="p">)</span>

        <span class="c1"># Define learning rate scheduler</span>
        <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

        <span class="c1"># Define closure</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">():</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn_start_values</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">loss</span>

        <span class="c1"># Optimize parameters</span>
        <span class="n">loss_vals</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">tolerance</span> <span class="o">=</span> <span class="mf">1e-5</span>
        <span class="n">patience</span> <span class="o">=</span> <span class="mi">5</span>
        <span class="n">best_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
        <span class="n">epochs_without_change</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
            <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="n">loss_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

            <span class="c1"># Stopping criterion (no improvement in loss)</span>
            <span class="k">if</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">best_loss</span> <span class="o">-</span> <span class="n">tolerance</span><span class="p">:</span>
                <span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">epochs_without_change</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">epochs_without_change</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="k">if</span> <span class="n">epochs_without_change</span> <span class="o">&gt;=</span> <span class="n">patience</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="c1"># Get final loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss_vals</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Get start values</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)])</span>

        <span class="c1"># Replace any remaining NaNs or infinity values with 0.5</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">start_values</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_params_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                        <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                        <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">start_values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
                        <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that returns the predicted parameters and the loss.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt: np.ndarray</span>
<span class="sd">            Predicted values.</span>
<span class="sd">        target: torch.Tensor</span>
<span class="sd">            Target values.</span>
<span class="sd">        start_values: List</span>
<span class="sd">            Starting values for each distributional parameter.</span>
<span class="sd">        requires_grad: bool</span>
<span class="sd">            Whether to add to the computational graph or not.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        predt: torch.Tensor</span>
<span class="sd">            Predicted parameters.</span>
<span class="sd">        loss: torch.Tensor</span>
<span class="sd">            Loss value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Predicted Parameters</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="n">predt</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>

        <span class="c1"># Replace NaNs and infinity values with unconditional start values</span>
        <span class="n">nan_inf_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span>
        <span class="n">predt</span><span class="p">[</span><span class="n">nan_inf_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_mask</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hessian_mode</span> <span class="o">==</span> <span class="s2">&quot;grouped&quot;</span><span class="p">:</span>
            <span class="c1"># Convert to torch.Tensor: splits the parameters into tensors for each parameter-type</span>
            <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># Transform parameters to response scale</span>
            <span class="n">predt_transformed</span> <span class="o">=</span> <span class="p">[</span><span class="n">response_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())]</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Convert to torch.Tensor: splits the parameters into tensors for each parameter individually</span>
            <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># Transform parameters to response scale</span>
            <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="n">max_index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span>
            <span class="n">index_ranges</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span> <span class="o">&gt;=</span> <span class="n">max_index</span><span class="p">:</span>
                    <span class="n">index_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
                    <span class="k">break</span>
                <span class="n">index_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">))</span>

            <span class="n">predt_transformed</span> <span class="o">=</span> <span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="p">[</span><span class="n">key</span><span class="p">](</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">predt</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
                <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">index_ranges</span><span class="p">)</span>
            <span class="p">]</span>

        <span class="c1"># Specify Distribution and Loss</span>
        <span class="n">dist_fit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_mixture_distribution</span><span class="p">(</span><span class="n">predt_transformed</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist_fit</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">draw_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">predt_params</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                     <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                     <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">123</span>
                     <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that draws n_samples from a predicted distribution.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt_params: pd.DataFrame</span>
<span class="sd">            pd.DataFrame with predicted distributional parameters.</span>
<span class="sd">        n_samples: int</span>
<span class="sd">            Number of sample to draw from predicted response distribution.</span>
<span class="sd">        seed: int</span>
<span class="sd">            Manual seed.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        pred_dist: pd.DataFrame</span>
<span class="sd">            DataFrame with n_samples drawn from predicted response distribution.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

        <span class="n">pred_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt_params</span><span class="o">.</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
        <span class="n">pred_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">pred_params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">dist_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_mixture_distribution</span><span class="p">(</span><span class="n">pred_params</span><span class="p">)</span>
        <span class="c1"># sample (n_samples, n_obs, *event_shape)</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_pred</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="c1"># Flatten event dims, keep (n_samples, n_obs) as outer shape</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (n_samples, n_obs)</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_samples</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># (n_obs, n_samples)</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_samples</span><span class="p">)</span>
        <span class="n">dist_samples</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;y_sample&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dist_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
            <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_samples</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">dist_samples</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">booster</span><span class="p">:</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Booster</span><span class="p">,</span>
                     <span class="n">data</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                     <span class="n">start_values</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                     <span class="n">pred_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;parameters&quot;</span><span class="p">,</span>
                     <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                     <span class="n">quantiles</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
                     <span class="n">seed</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="mi">123</span>
                     <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that predicts from the trained model.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        booster : lgb.Booster</span>
<span class="sd">            Trained model.</span>
<span class="sd">        data : pd.DataFrame</span>
<span class="sd">            Data to predict from.</span>
<span class="sd">        start_values : np.ndarray.</span>
<span class="sd">            Starting values for each distributional parameter.</span>
<span class="sd">        pred_type : str</span>
<span class="sd">            Type of prediction:</span>
<span class="sd">            - &quot;samples&quot; draws n_samples from the predicted distribution.</span>
<span class="sd">            - &quot;quantiles&quot; calculates the quantiles from the predicted distribution.</span>
<span class="sd">            - &quot;parameters&quot; returns the predicted distributional parameters.</span>
<span class="sd">        n_samples : int</span>
<span class="sd">            Number of samples to draw from the predicted distribution.</span>
<span class="sd">        quantiles : List[float]</span>
<span class="sd">            List of quantiles to calculate from the predicted distribution.</span>
<span class="sd">        seed : int</span>
<span class="sd">            Seed for random number generator used to draw samples from the predicted distribution.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        pred : pd.DataFrame</span>
<span class="sd">            Predictions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="n">booster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">raw_score</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>

        <span class="c1"># Set init_score as starting point for each distributional parameter.</span>
        <span class="n">init_score_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="n">start_values</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>

        <span class="c1"># The predictions don&#39;t include the init_score specified in creating the train data.</span>
        <span class="c1"># Hence, it needs to be added manually with the corresponding transform for each distributional parameter.</span>
        <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">predt</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">init_score_pred</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
                <span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span>
            <span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>

        <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">response_fn</span><span class="p">(</span><span class="n">dist_params_predt</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
            <span class="p">],</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_params_predt</span><span class="p">)</span>
        <span class="n">dist_params_predt</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span>

        <span class="c1"># Draw samples from predicted response distribution</span>
        <span class="n">pred_samples_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">predt_params</span><span class="o">=</span><span class="n">dist_params_predt</span><span class="p">,</span>
                                            <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                            <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;parameters&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">dist_params_predt</span>

        <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;samples&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pred_samples_df</span>

        <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;quantiles&quot;</span><span class="p">:</span>
            <span class="c1"># Calculate quantiles from predicted response distribution</span>
            <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_samples_df</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">quantiles</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
            <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;quant_&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">quantiles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">quantiles</span><span class="p">))]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
                <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">pred_quant_df</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">compute_gradients_and_hessians</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                       <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                       <span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                       <span class="n">weights</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates gradients and hessians.</span>

<span class="sd">        Output gradients and hessians have shape (n_samples*n_outputs, 1).</span>

<span class="sd">        Arguments:</span>
<span class="sd">        ---------</span>
<span class="sd">        loss: torch.Tensor</span>
<span class="sd">            Loss.</span>
<span class="sd">        predt: torch.Tensor</span>
<span class="sd">            List of predicted parameters.</span>
<span class="sd">        weights: np.ndarray</span>
<span class="sd">            Weights.</span>

<span class="sd">        Returns:</span>
<span class="sd">        -------</span>
<span class="sd">        grad: torch.Tensor</span>
<span class="sd">            Gradients.</span>
<span class="sd">        hess: torch.Tensor</span>
<span class="sd">            Hessians.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Gradient and Hessian</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">autograd</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">nansum</span><span class="p">(),</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>

        <span class="c1"># Stabilization of Derivatives</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span> <span class="o">!=</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
            <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">hess</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hess</span><span class="p">))]</span>

        <span class="c1"># Reshape</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">hess</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="c1"># Weighting</span>
        <span class="n">grad</span> <span class="o">*=</span> <span class="n">weights</span>
        <span class="n">hess</span> <span class="o">*=</span> <span class="n">weights</span>

        <span class="c1"># Reshape</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="n">hess</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">stabilize_derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_der</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;MAD&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that stabilizes Gradients and Hessians.</span>

<span class="sd">        As LightGBMLSS updates the parameter estimates by optimizing Gradients and Hessians, it is important</span>
<span class="sd">        that these are comparable in magnitude for all distributional parameters. Due to imbalances regarding the ranges,</span>
<span class="sd">        the estimation might become unstable so that it does not converge (or converge very slowly) to the optimal solution.</span>
<span class="sd">        Another way to improve convergence might be to standardize the response variable. This is especially useful if the</span>
<span class="sd">        range of the response differs strongly from the range of the Gradients and Hessians. Both, the stabilization and</span>
<span class="sd">        the standardization of the response are not always advised but need to be carefully considered.</span>
<span class="sd">        Source: https://github.com/boost-R/gamboostLSS/blob/7792951d2984f289ed7e530befa42a2a4cb04d1d/R/helpers.R#L173</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_der : torch.Tensor</span>
<span class="sd">            Input derivative, either Gradient or Hessian.</span>
<span class="sd">        type: str</span>
<span class="sd">            Stabilization method. Can be either &quot;None&quot;, &quot;MAD&quot; or &quot;L2&quot;.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        stab_der : torch.Tensor</span>
<span class="sd">            Stabilized Gradient or Hessian.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;MAD&quot;</span><span class="p">:</span>
            <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span>
                                         <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)),</span>
                                         <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)),</span>
                                         <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">))</span>
                                         <span class="p">)</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">input_der</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
            <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;L2&quot;</span><span class="p">:</span>
            <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span>
                                         <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)),</span>
                                         <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)),</span>
                                         <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">))</span>
                                         <span class="p">)</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
            <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
            <span class="n">stab_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span>
                                        <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)),</span>
                                        <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)),</span>
                                        <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">))</span>
                                        <span class="p">)</span>

        <span class="k">return</span> <span class="n">stab_der</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">dist_select</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                    <span class="n">candidate_distributions</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
                    <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
                    <span class="n">plot</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="n">figure_size</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that selects the most suitable distribution among the candidate_distributions for the target variable,</span>
<span class="sd">        based on the NegLogLikelihood (lower is better).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        target: np.ndarray</span>
<span class="sd">            Response variable.</span>
<span class="sd">        candidate_distributions: List</span>
<span class="sd">            List of candidate distributions.</span>
<span class="sd">        max_iter: int</span>
<span class="sd">            Maximum number of iterations for the optimization.</span>
<span class="sd">        plot: bool</span>
<span class="sd">            If True, a density plot of the actual and fitted distribution is created.</span>
<span class="sd">        figure_size: tuple</span>
<span class="sd">            Figure size of the density plot.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        fit_df: pd.DataFrame</span>
<span class="sd">            Dataframe with the loss values of the fitted candidate distributions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dist_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">total_iterations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">total_iterations</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Fitting candidate distributions&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">)):</span>
                <span class="n">dist_name</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
                <span class="n">n_mix</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">M</span>
                <span class="n">tau</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">temperature</span>
                <span class="n">dist_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Mixture(</span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2">, tau=</span><span class="si">{</span><span class="n">tau</span><span class="si">}</span><span class="s2">, M=</span><span class="si">{</span><span class="n">n_mix</span><span class="si">}</span><span class="s2">)&quot;</span>
                <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting </span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2"> distribution&quot;</span><span class="p">)</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">loss</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">)</span>
                    <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
                        <span class="p">{</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span>
                         <span class="s2">&quot;distribution&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">dist_name</span><span class="p">),</span>
                         <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">params</span><span class="p">],</span>
                         <span class="s2">&quot;dist_pos&quot;</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span>
                         <span class="s2">&quot;M&quot;</span><span class="p">:</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">M</span>
                         <span class="p">}</span>
                    <span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error fitting </span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2"> distribution: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
                        <span class="p">{</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
                         <span class="s2">&quot;distribution&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">dist_name</span><span class="p">),</span>
                         <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">,</span>
                         <span class="s2">&quot;dist_pos&quot;</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span>
                         <span class="s2">&quot;M&quot;</span><span class="p">:</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">M</span>
                         <span class="p">}</span>
                    <span class="p">)</span>
                <span class="n">dist_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fit_df</span><span class="p">)</span>
                <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting of candidate distributions completed&quot;</span><span class="p">)</span>
            <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">dist_list</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="n">fit_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;dist_select with plot=True requires &#39;matplotlib&#39; and &#39;seaborn&#39; &quot;</span>
                <span class="s2">&quot;to be installed. Please install the packages to use this feature. &quot;</span>
                <span class="s2">&quot;Installing via pip install lightgbmlss[all_extras] also installs &quot;</span>
                <span class="s2">&quot;the required dependencies.&quot;</span>
            <span class="p">)</span>
            <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;matplotlib&quot;</span><span class="p">,</span> <span class="s2">&quot;seaborn&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

            <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

            <span class="c1"># Select best distribution</span>
            <span class="n">best_dist</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="mi">0</span><span class="p">]]</span>
            <span class="n">best_dist_pos</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;dist_pos&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">best_dist_sel</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">best_dist_pos</span><span class="p">]</span>
            <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">fitted_params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">response_fun</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
                <span class="p">],</span>
                <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">fitted_params</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">fitted_params</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">)</span>
            <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="mi">10000</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
            <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">&gt;</span> <span class="mi">500000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
            <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">fitted_params</span><span class="p">,</span>
                                                      <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                                      <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>

            <span class="c1"># Plot actual and fitted distribution</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figure_size</span><span class="p">)</span>
            <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Actual&quot;</span><span class="p">)</span>
            <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">dist_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Best-Fit: </span><span class="si">{</span><span class="n">best_dist</span><span class="p">[</span><span class="s1">&#39;distribution&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Actual vs. Best-Fit Density&quot;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s2">&quot;bold&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

        <span class="n">fit_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">,</span> <span class="s2">&quot;params&quot;</span><span class="p">,</span> <span class="s2">&quot;dist_pos&quot;</span><span class="p">,</span> <span class="s2">&quot;M&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">fit_df</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.calculate_start_values" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that calculates the starting values for each distributional parameter.</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.calculate_start_values--arguments">Arguments</h6>
<p>target: np.ndarray
    Data from which starting values are calculated.
max_iter: int
    Maximum number of iterations.</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.calculate_start_values--returns">Returns</h6>
<p>loss: float
    Loss value.
start_values: np.ndarray
    Starting values for each distributional parameter.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">calculate_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                           <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                           <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
                           <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that calculates the starting values for each distributional parameter.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    target: np.ndarray</span>
<span class="sd">        Data from which starting values are calculated.</span>
<span class="sd">    max_iter: int</span>
<span class="sd">        Maximum number of iterations.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss: float</span>
<span class="sd">        Loss value.</span>
<span class="sd">    start_values: np.ndarray</span>
<span class="sd">        Starting values for each distributional parameter.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Convert target to torch.tensor</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

    <span class="c1"># Initialize parameters</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)]</span>

    <span class="c1"># Specify optimizer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">LBFGS</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">max_iter</span> <span class="o">/</span> <span class="mi">4</span><span class="p">),</span> <span class="mi">20</span><span class="p">]),</span> <span class="n">line_search_fn</span><span class="o">=</span><span class="s2">&quot;strong_wolfe&quot;</span><span class="p">)</span>

    <span class="c1"># Define learning rate scheduler</span>
    <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="c1"># Define closure</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">():</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn_start_values</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="c1"># Optimize parameters</span>
    <span class="n">loss_vals</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tolerance</span> <span class="o">=</span> <span class="mf">1e-5</span>
    <span class="n">patience</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">best_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
    <span class="n">epochs_without_change</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
        <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">loss_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="c1"># Stopping criterion (no improvement in loss)</span>
        <span class="k">if</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">best_loss</span> <span class="o">-</span> <span class="n">tolerance</span><span class="p">:</span>
            <span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">epochs_without_change</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">epochs_without_change</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="n">epochs_without_change</span> <span class="o">&gt;=</span> <span class="n">patience</span><span class="p">:</span>
            <span class="k">break</span>

    <span class="c1"># Get final loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss_vals</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Get start values</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)])</span>

    <span class="c1"># Replace any remaining NaNs or infinity values with 0.5</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">start_values</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.compute_gradients_and_hessians" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute_gradients_and_hessians</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">predt</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Calculates gradients and hessians.</p>
<p>Output gradients and hessians have shape (n_samples*n_outputs, 1).</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.compute_gradients_and_hessians--arguments">Arguments:</h6>
<p>loss: torch.Tensor
    Loss.
predt: torch.Tensor
    List of predicted parameters.
weights: np.ndarray
    Weights.</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.compute_gradients_and_hessians--returns">Returns:</h6>
<p>grad: torch.Tensor
    Gradients.
hess: torch.Tensor
    Hessians.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">compute_gradients_and_hessians</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                   <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                   <span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                   <span class="n">weights</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates gradients and hessians.</span>

<span class="sd">    Output gradients and hessians have shape (n_samples*n_outputs, 1).</span>

<span class="sd">    Arguments:</span>
<span class="sd">    ---------</span>
<span class="sd">    loss: torch.Tensor</span>
<span class="sd">        Loss.</span>
<span class="sd">    predt: torch.Tensor</span>
<span class="sd">        List of predicted parameters.</span>
<span class="sd">    weights: np.ndarray</span>
<span class="sd">        Weights.</span>

<span class="sd">    Returns:</span>
<span class="sd">    -------</span>
<span class="sd">    grad: torch.Tensor</span>
<span class="sd">        Gradients.</span>
<span class="sd">    hess: torch.Tensor</span>
<span class="sd">        Hessians.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Gradient and Hessian</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">autograd</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">nansum</span><span class="p">(),</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>

    <span class="c1"># Stabilization of Derivatives</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span> <span class="o">!=</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">hess</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hess</span><span class="p">))]</span>

    <span class="c1"># Reshape</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">hess</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">hess</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="c1"># Weighting</span>
    <span class="n">grad</span> <span class="o">*=</span> <span class="n">weights</span>
    <span class="n">hess</span> <span class="o">*=</span> <span class="n">weights</span>

    <span class="c1"># Reshape</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>
    <span class="n">hess</span> <span class="o">=</span> <span class="n">hess</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.create_mixture_distribution" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">create_mixture_distribution</span><span class="p">(</span><span class="n">params</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that creates a mixture distribution.</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.create_mixture_distribution--arguments">Arguments</h6>
<p>params: torch.Tensor
    Distributional parameters.</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.create_mixture_distribution--returns">Returns</h6>
<p>dist: torch.distributions.Distribution
    Mixture distribution.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">create_mixture_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                <span class="n">params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
                                <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that creates a mixture distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    params: torch.Tensor</span>
<span class="sd">        Distributional parameters.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dist: torch.distributions.Distribution</span>
<span class="sd">        Mixture distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Create Mixture Distribution</span>
    <span class="n">mixture_cat</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">mixture_comp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">*</span><span class="n">params</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">mixture_dist</span> <span class="o">=</span> <span class="n">MixtureSameFamily</span><span class="p">(</span><span class="n">mixture_cat</span><span class="p">,</span> <span class="n">mixture_comp</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">mixture_dist</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.dist_select" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">dist_select</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">candidate_distributions</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">figure_size</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that selects the most suitable distribution among the candidate_distributions for the target variable,
based on the NegLogLikelihood (lower is better).</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.dist_select--parameters">Parameters</h6>
<p>target: np.ndarray
    Response variable.
candidate_distributions: List
    List of candidate distributions.
max_iter: int
    Maximum number of iterations for the optimization.
plot: bool
    If True, a density plot of the actual and fitted distribution is created.
figure_size: tuple
    Figure size of the density plot.</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.dist_select--returns">Returns</h6>
<p>fit_df: pd.DataFrame
    Dataframe with the loss values of the fitted candidate distributions.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">dist_select</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                <span class="n">candidate_distributions</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
                <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
                <span class="n">plot</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                <span class="n">figure_size</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that selects the most suitable distribution among the candidate_distributions for the target variable,</span>
<span class="sd">    based on the NegLogLikelihood (lower is better).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    target: np.ndarray</span>
<span class="sd">        Response variable.</span>
<span class="sd">    candidate_distributions: List</span>
<span class="sd">        List of candidate distributions.</span>
<span class="sd">    max_iter: int</span>
<span class="sd">        Maximum number of iterations for the optimization.</span>
<span class="sd">    plot: bool</span>
<span class="sd">        If True, a density plot of the actual and fitted distribution is created.</span>
<span class="sd">    figure_size: tuple</span>
<span class="sd">        Figure size of the density plot.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    fit_df: pd.DataFrame</span>
<span class="sd">        Dataframe with the loss values of the fitted candidate distributions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dist_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">total_iterations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">total_iterations</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Fitting candidate distributions&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">)):</span>
            <span class="n">dist_name</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
            <span class="n">n_mix</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">M</span>
            <span class="n">tau</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">temperature</span>
            <span class="n">dist_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Mixture(</span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2">, tau=</span><span class="si">{</span><span class="n">tau</span><span class="si">}</span><span class="s2">, M=</span><span class="si">{</span><span class="n">n_mix</span><span class="si">}</span><span class="s2">)&quot;</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting </span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2"> distribution&quot;</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">loss</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">)</span>
                <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
                    <span class="p">{</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span>
                     <span class="s2">&quot;distribution&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">dist_name</span><span class="p">),</span>
                     <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">params</span><span class="p">],</span>
                     <span class="s2">&quot;dist_pos&quot;</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span>
                     <span class="s2">&quot;M&quot;</span><span class="p">:</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">M</span>
                     <span class="p">}</span>
                <span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error fitting </span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2"> distribution: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
                    <span class="p">{</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
                     <span class="s2">&quot;distribution&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">dist_name</span><span class="p">),</span>
                     <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">,</span>
                     <span class="s2">&quot;dist_pos&quot;</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span>
                     <span class="s2">&quot;M&quot;</span><span class="p">:</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">M</span>
                     <span class="p">}</span>
                <span class="p">)</span>
            <span class="n">dist_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fit_df</span><span class="p">)</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting of candidate distributions completed&quot;</span><span class="p">)</span>
        <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">dist_list</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">fit_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;dist_select with plot=True requires &#39;matplotlib&#39; and &#39;seaborn&#39; &quot;</span>
            <span class="s2">&quot;to be installed. Please install the packages to use this feature. &quot;</span>
            <span class="s2">&quot;Installing via pip install lightgbmlss[all_extras] also installs &quot;</span>
            <span class="s2">&quot;the required dependencies.&quot;</span>
        <span class="p">)</span>
        <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;matplotlib&quot;</span><span class="p">,</span> <span class="s2">&quot;seaborn&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

        <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

        <span class="c1"># Select best distribution</span>
        <span class="n">best_dist</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">best_dist_pos</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;dist_pos&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">best_dist_sel</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">best_dist_pos</span><span class="p">]</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">fitted_params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">response_fun</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
            <span class="p">],</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">fitted_params</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">fitted_params</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">)</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="mi">10000</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">&gt;</span> <span class="mi">500000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">fitted_params</span><span class="p">,</span>
                                                  <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                                  <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>

        <span class="c1"># Plot actual and fitted distribution</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figure_size</span><span class="p">)</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Actual&quot;</span><span class="p">)</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">dist_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Best-Fit: </span><span class="si">{</span><span class="n">best_dist</span><span class="p">[</span><span class="s1">&#39;distribution&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Actual vs. Best-Fit Density&quot;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s2">&quot;bold&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="n">fit_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">,</span> <span class="s2">&quot;params&quot;</span><span class="p">,</span> <span class="s2">&quot;dist_pos&quot;</span><span class="p">,</span> <span class="s2">&quot;M&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fit_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.draw_samples" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">draw_samples</span><span class="p">(</span><span class="n">predt_params</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that draws n_samples from a predicted distribution.</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.draw_samples--arguments">Arguments</h6>
<p>predt_params: pd.DataFrame
    pd.DataFrame with predicted distributional parameters.
n_samples: int
    Number of sample to draw from predicted response distribution.
seed: int
    Manual seed.</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.draw_samples--returns">Returns</h6>
<p>pred_dist: pd.DataFrame
    DataFrame with n_samples drawn from predicted response distribution.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">draw_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">predt_params</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                 <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                 <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">123</span>
                 <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that draws n_samples from a predicted distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt_params: pd.DataFrame</span>
<span class="sd">        pd.DataFrame with predicted distributional parameters.</span>
<span class="sd">    n_samples: int</span>
<span class="sd">        Number of sample to draw from predicted response distribution.</span>
<span class="sd">    seed: int</span>
<span class="sd">        Manual seed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pred_dist: pd.DataFrame</span>
<span class="sd">        DataFrame with n_samples drawn from predicted response distribution.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">pred_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt_params</span><span class="o">.</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
    <span class="n">pred_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">pred_params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">dist_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_mixture_distribution</span><span class="p">(</span><span class="n">pred_params</span><span class="p">)</span>
    <span class="c1"># sample (n_samples, n_obs, *event_shape)</span>
    <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_pred</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="c1"># Flatten event dims, keep (n_samples, n_obs) as outer shape</span>
    <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (n_samples, n_obs)</span>
    <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_samples</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># (n_obs, n_samples)</span>
    <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_samples</span><span class="p">)</span>
    <span class="n">dist_samples</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;y_sample&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dist_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_samples</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dist_samples</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.get_params_loss" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that returns the predicted parameters and the loss.</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.get_params_loss--arguments">Arguments</h6>
<p>predt: np.ndarray
    Predicted values.
target: torch.Tensor
    Target values.
start_values: List
    Starting values for each distributional parameter.
requires_grad: bool
    Whether to add to the computational graph or not.</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.get_params_loss--returns">Returns</h6>
<p>predt: torch.Tensor
    Predicted parameters.
loss: torch.Tensor
    Loss value.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_params_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                    <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">start_values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
                    <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that returns the predicted parameters and the loss.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: np.ndarray</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    target: torch.Tensor</span>
<span class="sd">        Target values.</span>
<span class="sd">    start_values: List</span>
<span class="sd">        Starting values for each distributional parameter.</span>
<span class="sd">    requires_grad: bool</span>
<span class="sd">        Whether to add to the computational graph or not.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.Tensor</span>
<span class="sd">        Predicted parameters.</span>
<span class="sd">    loss: torch.Tensor</span>
<span class="sd">        Loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Predicted Parameters</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">predt</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>

    <span class="c1"># Replace NaNs and infinity values with unconditional start values</span>
    <span class="n">nan_inf_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span>
    <span class="n">predt</span><span class="p">[</span><span class="n">nan_inf_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_mask</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hessian_mode</span> <span class="o">==</span> <span class="s2">&quot;grouped&quot;</span><span class="p">:</span>
        <span class="c1"># Convert to torch.Tensor: splits the parameters into tensors for each parameter-type</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Transform parameters to response scale</span>
        <span class="n">predt_transformed</span> <span class="o">=</span> <span class="p">[</span><span class="n">response_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())]</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Convert to torch.Tensor: splits the parameters into tensors for each parameter individually</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Transform parameters to response scale</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">max_index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span>
        <span class="n">index_ranges</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span> <span class="o">&gt;=</span> <span class="n">max_index</span><span class="p">:</span>
                <span class="n">index_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
                <span class="k">break</span>
            <span class="n">index_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">))</span>

        <span class="n">predt_transformed</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="p">[</span><span class="n">key</span><span class="p">](</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">predt</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">index_ranges</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="c1"># Specify Distribution and Loss</span>
    <span class="n">dist_fit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_mixture_distribution</span><span class="p">(</span><span class="n">predt_transformed</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist_fit</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.loss_fn_start_values" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">loss_fn_start_values</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that calculates the loss for a given set of distributional parameters. Only used for calculating
the loss for the start values.</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.loss_fn_start_values--parameter">Parameter</h6>
<p>params: torch.Tensor
    Distributional parameters.
target: torch.Tensor
    Target values.</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.loss_fn_start_values--returns">Returns</h6>
<p>loss: torch.Tensor
    Loss value.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">loss_fn_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                         <span class="n">params</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that calculates the loss for a given set of distributional parameters. Only used for calculating</span>
<span class="sd">    the loss for the start values.</span>

<span class="sd">    Parameter</span>
<span class="sd">    ---------</span>
<span class="sd">    params: torch.Tensor</span>
<span class="sd">        Distributional parameters.</span>
<span class="sd">    target: torch.Tensor</span>
<span class="sd">        Target values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss: torch.Tensor</span>
<span class="sd">        Loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Replace NaNs and infinity values with 0.5</span>
    <span class="n">nan_inf_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_idx</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Transform parameters to response scale</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">response_fn</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())]</span>

    <span class="c1"># Specify Distribution and Loss</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_mixture_distribution</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.metric_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">metric_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that evaluates the predictions using the negative log-likelihood.</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.metric_fn--arguments">Arguments</h6>
<p>predt: np.ndarray
    Predicted values.
data: lgb.Dataset
    Data used for training.</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.metric_fn--returns">Returns</h6>
<p>name: str
    Name of the evaluation metric.
nll: float
    Negative log-likelihood.
is_higher_better: bool
    Whether a higher value of the metric is better or not.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">metric_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that evaluates the predictions using the negative log-likelihood.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: np.ndarray</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    data: lgb.Dataset</span>
<span class="sd">        Data used for training.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    name: str</span>
<span class="sd">        Name of the evaluation metric.</span>
<span class="sd">    nll: float</span>
<span class="sd">        Negative log-likelihood.</span>
<span class="sd">    is_higher_better: bool</span>
<span class="sd">        Whether a higher value of the metric is better or not.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Target</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">n_obs</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Start values (needed to replace NaNs in predt)</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_init_score</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="c1"># Calculate loss</span>
    <span class="n">is_higher_better</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">n_obs</span><span class="p">,</span> <span class="n">is_higher_better</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.objective_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">objective_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function to estimate gradients and hessians of distributional parameters.</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.objective_fn--arguments">Arguments</h6>
<p>predt: np.ndarray
    Predicted values.
data: lgb.DMatrix
    Data used for training.</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.objective_fn--returns">Returns</h6>
<p>grad: np.ndarray
    Gradient.
hess: np.ndarray
    Hessian.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">objective_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function to estimate gradients and hessians of distributional parameters.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: np.ndarray</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    data: lgb.DMatrix</span>
<span class="sd">        Data used for training.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    grad: np.ndarray</span>
<span class="sd">        Gradient.</span>
<span class="sd">    hess: np.ndarray</span>
<span class="sd">        Hessian.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Target</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Weights</span>
    <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Use 1 as weight if no weights are specified</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Start values (needed to replace NaNs in predt)</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_init_score</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="c1"># Calculate gradients and hessians</span>
    <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_gradients_and_hessians</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">predt</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.predict_dist" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">predict_dist</span><span class="p">(</span><span class="n">booster</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">pred_type</span><span class="o">=</span><span class="s1">&#39;parameters&#39;</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">quantiles</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that predicts from the trained model.</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.predict_dist--arguments">Arguments</h6>
<p>booster : lgb.Booster
    Trained model.
data : pd.DataFrame
    Data to predict from.
start_values : np.ndarray.
    Starting values for each distributional parameter.
pred_type : str
    Type of prediction:
    - "samples" draws n_samples from the predicted distribution.
    - "quantiles" calculates the quantiles from the predicted distribution.
    - "parameters" returns the predicted distributional parameters.
n_samples : int
    Number of samples to draw from the predicted distribution.
quantiles : List[float]
    List of quantiles to calculate from the predicted distribution.
seed : int
    Seed for random number generator used to draw samples from the predicted distribution.</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.predict_dist--returns">Returns</h6>
<p>pred : pd.DataFrame
    Predictions.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">predict_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">booster</span><span class="p">:</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Booster</span><span class="p">,</span>
                 <span class="n">data</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                 <span class="n">start_values</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                 <span class="n">pred_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;parameters&quot;</span><span class="p">,</span>
                 <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                 <span class="n">quantiles</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
                 <span class="n">seed</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="mi">123</span>
                 <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that predicts from the trained model.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    booster : lgb.Booster</span>
<span class="sd">        Trained model.</span>
<span class="sd">    data : pd.DataFrame</span>
<span class="sd">        Data to predict from.</span>
<span class="sd">    start_values : np.ndarray.</span>
<span class="sd">        Starting values for each distributional parameter.</span>
<span class="sd">    pred_type : str</span>
<span class="sd">        Type of prediction:</span>
<span class="sd">        - &quot;samples&quot; draws n_samples from the predicted distribution.</span>
<span class="sd">        - &quot;quantiles&quot; calculates the quantiles from the predicted distribution.</span>
<span class="sd">        - &quot;parameters&quot; returns the predicted distributional parameters.</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples to draw from the predicted distribution.</span>
<span class="sd">    quantiles : List[float]</span>
<span class="sd">        List of quantiles to calculate from the predicted distribution.</span>
<span class="sd">    seed : int</span>
<span class="sd">        Seed for random number generator used to draw samples from the predicted distribution.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pred : pd.DataFrame</span>
<span class="sd">        Predictions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
        <span class="n">booster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">raw_score</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>

    <span class="c1"># Set init_score as starting point for each distributional parameter.</span>
    <span class="n">init_score_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="n">start_values</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="p">)</span>

    <span class="c1"># The predictions don&#39;t include the init_score specified in creating the train data.</span>
    <span class="c1"># Hence, it needs to be added manually with the corresponding transform for each distributional parameter.</span>
    <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">predt</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">init_score_pred</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
            <span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">)</span>

    <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">response_fn</span><span class="p">(</span><span class="n">dist_params_predt</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="p">],</span>
        <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_params_predt</span><span class="p">)</span>
    <span class="n">dist_params_predt</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span>

    <span class="c1"># Draw samples from predicted response distribution</span>
    <span class="n">pred_samples_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">predt_params</span><span class="o">=</span><span class="n">dist_params_predt</span><span class="p">,</span>
                                        <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;parameters&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">dist_params_predt</span>

    <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;samples&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pred_samples_df</span>

    <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;quantiles&quot;</span><span class="p">:</span>
        <span class="c1"># Calculate quantiles from predicted response distribution</span>
        <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_samples_df</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">quantiles</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;quant_&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">quantiles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">quantiles</span><span class="p">))]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
            <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pred_quant_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.stabilize_derivative" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s1">&#39;MAD&#39;</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that stabilizes Gradients and Hessians.</p>
<p>As LightGBMLSS updates the parameter estimates by optimizing Gradients and Hessians, it is important
that these are comparable in magnitude for all distributional parameters. Due to imbalances regarding the ranges,
the estimation might become unstable so that it does not converge (or converge very slowly) to the optimal solution.
Another way to improve convergence might be to standardize the response variable. This is especially useful if the
range of the response differs strongly from the range of the Gradients and Hessians. Both, the stabilization and
the standardization of the response are not always advised but need to be carefully considered.
Source: https://github.com/boost-R/gamboostLSS/blob/7792951d2984f289ed7e530befa42a2a4cb04d1d/R/helpers.R#L173</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.stabilize_derivative--parameters">Parameters</h6>
<p>input_der : torch.Tensor
    Input derivative, either Gradient or Hessian.
type: str
    Stabilization method. Can be either "None", "MAD" or "L2".</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.MixtureDistributionClass.stabilize_derivative--returns">Returns</h6>
<p>stab_der : torch.Tensor
    Stabilized Gradient or Hessian.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">stabilize_derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_der</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;MAD&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that stabilizes Gradients and Hessians.</span>

<span class="sd">    As LightGBMLSS updates the parameter estimates by optimizing Gradients and Hessians, it is important</span>
<span class="sd">    that these are comparable in magnitude for all distributional parameters. Due to imbalances regarding the ranges,</span>
<span class="sd">    the estimation might become unstable so that it does not converge (or converge very slowly) to the optimal solution.</span>
<span class="sd">    Another way to improve convergence might be to standardize the response variable. This is especially useful if the</span>
<span class="sd">    range of the response differs strongly from the range of the Gradients and Hessians. Both, the stabilization and</span>
<span class="sd">    the standardization of the response are not always advised but need to be carefully considered.</span>
<span class="sd">    Source: https://github.com/boost-R/gamboostLSS/blob/7792951d2984f289ed7e530befa42a2a4cb04d1d/R/helpers.R#L173</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_der : torch.Tensor</span>
<span class="sd">        Input derivative, either Gradient or Hessian.</span>
<span class="sd">    type: str</span>
<span class="sd">        Stabilization method. Can be either &quot;None&quot;, &quot;MAD&quot; or &quot;L2&quot;.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    stab_der : torch.Tensor</span>
<span class="sd">        Stabilized Gradient or Hessian.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;MAD&quot;</span><span class="p">:</span>
        <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span>
                                     <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)),</span>
                                     <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)),</span>
                                     <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">))</span>
                                     <span class="p">)</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">input_der</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
        <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;L2&quot;</span><span class="p">:</span>
        <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span>
                                     <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)),</span>
                                     <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)),</span>
                                     <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">))</span>
                                     <span class="p">)</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
        <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
        <span class="n">stab_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span>
                                    <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)),</span>
                                    <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)),</span>
                                    <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">))</span>
                                    <span class="p">)</span>

    <span class="k">return</span> <span class="n">stab_der</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.distributions.mixture_distribution_utils.get_component_distributions" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_component_distributions</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function that returns component distributions for creating a mixing distribution.</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.get_component_distributions--arguments">Arguments</h6>
<p>None</p>
<h6 id="lightgbmlss.distributions.mixture_distribution_utils.get_component_distributions--returns">Returns</h6>
<p>distns: List
    List of all available distributions.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_component_distributions</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that returns component distributions for creating a mixing distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    None</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    distns: List</span>
<span class="sd">        List of all available distributions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Get all distribution names</span>
    <span class="n">mixture_distns</span> <span class="o">=</span> <span class="p">[</span><span class="n">dist</span> <span class="k">for</span> <span class="n">dist</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">distributions</span><span class="p">)</span> <span class="k">if</span> <span class="n">dist</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">isupper</span><span class="p">()]</span>

    <span class="c1"># Remove specific distributions</span>
    <span class="n">distns_remove</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;Expectile&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Mixture&quot;</span><span class="p">,</span>
        <span class="s2">&quot;SplineFlow&quot;</span>
    <span class="p">]</span>

    <span class="n">mixture_distns</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">mixture_distns</span> <span class="k">if</span> <span class="n">item</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">distns_remove</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">mixture_distns</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="lightgbmlss.distributions.zero_inflated" class="doc doc-heading">
            <code>zero_inflated</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.zero_inflated.ZeroAdjustedBeta" class="doc doc-heading">
            <code>ZeroAdjustedBeta</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="ZeroInflatedDistribution (lightgbmlss.distributions.zero_inflated.ZeroInflatedDistribution)" href="#lightgbmlss.distributions.zero_inflated.ZeroInflatedDistribution">ZeroInflatedDistribution</a></code></p>



        <p>A Zero-Adjusted Beta distribution.</p>
<h6 id="lightgbmlss.distributions.zero_inflated.ZeroAdjustedBeta--parameter">Parameter</h6>
<p>concentration1: torch.Tensor
    1st concentration parameter of the distribution (often referred to as alpha).
concentration0: torch.Tensor
    2nd concentration parameter of the distribution (often referred to as beta).
gate: torch.Tensor
    Probability of zeros given via a Bernoulli distribution.</p>
<h6 id="lightgbmlss.distributions.zero_inflated.ZeroAdjustedBeta--source">Source</h6>
<p>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/zero_inflated.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZeroAdjustedBeta</span><span class="p">(</span><span class="n">ZeroInflatedDistribution</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A Zero-Adjusted Beta distribution.</span>

<span class="sd">    Parameter</span>
<span class="sd">    ---------</span>
<span class="sd">    concentration1: torch.Tensor</span>
<span class="sd">        1st concentration parameter of the distribution (often referred to as alpha).</span>
<span class="sd">    concentration0: torch.Tensor</span>
<span class="sd">        2nd concentration parameter of the distribution (often referred to as beta).</span>
<span class="sd">    gate: torch.Tensor</span>
<span class="sd">        Probability of zeros given via a Bernoulli distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    ------</span>
<span class="sd">    https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">arg_constraints</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;concentration1&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">positive</span><span class="p">,</span>
        <span class="s2">&quot;concentration0&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">positive</span><span class="p">,</span>
        <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">unit_interval</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">support</span> <span class="o">=</span> <span class="n">constraints</span><span class="o">.</span><span class="n">unit_interval</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">concentration1</span><span class="p">,</span> <span class="n">concentration0</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">base_dist</span> <span class="o">=</span> <span class="n">Beta</span><span class="p">(</span><span class="n">concentration1</span><span class="o">=</span><span class="n">concentration1</span><span class="p">,</span> <span class="n">concentration0</span><span class="o">=</span><span class="n">concentration0</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">base_dist</span><span class="o">.</span><span class="n">_validate_args</span> <span class="o">=</span> <span class="n">validate_args</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">base_dist</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="n">validate_args</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">concentration1</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">concentration1</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">concentration0</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">concentration0</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.zero_inflated.ZeroAdjustedGamma" class="doc doc-heading">
            <code>ZeroAdjustedGamma</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="ZeroInflatedDistribution (lightgbmlss.distributions.zero_inflated.ZeroInflatedDistribution)" href="#lightgbmlss.distributions.zero_inflated.ZeroInflatedDistribution">ZeroInflatedDistribution</a></code></p>



        <p>A Zero-Adjusted Gamma distribution.</p>
<h6 id="lightgbmlss.distributions.zero_inflated.ZeroAdjustedGamma--parameter">Parameter</h6>
<p>concentration: torch.Tensor
    shape parameter of the distribution (often referred to as alpha)
rate: torch.Tensor
    rate = 1 / scale of the distribution (often referred to as beta)
gate: torch.Tensor
    Probability of zeros given via a Bernoulli distribution.</p>
<h6 id="lightgbmlss.distributions.zero_inflated.ZeroAdjustedGamma--source">Source</h6>
<p>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/zero_inflated.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZeroAdjustedGamma</span><span class="p">(</span><span class="n">ZeroInflatedDistribution</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A Zero-Adjusted Gamma distribution.</span>

<span class="sd">    Parameter</span>
<span class="sd">    ---------</span>
<span class="sd">    concentration: torch.Tensor</span>
<span class="sd">        shape parameter of the distribution (often referred to as alpha)</span>
<span class="sd">    rate: torch.Tensor</span>
<span class="sd">        rate = 1 / scale of the distribution (often referred to as beta)</span>
<span class="sd">    gate: torch.Tensor</span>
<span class="sd">        Probability of zeros given via a Bernoulli distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    ------</span>
<span class="sd">    https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">arg_constraints</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;concentration&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">positive</span><span class="p">,</span>
        <span class="s2">&quot;rate&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">positive</span><span class="p">,</span>
        <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">unit_interval</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">support</span> <span class="o">=</span> <span class="n">constraints</span><span class="o">.</span><span class="n">nonnegative</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">concentration</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">base_dist</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="n">concentration</span><span class="o">=</span><span class="n">concentration</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="n">rate</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">base_dist</span><span class="o">.</span><span class="n">_validate_args</span> <span class="o">=</span> <span class="n">validate_args</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">base_dist</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="n">validate_args</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">concentration</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">concentration</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">rate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">rate</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.zero_inflated.ZeroAdjustedLogNormal" class="doc doc-heading">
            <code>ZeroAdjustedLogNormal</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="ZeroInflatedDistribution (lightgbmlss.distributions.zero_inflated.ZeroInflatedDistribution)" href="#lightgbmlss.distributions.zero_inflated.ZeroInflatedDistribution">ZeroInflatedDistribution</a></code></p>



        <p>A Zero-Adjusted Log-Normal distribution.</p>
<h6 id="lightgbmlss.distributions.zero_inflated.ZeroAdjustedLogNormal--parameter">Parameter</h6>
<p>loc: torch.Tensor
    Mean of log of distribution.
scale: torch.Tensor
    Standard deviation of log of the distribution.
gate: torch.Tensor
    Probability of zeros given via a Bernoulli distribution.</p>
<h6 id="lightgbmlss.distributions.zero_inflated.ZeroAdjustedLogNormal--source">Source</h6>
<p>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/zero_inflated.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZeroAdjustedLogNormal</span><span class="p">(</span><span class="n">ZeroInflatedDistribution</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A Zero-Adjusted Log-Normal distribution.</span>

<span class="sd">    Parameter</span>
<span class="sd">    ---------</span>
<span class="sd">    loc: torch.Tensor</span>
<span class="sd">        Mean of log of distribution.</span>
<span class="sd">    scale: torch.Tensor</span>
<span class="sd">        Standard deviation of log of the distribution.</span>
<span class="sd">    gate: torch.Tensor</span>
<span class="sd">        Probability of zeros given via a Bernoulli distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    ------</span>
<span class="sd">    https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">arg_constraints</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;loc&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">real</span><span class="p">,</span>
        <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">positive</span><span class="p">,</span>
        <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">unit_interval</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">support</span> <span class="o">=</span> <span class="n">constraints</span><span class="o">.</span><span class="n">nonnegative</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">base_dist</span> <span class="o">=</span> <span class="n">LogNormal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">base_dist</span><span class="o">.</span><span class="n">_validate_args</span> <span class="o">=</span> <span class="n">validate_args</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">base_dist</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="n">validate_args</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">loc</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">loc</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">scale</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">scale</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.zero_inflated.ZeroInflatedDistribution" class="doc doc-heading">
            <code>ZeroInflatedDistribution</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pyro.distributions.TorchDistribution">TorchDistribution</span></code></p>



        <p>Generic Zero Inflated distribution.</p>
<p>This can be used directly or can be used as a base class as e.g. for
:class:<code>ZeroInflatedPoisson</code> and :class:<code>ZeroInflatedNegativeBinomial</code>.</p>
<h6 id="lightgbmlss.distributions.zero_inflated.ZeroInflatedDistribution--parameters">Parameters</h6>
<p>gate : torch.Tensor
    Probability of extra zeros given via a Bernoulli distribution.
base_dist : torch.distributions.Distribution
    The base distribution.</p>
<h6 id="lightgbmlss.distributions.zero_inflated.ZeroInflatedDistribution--source">Source</h6>
<p>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L18</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/zero_inflated.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZeroInflatedDistribution</span><span class="p">(</span><span class="n">TorchDistribution</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generic Zero Inflated distribution.</span>

<span class="sd">    This can be used directly or can be used as a base class as e.g. for</span>
<span class="sd">    :class:`ZeroInflatedPoisson` and :class:`ZeroInflatedNegativeBinomial`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    gate : torch.Tensor</span>
<span class="sd">        Probability of extra zeros given via a Bernoulli distribution.</span>
<span class="sd">    base_dist : torch.distributions.Distribution</span>
<span class="sd">        The base distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    ------</span>
<span class="sd">    https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L18</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">arg_constraints</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">unit_interval</span><span class="p">,</span>
        <span class="s2">&quot;gate_logits&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">real</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base_dist</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gate_logits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">gate</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">gate_logits</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Either `gate` or `gate_logits` must be specified, but not both.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">gate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_shape</span> <span class="o">=</span> <span class="n">broadcast_shape</span><span class="p">(</span><span class="n">gate</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">base_dist</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gate</span> <span class="o">=</span> <span class="n">gate</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_shape</span> <span class="o">=</span> <span class="n">broadcast_shape</span><span class="p">(</span><span class="n">gate_logits</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">base_dist</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gate_logits</span> <span class="o">=</span> <span class="n">gate_logits</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">base_dist</span><span class="o">.</span><span class="n">event_shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;ZeroInflatedDistribution expected empty &quot;</span>
                <span class="s2">&quot;base_dist.event_shape but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">base_dist</span><span class="o">.</span><span class="n">event_shape</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span> <span class="o">=</span> <span class="n">base_dist</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span>
        <span class="n">event_shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">()</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">event_shape</span><span class="p">,</span> <span class="n">validate_args</span><span class="p">)</span>

    <span class="nd">@constraints</span><span class="o">.</span><span class="n">dependent_property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">support</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">support</span>

    <span class="nd">@lazy_property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">gate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">logits_to_probs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gate_logits</span><span class="p">)</span>

    <span class="nd">@lazy_property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">gate_logits</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">probs_to_logits</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_args</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_validate_sample</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="n">zero_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">value</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">support</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">support</span>
        <span class="n">epsilon</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">support</span><span class="p">,</span> <span class="s2">&quot;lower_bound&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">is_identically_zero</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">support</span><span class="p">,</span> <span class="s2">&quot;lower_bound&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)):</span>
                <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="n">epsilon</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">support</span><span class="p">,</span> <span class="s2">&quot;upper_bound&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">is_identically_one</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">support</span><span class="p">,</span> <span class="s2">&quot;upper_bound&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">):</span>
                <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">clamp_max</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;gate&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
            <span class="n">gate</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">broadcast_all</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
            <span class="n">log_prob</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">gate</span><span class="p">)</span><span class="o">.</span><span class="n">log1p</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
            <span class="n">log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">zero_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">gate</span> <span class="o">+</span> <span class="n">log_prob</span><span class="o">.</span><span class="n">exp</span><span class="p">())</span><span class="o">.</span><span class="n">log</span><span class="p">(),</span> <span class="n">log_prob</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">gate_logits</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">broadcast_all</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gate_logits</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
            <span class="n">log_prob_minus_log_gate</span> <span class="o">=</span> <span class="o">-</span><span class="n">gate_logits</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
            <span class="n">log_gate</span> <span class="o">=</span> <span class="o">-</span><span class="n">softplus</span><span class="p">(</span><span class="o">-</span><span class="n">gate_logits</span><span class="p">)</span>
            <span class="n">log_prob</span> <span class="o">=</span> <span class="n">log_prob_minus_log_gate</span> <span class="o">+</span> <span class="n">log_gate</span>
            <span class="n">zero_log_prob</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">log_prob_minus_log_gate</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_gate</span>
            <span class="n">log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">zero_idx</span><span class="p">,</span> <span class="n">zero_log_prob</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">log_prob</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">()):</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extended_shape</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
            <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="n">samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">samples</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(()),</span> <span class="n">samples</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">samples</span>

    <span class="nd">@lazy_property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">mean</span>

    <span class="nd">@lazy_property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">variance</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">mean</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">variance</span>
        <span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="o">**</span><span class="mi">2</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">expand</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_shape</span><span class="p">,</span> <span class="n">_instance</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">new</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_checked_instance</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="n">_instance</span><span class="p">)</span>
        <span class="n">batch_shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span>
        <span class="n">gate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span> <span class="k">if</span> <span class="s2">&quot;gate&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">gate_logits</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gate_logits</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span>
            <span class="k">if</span> <span class="s2">&quot;gate_logits&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span>
            <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="n">base_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span>
        <span class="n">ZeroInflatedDistribution</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">new</span><span class="p">,</span> <span class="n">base_dist</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span> <span class="n">gate_logits</span><span class="o">=</span><span class="n">gate_logits</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="n">new</span><span class="o">.</span><span class="n">_validate_args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_args</span>
        <span class="k">return</span> <span class="n">new</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.zero_inflated.ZeroInflatedNegativeBinomial" class="doc doc-heading">
            <code>ZeroInflatedNegativeBinomial</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="ZeroInflatedDistribution (lightgbmlss.distributions.zero_inflated.ZeroInflatedDistribution)" href="#lightgbmlss.distributions.zero_inflated.ZeroInflatedDistribution">ZeroInflatedDistribution</a></code></p>



        <p>A Zero Inflated Negative Binomial distribution.</p>
<h6 id="lightgbmlss.distributions.zero_inflated.ZeroInflatedNegativeBinomial--parameter">Parameter</h6>
<p>total_count: torch.Tensor
    Non-negative number of negative Bernoulli trial.
probs: torch.Tensor
    Event probabilities of success in the half open interval [0, 1).
logits: torch.Tensor
    Event log-odds of success (log(p/(1-p))).
gate: torch.Tensor
    Probability of extra zeros given via a Bernoulli distribution.</p>
<h6 id="lightgbmlss.distributions.zero_inflated.ZeroInflatedNegativeBinomial--source">Source</h6>
<ul>
<li>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L150</li>
</ul>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/zero_inflated.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZeroInflatedNegativeBinomial</span><span class="p">(</span><span class="n">ZeroInflatedDistribution</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A Zero Inflated Negative Binomial distribution.</span>

<span class="sd">    Parameter</span>
<span class="sd">    ---------</span>
<span class="sd">    total_count: torch.Tensor</span>
<span class="sd">        Non-negative number of negative Bernoulli trial.</span>
<span class="sd">    probs: torch.Tensor</span>
<span class="sd">        Event probabilities of success in the half open interval [0, 1).</span>
<span class="sd">    logits: torch.Tensor</span>
<span class="sd">        Event log-odds of success (log(p/(1-p))).</span>
<span class="sd">    gate: torch.Tensor</span>
<span class="sd">        Probability of extra zeros given via a Bernoulli distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    ------</span>
<span class="sd">    - https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L150</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">arg_constraints</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;total_count&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">greater_than_eq</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
        <span class="s2">&quot;probs&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">half_open_interval</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
        <span class="s2">&quot;logits&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">real</span><span class="p">,</span>
        <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">unit_interval</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">support</span> <span class="o">=</span> <span class="n">constraints</span><span class="o">.</span><span class="n">nonnegative_integer</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">total_count</span><span class="p">,</span> <span class="n">probs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">base_dist</span> <span class="o">=</span> <span class="n">NegativeBinomial</span><span class="p">(</span><span class="n">total_count</span><span class="o">=</span><span class="n">total_count</span><span class="p">,</span> <span class="n">probs</span><span class="o">=</span><span class="n">probs</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">base_dist</span><span class="o">.</span><span class="n">_validate_args</span> <span class="o">=</span> <span class="n">validate_args</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">base_dist</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="n">validate_args</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">total_count</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">total_count</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">probs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">probs</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">logits</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">logits</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h4 id="lightgbmlss.distributions.zero_inflated.ZeroInflatedPoisson" class="doc doc-heading">
            <code>ZeroInflatedPoisson</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="ZeroInflatedDistribution (lightgbmlss.distributions.zero_inflated.ZeroInflatedDistribution)" href="#lightgbmlss.distributions.zero_inflated.ZeroInflatedDistribution">ZeroInflatedDistribution</a></code></p>



        <p>A Zero-Inflated Poisson distribution.</p>
<h6 id="lightgbmlss.distributions.zero_inflated.ZeroInflatedPoisson--parameter">Parameter</h6>
<p>rate: torch.Tensor
    The rate of the Poisson distribution.
gate: torch.Tensor
    Probability of extra zeros given via a Bernoulli distribution.</p>
<h6 id="lightgbmlss.distributions.zero_inflated.ZeroInflatedPoisson--source">Source</h6>
<p>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L121</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/distributions/zero_inflated.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZeroInflatedPoisson</span><span class="p">(</span><span class="n">ZeroInflatedDistribution</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A Zero-Inflated Poisson distribution.</span>

<span class="sd">    Parameter</span>
<span class="sd">    ---------</span>
<span class="sd">    rate: torch.Tensor</span>
<span class="sd">        The rate of the Poisson distribution.</span>
<span class="sd">    gate: torch.Tensor</span>
<span class="sd">        Probability of extra zeros given via a Bernoulli distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    ------</span>
<span class="sd">    https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L121</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">arg_constraints</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;rate&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">positive</span><span class="p">,</span>
        <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">unit_interval</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">support</span> <span class="o">=</span> <span class="n">constraints</span><span class="o">.</span><span class="n">nonnegative_integer</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">base_dist</span> <span class="o">=</span> <span class="n">Poisson</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">rate</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">base_dist</span><span class="o">.</span><span class="n">_validate_args</span> <span class="o">=</span> <span class="n">validate_args</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">base_dist</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="n">validate_args</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">rate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">rate</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>




  </div>

    </div>

</div>


  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="lightgbmlss.model" class="doc doc-heading">
            <code>model</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="lightgbmlss.model.LightGBMLSS" class="doc doc-heading">
            <code>LightGBMLSS</code>


</h3>


    <div class="doc doc-contents ">



        <p>LightGBMLSS model class</p>
<h5 id="lightgbmlss.model.LightGBMLSS--parameters">Parameters</h5>
<p>dist : Distribution
    DistributionClass object.
 start_values : np.ndarray
    Starting values for each distributional parameter.</p>







              <details class="quote">
                <summary>Source code in <code>lightgbmlss/model.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">LightGBMLSS</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    LightGBMLSS model class</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    dist : Distribution</span>
<span class="sd">        DistributionClass object.</span>
<span class="sd">     start_values : np.ndarray</span>
<span class="sd">        Starting values for each distributional parameter.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dist</span><span class="p">:</span> <span class="n">DistributionClass</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dist</span> <span class="o">=</span> <span class="n">dist</span>             <span class="c1"># Distribution object</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_values</span> <span class="o">=</span> <span class="kc">None</span>     <span class="c1"># Starting values for distributional parameters</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set parameters for distributional model.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        params : Dict[str, Any]</span>
<span class="sd">            Parameters for model.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        params : Dict[str, Any]</span>
<span class="sd">            Updated Parameters for model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">params_adj</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;num_class&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">,</span>
                      <span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;objective&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">objective_fn</span><span class="p">,</span>
                      <span class="s2">&quot;random_seed&quot;</span><span class="p">:</span> <span class="mi">123</span><span class="p">,</span>
                      <span class="s2">&quot;verbose&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span>
                      <span class="p">}</span>
        <span class="n">params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">params_adj</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">params</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">set_init_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dmatrix</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set init_score for distributions.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        dmatrix : Dataset</span>
<span class="sd">            Dataset to set base margin for.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_values</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">initialize</span><span class="p">:</span>
                <span class="n">_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">dmatrix</span><span class="o">.</span><span class="n">get_label</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">start_values</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_dist_param</span>
        <span class="n">init_score</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">dmatrix</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_values</span>
        <span class="n">dmatrix</span><span class="o">.</span><span class="n">set_init_score</span><span class="p">(</span><span class="n">init_score</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
              <span class="n">params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
              <span class="n">train_set</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">,</span>
              <span class="n">num_boost_round</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
              <span class="n">valid_sets</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dataset</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
              <span class="n">valid_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
              <span class="n">init_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Path</span><span class="p">,</span> <span class="n">Booster</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
              <span class="n">keep_training_booster</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
              <span class="n">callbacks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Callable</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
              <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Booster</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Function to perform the training of a LightGBMLSS model with given parameters.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        params : dict</span>
<span class="sd">            Parameters for training. Values passed through ``params`` take precedence over those</span>
<span class="sd">            supplied via arguments.</span>
<span class="sd">        train_set : Dataset</span>
<span class="sd">            Data to be trained on.</span>
<span class="sd">        num_boost_round : int, optional (default=100)</span>
<span class="sd">            Number of boosting iterations.</span>
<span class="sd">        valid_sets : list of Dataset, or None, optional (default=None)</span>
<span class="sd">            List of data to be evaluated on during training.</span>
<span class="sd">        valid_names : list of str, or None, optional (default=None)</span>
<span class="sd">            Names of ``valid_sets``.</span>
<span class="sd">        init_model : str, pathlib.Path, Booster or None, optional (default=None)</span>
<span class="sd">            Filename of LightGBM model or Booster instance used for continue training.</span>
<span class="sd">        keep_training_booster : bool, optional (default=False)</span>
<span class="sd">            Whether the returned Booster will be used to keep training.</span>
<span class="sd">            If False, the returned value will be converted into _InnerPredictor before returning.</span>
<span class="sd">            This means you won&#39;t be able to use ``eval``, ``eval_train`` or ``eval_valid`` methods of the returned Booster.</span>
<span class="sd">            When your model is very large and cause the memory error,</span>
<span class="sd">            you can try to set this param to ``True`` to avoid the model conversion performed during the internal call of ``model_to_string``.</span>
<span class="sd">            You can still use _InnerPredictor as ``init_model`` for future continue training.</span>
<span class="sd">        callbacks : list of callable, or None, optional (default=None)</span>
<span class="sd">            List of callback functions that are applied at each iteration.</span>
<span class="sd">            See Callbacks in Python API for more information.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        booster : Booster</span>
<span class="sd">            The trained Booster model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_init_score</span><span class="p">(</span><span class="n">train_set</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">valid_sets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">valid_sets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_valid_margin</span><span class="p">(</span><span class="n">valid_sets</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">booster</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">params</span><span class="p">,</span>
                                 <span class="n">train_set</span><span class="p">,</span>
                                 <span class="n">num_boost_round</span><span class="o">=</span><span class="n">num_boost_round</span><span class="p">,</span>
                                 <span class="n">feval</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">metric_fn</span><span class="p">,</span>
                                 <span class="n">valid_sets</span><span class="o">=</span><span class="n">valid_sets</span><span class="p">,</span>
                                 <span class="n">valid_names</span><span class="o">=</span><span class="n">valid_names</span><span class="p">,</span>
                                 <span class="n">init_model</span><span class="o">=</span><span class="n">init_model</span><span class="p">,</span>
                                 <span class="n">keep_training_booster</span><span class="o">=</span><span class="n">keep_training_booster</span><span class="p">,</span>
                                 <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">cv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
           <span class="n">params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
           <span class="n">train_set</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">,</span>
           <span class="n">num_boost_round</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
           <span class="n">folds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Iterable</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]],</span> <span class="n">_LGBMBaseCrossValidator</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
           <span class="n">nfold</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
           <span class="n">stratified</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
           <span class="n">shuffle</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
           <span class="n">init_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Path</span><span class="p">,</span> <span class="n">Booster</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
           <span class="n">fpreproc</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_LGBM_PreprocFunction</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
           <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">123</span><span class="p">,</span>
           <span class="n">callbacks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Callable</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
           <span class="n">eval_train_metric</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
           <span class="n">return_cvbooster</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
           <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">CVBooster</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Function to cross-validate a LightGBMLSS model with given parameters.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        params : dict</span>
<span class="sd">            Parameters for training. Values passed through ``params`` take precedence over those</span>
<span class="sd">            supplied via arguments.</span>
<span class="sd">        train_set : Dataset</span>
<span class="sd">            Data to be trained on.</span>
<span class="sd">        num_boost_round : int, optional (default=100)</span>
<span class="sd">            Number of boosting iterations.</span>
<span class="sd">        folds : generator or iterator of (train_idx, test_idx) tuples, scikit-learn splitter object or None, optional (default=None)</span>
<span class="sd">            If generator or iterator, it should yield the train and test indices for each fold.</span>
<span class="sd">            If object, it should be one of the scikit-learn splitter classes</span>
<span class="sd">            (https://scikit-learn.org/stable/modules/classes.html#splitter-classes)</span>
<span class="sd">            and have ``split`` method.</span>
<span class="sd">            This argument has highest priority over other data split arguments.</span>
<span class="sd">        nfold : int, optional (default=5)</span>
<span class="sd">            Number of folds in CV.</span>
<span class="sd">        stratified : bool, optional (default=True)</span>
<span class="sd">            Whether to perform stratified sampling.</span>
<span class="sd">        shuffle : bool, optional (default=True)</span>
<span class="sd">            Whether to shuffle before splitting data.</span>
<span class="sd">        init_model : str, pathlib.Path, Booster or None, optional (default=None)</span>
<span class="sd">            Filename of LightGBM model or Booster instance used for continue training.</span>
<span class="sd">        fpreproc : callable or None, optional (default=None)</span>
<span class="sd">            Preprocessing function that takes (dtrain, dtest, params)</span>
<span class="sd">            and returns transformed versions of those.</span>
<span class="sd">        seed : int, optional (default=0)</span>
<span class="sd">            Seed used to generate the folds (passed to numpy.random.seed).</span>
<span class="sd">        callbacks : list of callable, or None, optional (default=None)</span>
<span class="sd">            List of callback functions that are applied at each iteration.</span>
<span class="sd">            See Callbacks in Python API for more information.</span>
<span class="sd">        eval_train_metric : bool, optional (default=False)</span>
<span class="sd">            Whether to display the train metric in progress.</span>
<span class="sd">            The score of the metric is calculated again after each training step, so there is some impact on performance.</span>
<span class="sd">        return_cvbooster : bool, optional (default=False)</span>
<span class="sd">            Whether to return Booster models trained on each fold through ``CVBooster``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        eval_hist : dict</span>
<span class="sd">            Evaluation history.</span>
<span class="sd">            The dictionary has the following format:</span>
<span class="sd">            {&#39;metric1-mean&#39;: [values], &#39;metric1-stdv&#39;: [values],</span>
<span class="sd">            &#39;metric2-mean&#39;: [values], &#39;metric2-stdv&#39;: [values],</span>
<span class="sd">            ...}.</span>
<span class="sd">            If ``return_cvbooster=True``, also returns trained boosters wrapped in a ``CVBooster`` object via ``cvbooster`` key.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_init_score</span><span class="p">(</span><span class="n">train_set</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bstLSS_cv</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span><span class="n">params</span><span class="p">,</span>
                                <span class="n">train_set</span><span class="p">,</span>
                                <span class="n">feval</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">metric_fn</span><span class="p">,</span>
                                <span class="n">num_boost_round</span><span class="o">=</span><span class="n">num_boost_round</span><span class="p">,</span>
                                <span class="n">folds</span><span class="o">=</span><span class="n">folds</span><span class="p">,</span>
                                <span class="n">nfold</span><span class="o">=</span><span class="n">nfold</span><span class="p">,</span>
                                <span class="n">stratified</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">init_model</span><span class="o">=</span><span class="n">init_model</span><span class="p">,</span>
                                <span class="n">fpreproc</span><span class="o">=</span><span class="n">fpreproc</span><span class="p">,</span>
                                <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
                                <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
                                <span class="n">eval_train_metric</span><span class="o">=</span><span class="n">eval_train_metric</span><span class="p">,</span>
                                <span class="n">return_cvbooster</span><span class="o">=</span><span class="n">return_cvbooster</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">bstLSS_cv</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">hyper_opt</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">hp_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
            <span class="n">train_set</span><span class="p">:</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">,</span>
            <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
            <span class="n">nfold</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
            <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
            <span class="n">max_minutes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
            <span class="n">n_trials</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">study_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">silence</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">hp_seed</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function to tune hyperparameters using optuna.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ----------</span>
<span class="sd">        hp_dict: dict</span>
<span class="sd">            Dictionary of hyperparameters to tune.</span>
<span class="sd">        train_set: lgb.Dataset</span>
<span class="sd">            Training data.</span>
<span class="sd">        num_boost_round: int</span>
<span class="sd">            Number of boosting iterations.</span>
<span class="sd">        nfold: int</span>
<span class="sd">            Number of folds in CV.</span>
<span class="sd">        early_stopping_rounds: int</span>
<span class="sd">            Activates early stopping. Cross-Validation metric (average of validation</span>
<span class="sd">            metric computed over CV folds) needs to improve at least once in</span>
<span class="sd">            every **early_stopping_rounds** round(s) to continue training.</span>
<span class="sd">            The last entry in the evaluation history will represent the best iteration.</span>
<span class="sd">            If there&#39;s more than one metric in the **eval_metric** parameter given in</span>
<span class="sd">            **params**, the last metric will be used for early stopping.</span>
<span class="sd">        max_minutes: int</span>
<span class="sd">            Time budget in minutes, i.e., stop study after the given number of minutes.</span>
<span class="sd">        n_trials: int</span>
<span class="sd">            The number of trials. If this argument is set to None, there is no limitation on the number of trials.</span>
<span class="sd">        study_name: str</span>
<span class="sd">            Name of the hyperparameter study.</span>
<span class="sd">        silence: bool</span>
<span class="sd">            Controls the verbosity of the trail, i.e., user can silence the outputs of the trail.</span>
<span class="sd">        seed: int</span>
<span class="sd">            Seed used to generate the folds (passed to numpy.random.seed).</span>
<span class="sd">        hp_seed: int</span>
<span class="sd">            Seed for random number generator used in the Bayesian hyper-parameter search.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        opt_params : dict</span>
<span class="sd">            Optimal hyper-parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;LightGBMLSS.hyper_opt requires &#39;optuna&#39; and &#39;optuna-integration&#39; &quot;</span>
            <span class="s2">&quot;to be installed. Please install the package to use this feature. &quot;</span>
            <span class="s2">&quot;Installing via pip install lightgbmlss[all_extras] also installs &quot;</span>
            <span class="s2">&quot;the required dependencies.&quot;</span>
        <span class="p">)</span>
        <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;optuna&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

        <span class="kn">import</span><span class="w"> </span><span class="nn">optuna</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">optuna.samplers</span><span class="w"> </span><span class="kn">import</span> <span class="n">TPESampler</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">optuna.integration</span><span class="w"> </span><span class="kn">import</span> <span class="n">LightGBMPruningCallback</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">objective</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

            <span class="n">hyper_params</span> <span class="o">=</span> <span class="p">{}</span>

            <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param_value</span> <span class="ow">in</span> <span class="n">hp_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>

                <span class="n">param_type</span> <span class="o">=</span> <span class="n">param_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

                <span class="k">if</span> <span class="n">param_type</span> <span class="o">==</span> <span class="s2">&quot;categorical&quot;</span> <span class="ow">or</span> <span class="n">param_type</span> <span class="o">==</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
                    <span class="n">hyper_params</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">param_name</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">param_value</span><span class="p">[</span><span class="mi">1</span><span class="p">])})</span>

                <span class="k">elif</span> <span class="n">param_type</span> <span class="o">==</span> <span class="s2">&quot;float&quot;</span><span class="p">:</span>
                    <span class="n">param_constraints</span> <span class="o">=</span> <span class="n">param_value</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                    <span class="n">param_low</span> <span class="o">=</span> <span class="n">param_constraints</span><span class="p">[</span><span class="s2">&quot;low&quot;</span><span class="p">]</span>
                    <span class="n">param_high</span> <span class="o">=</span> <span class="n">param_constraints</span><span class="p">[</span><span class="s2">&quot;high&quot;</span><span class="p">]</span>
                    <span class="n">param_log</span> <span class="o">=</span> <span class="n">param_constraints</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span>
                    <span class="n">hyper_params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                        <span class="p">{</span><span class="n">param_name</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span>
                                                         <span class="n">low</span><span class="o">=</span><span class="n">param_low</span><span class="p">,</span>
                                                         <span class="n">high</span><span class="o">=</span><span class="n">param_high</span><span class="p">,</span>
                                                         <span class="n">log</span><span class="o">=</span><span class="n">param_log</span>
                                                         <span class="p">)</span>
                         <span class="p">})</span>

                <span class="k">elif</span> <span class="n">param_type</span> <span class="o">==</span> <span class="s2">&quot;int&quot;</span><span class="p">:</span>
                    <span class="n">param_constraints</span> <span class="o">=</span> <span class="n">param_value</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                    <span class="n">param_low</span> <span class="o">=</span> <span class="n">param_constraints</span><span class="p">[</span><span class="s2">&quot;low&quot;</span><span class="p">]</span>
                    <span class="n">param_high</span> <span class="o">=</span> <span class="n">param_constraints</span><span class="p">[</span><span class="s2">&quot;high&quot;</span><span class="p">]</span>
                    <span class="n">param_log</span> <span class="o">=</span> <span class="n">param_constraints</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span>
                    <span class="n">hyper_params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                        <span class="p">{</span><span class="n">param_name</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span>
                                                       <span class="n">low</span><span class="o">=</span><span class="n">param_low</span><span class="p">,</span>
                                                       <span class="n">high</span><span class="o">=</span><span class="n">param_high</span><span class="p">,</span>
                                                       <span class="n">log</span><span class="o">=</span><span class="n">param_log</span>
                                                       <span class="p">)</span>
                         <span class="p">})</span>

            <span class="c1"># Add booster if not included in dictionary</span>
            <span class="k">if</span> <span class="s2">&quot;boosting&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">hyper_params</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">hyper_params</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;boosting&quot;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s2">&quot;boosting&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;gbdt&quot;</span><span class="p">])})</span>

            <span class="c1"># Add pruning and early stopping</span>
            <span class="n">pruning_callback</span> <span class="o">=</span> <span class="n">LightGBMPruningCallback</span><span class="p">(</span><span class="n">trial</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">)</span>
            <span class="n">early_stopping_callback</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="n">stopping_rounds</span><span class="o">=</span><span class="n">early_stopping_rounds</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="n">lgblss_param_tuning</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span><span class="n">hyper_params</span><span class="p">,</span>
                                          <span class="n">train_set</span><span class="p">,</span>
                                          <span class="n">num_boost_round</span><span class="o">=</span><span class="n">num_boost_round</span><span class="p">,</span>
                                          <span class="n">nfold</span><span class="o">=</span><span class="n">nfold</span><span class="p">,</span>
                                          <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">pruning_callback</span><span class="p">,</span> <span class="n">early_stopping_callback</span><span class="p">],</span>
                                          <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
                                          <span class="p">)</span>

            <span class="c1"># Extract the optimal number of boosting rounds</span>
            <span class="n">opt_rounds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">lgblss_param_tuning</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;valid </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">loss_fn</span><span class="si">}</span><span class="s2">-mean&quot;</span><span class="p">]))</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">trial</span><span class="o">.</span><span class="n">set_user_attr</span><span class="p">(</span><span class="s2">&quot;opt_round&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">opt_rounds</span><span class="p">))</span>

            <span class="c1"># Extract the best score</span>
            <span class="n">best_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">lgblss_param_tuning</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;valid </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">loss_fn</span><span class="si">}</span><span class="s2">-mean&quot;</span><span class="p">]))</span>

            <span class="k">return</span> <span class="n">best_score</span>

        <span class="k">if</span> <span class="n">study_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">study_name</span> <span class="o">=</span> <span class="s2">&quot;LightGBMLSS Hyper-Parameter Optimization&quot;</span>

        <span class="k">if</span> <span class="n">silence</span><span class="p">:</span>
            <span class="n">optuna</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="n">optuna</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">WARNING</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">hp_seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sampler</span> <span class="o">=</span> <span class="n">TPESampler</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">hp_seed</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sampler</span> <span class="o">=</span> <span class="n">TPESampler</span><span class="p">()</span>

        <span class="n">pruner</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">pruners</span><span class="o">.</span><span class="n">MedianPruner</span><span class="p">(</span><span class="n">n_startup_trials</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_warmup_steps</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
        <span class="n">study</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span> <span class="n">pruner</span><span class="o">=</span><span class="n">pruner</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s2">&quot;minimize&quot;</span><span class="p">,</span> <span class="n">study_name</span><span class="o">=</span><span class="n">study_name</span><span class="p">)</span>
        <span class="n">study</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="n">n_trials</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">60</span> <span class="o">*</span> <span class="n">max_minutes</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Hyper-Parameter Optimization successfully finished.&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Number of finished trials: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">study</span><span class="o">.</span><span class="n">trials</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Best trial:&quot;</span><span class="p">)</span>
        <span class="n">opt_param</span> <span class="o">=</span> <span class="n">study</span><span class="o">.</span><span class="n">best_trial</span>

        <span class="c1"># Add optimal stopping round</span>
        <span class="n">opt_param</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;opt_rounds&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">study</span><span class="o">.</span><span class="n">trials_dataframe</span><span class="p">()[</span><span class="s2">&quot;user_attrs_opt_round&quot;</span><span class="p">][</span>
            <span class="n">study</span><span class="o">.</span><span class="n">trials_dataframe</span><span class="p">()[</span><span class="s2">&quot;value&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()]</span>
        <span class="n">opt_param</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;opt_rounds&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">opt_param</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;opt_rounds&quot;</span><span class="p">])</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    Value: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">opt_param</span><span class="o">.</span><span class="n">value</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    Params: &quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">opt_param</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    </span><span class="si">{}</span><span class="s2">: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">opt_param</span><span class="o">.</span><span class="n">params</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">data</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                <span class="n">pred_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;parameters&quot;</span><span class="p">,</span>
                <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                <span class="n">quantiles</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
                <span class="n">seed</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="mi">123</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that predicts from the trained model.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        data : pd.DataFrame</span>
<span class="sd">            Data to predict from.</span>
<span class="sd">        pred_type : str</span>
<span class="sd">            Type of prediction:</span>
<span class="sd">            - &quot;samples&quot; draws n_samples from the predicted distribution.</span>
<span class="sd">            - &quot;quantiles&quot; calculates the quantiles from the predicted distribution.</span>
<span class="sd">            - &quot;parameters&quot; returns the predicted distributional parameters.</span>
<span class="sd">            - &quot;expectiles&quot; returns the predicted expectiles.</span>
<span class="sd">        n_samples : int</span>
<span class="sd">            Number of samples to draw from the predicted distribution.</span>
<span class="sd">        quantiles : List[float]</span>
<span class="sd">            List of quantiles to calculate from the predicted distribution.</span>
<span class="sd">        seed : int</span>
<span class="sd">            Seed for random number generator used to draw samples from the predicted distribution.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        predt_df : pd.DataFrame</span>
<span class="sd">            Predictions.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Predict</span>
        <span class="n">predt_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">predict_dist</span><span class="p">(</span><span class="n">booster</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">booster</span><span class="p">,</span>
                                          <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
                                          <span class="n">start_values</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">start_values</span><span class="p">,</span>
                                          <span class="n">pred_type</span><span class="o">=</span><span class="n">pred_type</span><span class="p">,</span>
                                          <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                          <span class="n">quantiles</span><span class="o">=</span><span class="n">quantiles</span><span class="p">,</span>
                                          <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">predt_df</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="n">X</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
             <span class="n">feature</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span>
             <span class="n">parameter</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;loc&quot;</span><span class="p">,</span>
             <span class="n">max_display</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span>
             <span class="n">plot_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;Partial_Dependence&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        LightGBMLSS SHap plotting function.</span>

<span class="sd">        Arguments:</span>
<span class="sd">        ---------</span>
<span class="sd">        X: pd.DataFrame</span>
<span class="sd">            Train/Test Data</span>
<span class="sd">        feature: str</span>
<span class="sd">            Specifies which feature is to be plotted.</span>
<span class="sd">        parameter: str</span>
<span class="sd">            Specifies which distributional parameter is to be plotted.</span>
<span class="sd">        max_display: int</span>
<span class="sd">            Specifies the maximum number of features to be displayed.</span>
<span class="sd">        plot_type: str</span>
<span class="sd">            Specifies the type of plot:</span>
<span class="sd">                &quot;Partial_Dependence&quot; plots the partial dependence of the parameter on the feature.</span>
<span class="sd">                &quot;Feature_Importance&quot; plots the feature importance of the parameter.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;LightGBMLSS.plot requires &#39;shap&#39; &quot;</span>
            <span class="s2">&quot;to be installed. Please install the package to use this feature. &quot;</span>
            <span class="s2">&quot;Installing via pip install lightgbmlss[all_extras] also installs &quot;</span>
            <span class="s2">&quot;the required dependencies.&quot;</span>
        <span class="p">)</span>
        <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;shap&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

        <span class="kn">import</span><span class="w"> </span><span class="nn">shap</span>

        <span class="n">shap</span><span class="o">.</span><span class="n">initjs</span><span class="p">()</span>
        <span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">booster</span><span class="p">)</span>
        <span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="n">param_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">parameter</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">plot_type</span> <span class="o">==</span> <span class="s2">&quot;Partial_Dependence&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_dist_param</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">][:,</span> <span class="n">param_pos</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">][:,</span> <span class="n">param_pos</span><span class="p">])</span>
        <span class="k">elif</span> <span class="n">plot_type</span> <span class="o">==</span> <span class="s2">&quot;Feature_Importance&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_dist_param</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span> <span class="n">max_display</span><span class="o">=</span><span class="n">max_display</span> <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">max_display</span> <span class="k">else</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span>
                    <span class="n">shap_values</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">param_pos</span><span class="p">],</span> <span class="n">max_display</span><span class="o">=</span><span class="n">max_display</span> <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">max_display</span> <span class="k">else</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">expectile_plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                       <span class="n">X</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                       <span class="n">feature</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span>
                       <span class="n">expectile</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;0.05&quot;</span><span class="p">,</span>
                       <span class="n">plot_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;Partial_Dependence&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        LightGBMLSS function for plotting expectile SHapley values.</span>

<span class="sd">        X: pd.DataFrame</span>
<span class="sd">            Train/Test Data</span>
<span class="sd">        feature: str</span>
<span class="sd">            Specifies which feature to use for plotting Partial_Dependence plot.</span>
<span class="sd">        expectile: str</span>
<span class="sd">            Specifies which expectile to plot.</span>
<span class="sd">        plot_type: str</span>
<span class="sd">            Specifies which SHapley-plot to visualize. Currently, &quot;Partial_Dependence&quot; and &quot;Feature_Importance&quot;</span>
<span class="sd">            are supported.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;LightGBMLSS.expectile_plot requires &#39;shap&#39; &quot;</span>
            <span class="s2">&quot;to be installed. Please install the package to use this feature. &quot;</span>
            <span class="s2">&quot;Installing via pip install lightgbmlss[all_extras] also installs &quot;</span>
            <span class="s2">&quot;the required dependencies.&quot;</span>
        <span class="p">)</span>
        <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;shap&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

        <span class="kn">import</span><span class="w"> </span><span class="nn">shap</span>

        <span class="n">shap</span><span class="o">.</span><span class="n">initjs</span><span class="p">()</span>
        <span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">booster</span><span class="p">)</span>
        <span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="n">expect_pos</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">expectile</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">plot_type</span> <span class="o">==</span> <span class="s2">&quot;Partial_Dependence&quot;</span><span class="p">:</span>
            <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">][:,</span> <span class="n">expect_pos</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">][:,</span> <span class="n">expect_pos</span><span class="p">])</span>
        <span class="k">elif</span> <span class="n">plot_type</span> <span class="o">==</span> <span class="s2">&quot;Feature_Importance&quot;</span><span class="p">:</span>
            <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">expect_pos</span><span class="p">],</span> <span class="n">max_display</span><span class="o">=</span><span class="mi">15</span> <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">15</span> <span class="k">else</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">set_valid_margin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                         <span class="n">valid_sets</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
                         <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that sets the base margin for the validation set.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        valid_sets : list</span>
<span class="sd">            List of tuples containing the evaluation set(s).</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        valid_sets : list</span>
<span class="sd">            List of tuples containing the evaluation set(s).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">valid_set</span> <span class="ow">in</span> <span class="n">valid_sets</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_init_score</span><span class="p">(</span><span class="n">valid_set</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">valid_sets</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">save_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                   <span class="n">model_path</span><span class="p">:</span> <span class="nb">str</span>
                   <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the model to a file.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        model_path : str</span>
<span class="sd">            The path to save the model.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_model</span><span class="p">(</span><span class="n">model_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load the model from a file.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        model_path : str</span>
<span class="sd">            The path to the saved model.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        The loaded model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.model.LightGBMLSS.cv" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">cv</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">train_set</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">folds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nfold</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">stratified</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">init_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fpreproc</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eval_train_metric</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_cvbooster</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function to cross-validate a LightGBMLSS model with given parameters.</p>
<h6 id="lightgbmlss.model.LightGBMLSS.cv--parameters">Parameters</h6>
<p>params : dict
    Parameters for training. Values passed through <code>params</code> take precedence over those
    supplied via arguments.
train_set : Dataset
    Data to be trained on.
num_boost_round : int, optional (default=100)
    Number of boosting iterations.
folds : generator or iterator of (train_idx, test_idx) tuples, scikit-learn splitter object or None, optional (default=None)
    If generator or iterator, it should yield the train and test indices for each fold.
    If object, it should be one of the scikit-learn splitter classes
    (https://scikit-learn.org/stable/modules/classes.html#splitter-classes)
    and have <code>split</code> method.
    This argument has highest priority over other data split arguments.
nfold : int, optional (default=5)
    Number of folds in CV.
stratified : bool, optional (default=True)
    Whether to perform stratified sampling.
shuffle : bool, optional (default=True)
    Whether to shuffle before splitting data.
init_model : str, pathlib.Path, Booster or None, optional (default=None)
    Filename of LightGBM model or Booster instance used for continue training.
fpreproc : callable or None, optional (default=None)
    Preprocessing function that takes (dtrain, dtest, params)
    and returns transformed versions of those.
seed : int, optional (default=0)
    Seed used to generate the folds (passed to numpy.random.seed).
callbacks : list of callable, or None, optional (default=None)
    List of callback functions that are applied at each iteration.
    See Callbacks in Python API for more information.
eval_train_metric : bool, optional (default=False)
    Whether to display the train metric in progress.
    The score of the metric is calculated again after each training step, so there is some impact on performance.
return_cvbooster : bool, optional (default=False)
    Whether to return Booster models trained on each fold through <code>CVBooster</code>.</p>
<h6 id="lightgbmlss.model.LightGBMLSS.cv--returns">Returns</h6>
<p>eval_hist : dict
    Evaluation history.
    The dictionary has the following format:
    {'metric1-mean': [values], 'metric1-stdv': [values],
    'metric2-mean': [values], 'metric2-stdv': [values],
    ...}.
    If <code>return_cvbooster=True</code>, also returns trained boosters wrapped in a <code>CVBooster</code> object via <code>cvbooster</code> key.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">cv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
       <span class="n">params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
       <span class="n">train_set</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">,</span>
       <span class="n">num_boost_round</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
       <span class="n">folds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Iterable</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]],</span> <span class="n">_LGBMBaseCrossValidator</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
       <span class="n">nfold</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
       <span class="n">stratified</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
       <span class="n">shuffle</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
       <span class="n">init_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Path</span><span class="p">,</span> <span class="n">Booster</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
       <span class="n">fpreproc</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_LGBM_PreprocFunction</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
       <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">123</span><span class="p">,</span>
       <span class="n">callbacks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Callable</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
       <span class="n">eval_train_metric</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
       <span class="n">return_cvbooster</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
       <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">CVBooster</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Function to cross-validate a LightGBMLSS model with given parameters.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    params : dict</span>
<span class="sd">        Parameters for training. Values passed through ``params`` take precedence over those</span>
<span class="sd">        supplied via arguments.</span>
<span class="sd">    train_set : Dataset</span>
<span class="sd">        Data to be trained on.</span>
<span class="sd">    num_boost_round : int, optional (default=100)</span>
<span class="sd">        Number of boosting iterations.</span>
<span class="sd">    folds : generator or iterator of (train_idx, test_idx) tuples, scikit-learn splitter object or None, optional (default=None)</span>
<span class="sd">        If generator or iterator, it should yield the train and test indices for each fold.</span>
<span class="sd">        If object, it should be one of the scikit-learn splitter classes</span>
<span class="sd">        (https://scikit-learn.org/stable/modules/classes.html#splitter-classes)</span>
<span class="sd">        and have ``split`` method.</span>
<span class="sd">        This argument has highest priority over other data split arguments.</span>
<span class="sd">    nfold : int, optional (default=5)</span>
<span class="sd">        Number of folds in CV.</span>
<span class="sd">    stratified : bool, optional (default=True)</span>
<span class="sd">        Whether to perform stratified sampling.</span>
<span class="sd">    shuffle : bool, optional (default=True)</span>
<span class="sd">        Whether to shuffle before splitting data.</span>
<span class="sd">    init_model : str, pathlib.Path, Booster or None, optional (default=None)</span>
<span class="sd">        Filename of LightGBM model or Booster instance used for continue training.</span>
<span class="sd">    fpreproc : callable or None, optional (default=None)</span>
<span class="sd">        Preprocessing function that takes (dtrain, dtest, params)</span>
<span class="sd">        and returns transformed versions of those.</span>
<span class="sd">    seed : int, optional (default=0)</span>
<span class="sd">        Seed used to generate the folds (passed to numpy.random.seed).</span>
<span class="sd">    callbacks : list of callable, or None, optional (default=None)</span>
<span class="sd">        List of callback functions that are applied at each iteration.</span>
<span class="sd">        See Callbacks in Python API for more information.</span>
<span class="sd">    eval_train_metric : bool, optional (default=False)</span>
<span class="sd">        Whether to display the train metric in progress.</span>
<span class="sd">        The score of the metric is calculated again after each training step, so there is some impact on performance.</span>
<span class="sd">    return_cvbooster : bool, optional (default=False)</span>
<span class="sd">        Whether to return Booster models trained on each fold through ``CVBooster``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    eval_hist : dict</span>
<span class="sd">        Evaluation history.</span>
<span class="sd">        The dictionary has the following format:</span>
<span class="sd">        {&#39;metric1-mean&#39;: [values], &#39;metric1-stdv&#39;: [values],</span>
<span class="sd">        &#39;metric2-mean&#39;: [values], &#39;metric2-stdv&#39;: [values],</span>
<span class="sd">        ...}.</span>
<span class="sd">        If ``return_cvbooster=True``, also returns trained boosters wrapped in a ``CVBooster`` object via ``cvbooster`` key.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">set_init_score</span><span class="p">(</span><span class="n">train_set</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">bstLSS_cv</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span><span class="n">params</span><span class="p">,</span>
                            <span class="n">train_set</span><span class="p">,</span>
                            <span class="n">feval</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">metric_fn</span><span class="p">,</span>
                            <span class="n">num_boost_round</span><span class="o">=</span><span class="n">num_boost_round</span><span class="p">,</span>
                            <span class="n">folds</span><span class="o">=</span><span class="n">folds</span><span class="p">,</span>
                            <span class="n">nfold</span><span class="o">=</span><span class="n">nfold</span><span class="p">,</span>
                            <span class="n">stratified</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                            <span class="n">init_model</span><span class="o">=</span><span class="n">init_model</span><span class="p">,</span>
                            <span class="n">fpreproc</span><span class="o">=</span><span class="n">fpreproc</span><span class="p">,</span>
                            <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
                            <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
                            <span class="n">eval_train_metric</span><span class="o">=</span><span class="n">eval_train_metric</span><span class="p">,</span>
                            <span class="n">return_cvbooster</span><span class="o">=</span><span class="n">return_cvbooster</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">bstLSS_cv</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.model.LightGBMLSS.expectile_plot" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">expectile_plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">feature</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">expectile</span><span class="o">=</span><span class="s1">&#39;0.05&#39;</span><span class="p">,</span> <span class="n">plot_type</span><span class="o">=</span><span class="s1">&#39;Partial_Dependence&#39;</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>LightGBMLSS function for plotting expectile SHapley values.</p>


<details class="x" open>
  <summary>pd.DataFrame</summary>
  <p>Train/Test Data</p>
</details>        <p>feature: str
    Specifies which feature to use for plotting Partial_Dependence plot.
expectile: str
    Specifies which expectile to plot.
plot_type: str
    Specifies which SHapley-plot to visualize. Currently, "Partial_Dependence" and "Feature_Importance"
    are supported.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">expectile_plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                   <span class="n">X</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                   <span class="n">feature</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span>
                   <span class="n">expectile</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;0.05&quot;</span><span class="p">,</span>
                   <span class="n">plot_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;Partial_Dependence&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    LightGBMLSS function for plotting expectile SHapley values.</span>

<span class="sd">    X: pd.DataFrame</span>
<span class="sd">        Train/Test Data</span>
<span class="sd">    feature: str</span>
<span class="sd">        Specifies which feature to use for plotting Partial_Dependence plot.</span>
<span class="sd">    expectile: str</span>
<span class="sd">        Specifies which expectile to plot.</span>
<span class="sd">    plot_type: str</span>
<span class="sd">        Specifies which SHapley-plot to visualize. Currently, &quot;Partial_Dependence&quot; and &quot;Feature_Importance&quot;</span>
<span class="sd">        are supported.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;LightGBMLSS.expectile_plot requires &#39;shap&#39; &quot;</span>
        <span class="s2">&quot;to be installed. Please install the package to use this feature. &quot;</span>
        <span class="s2">&quot;Installing via pip install lightgbmlss[all_extras] also installs &quot;</span>
        <span class="s2">&quot;the required dependencies.&quot;</span>
    <span class="p">)</span>
    <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;shap&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

    <span class="kn">import</span><span class="w"> </span><span class="nn">shap</span>

    <span class="n">shap</span><span class="o">.</span><span class="n">initjs</span><span class="p">()</span>
    <span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">booster</span><span class="p">)</span>
    <span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="n">expect_pos</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">expectile</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">plot_type</span> <span class="o">==</span> <span class="s2">&quot;Partial_Dependence&quot;</span><span class="p">:</span>
        <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">][:,</span> <span class="n">expect_pos</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">][:,</span> <span class="n">expect_pos</span><span class="p">])</span>
    <span class="k">elif</span> <span class="n">plot_type</span> <span class="o">==</span> <span class="s2">&quot;Feature_Importance&quot;</span><span class="p">:</span>
        <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">expect_pos</span><span class="p">],</span> <span class="n">max_display</span><span class="o">=</span><span class="mi">15</span> <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">15</span> <span class="k">else</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.model.LightGBMLSS.hyper_opt" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">hyper_opt</span><span class="p">(</span><span class="n">hp_dict</span><span class="p">,</span> <span class="n">train_set</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">nfold</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_minutes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">study_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">silence</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hp_seed</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function to tune hyperparameters using optuna.</p>
<h6 id="lightgbmlss.model.LightGBMLSS.hyper_opt--arguments">Arguments</h6>
<p>hp_dict: dict
    Dictionary of hyperparameters to tune.
train_set: lgb.Dataset
    Training data.
num_boost_round: int
    Number of boosting iterations.
nfold: int
    Number of folds in CV.
early_stopping_rounds: int
    Activates early stopping. Cross-Validation metric (average of validation
    metric computed over CV folds) needs to improve at least once in
    every <strong>early_stopping_rounds</strong> round(s) to continue training.
    The last entry in the evaluation history will represent the best iteration.
    If there's more than one metric in the <strong>eval_metric</strong> parameter given in
    <strong>params</strong>, the last metric will be used for early stopping.
max_minutes: int
    Time budget in minutes, i.e., stop study after the given number of minutes.
n_trials: int
    The number of trials. If this argument is set to None, there is no limitation on the number of trials.
study_name: str
    Name of the hyperparameter study.
silence: bool
    Controls the verbosity of the trail, i.e., user can silence the outputs of the trail.
seed: int
    Seed used to generate the folds (passed to numpy.random.seed).
hp_seed: int
    Seed for random number generator used in the Bayesian hyper-parameter search.</p>
<h6 id="lightgbmlss.model.LightGBMLSS.hyper_opt--returns">Returns</h6>
<p>opt_params : dict
    Optimal hyper-parameters.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">hyper_opt</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hp_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
        <span class="n">train_set</span><span class="p">:</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">,</span>
        <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
        <span class="n">nfold</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
        <span class="n">max_minutes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">n_trials</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">study_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">silence</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hp_seed</span><span class="o">=</span><span class="kc">None</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function to tune hyperparameters using optuna.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ----------</span>
<span class="sd">    hp_dict: dict</span>
<span class="sd">        Dictionary of hyperparameters to tune.</span>
<span class="sd">    train_set: lgb.Dataset</span>
<span class="sd">        Training data.</span>
<span class="sd">    num_boost_round: int</span>
<span class="sd">        Number of boosting iterations.</span>
<span class="sd">    nfold: int</span>
<span class="sd">        Number of folds in CV.</span>
<span class="sd">    early_stopping_rounds: int</span>
<span class="sd">        Activates early stopping. Cross-Validation metric (average of validation</span>
<span class="sd">        metric computed over CV folds) needs to improve at least once in</span>
<span class="sd">        every **early_stopping_rounds** round(s) to continue training.</span>
<span class="sd">        The last entry in the evaluation history will represent the best iteration.</span>
<span class="sd">        If there&#39;s more than one metric in the **eval_metric** parameter given in</span>
<span class="sd">        **params**, the last metric will be used for early stopping.</span>
<span class="sd">    max_minutes: int</span>
<span class="sd">        Time budget in minutes, i.e., stop study after the given number of minutes.</span>
<span class="sd">    n_trials: int</span>
<span class="sd">        The number of trials. If this argument is set to None, there is no limitation on the number of trials.</span>
<span class="sd">    study_name: str</span>
<span class="sd">        Name of the hyperparameter study.</span>
<span class="sd">    silence: bool</span>
<span class="sd">        Controls the verbosity of the trail, i.e., user can silence the outputs of the trail.</span>
<span class="sd">    seed: int</span>
<span class="sd">        Seed used to generate the folds (passed to numpy.random.seed).</span>
<span class="sd">    hp_seed: int</span>
<span class="sd">        Seed for random number generator used in the Bayesian hyper-parameter search.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    opt_params : dict</span>
<span class="sd">        Optimal hyper-parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;LightGBMLSS.hyper_opt requires &#39;optuna&#39; and &#39;optuna-integration&#39; &quot;</span>
        <span class="s2">&quot;to be installed. Please install the package to use this feature. &quot;</span>
        <span class="s2">&quot;Installing via pip install lightgbmlss[all_extras] also installs &quot;</span>
        <span class="s2">&quot;the required dependencies.&quot;</span>
    <span class="p">)</span>
    <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;optuna&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

    <span class="kn">import</span><span class="w"> </span><span class="nn">optuna</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">optuna.samplers</span><span class="w"> </span><span class="kn">import</span> <span class="n">TPESampler</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">optuna.integration</span><span class="w"> </span><span class="kn">import</span> <span class="n">LightGBMPruningCallback</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">objective</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

        <span class="n">hyper_params</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param_value</span> <span class="ow">in</span> <span class="n">hp_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>

            <span class="n">param_type</span> <span class="o">=</span> <span class="n">param_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">param_type</span> <span class="o">==</span> <span class="s2">&quot;categorical&quot;</span> <span class="ow">or</span> <span class="n">param_type</span> <span class="o">==</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
                <span class="n">hyper_params</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">param_name</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">param_value</span><span class="p">[</span><span class="mi">1</span><span class="p">])})</span>

            <span class="k">elif</span> <span class="n">param_type</span> <span class="o">==</span> <span class="s2">&quot;float&quot;</span><span class="p">:</span>
                <span class="n">param_constraints</span> <span class="o">=</span> <span class="n">param_value</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">param_low</span> <span class="o">=</span> <span class="n">param_constraints</span><span class="p">[</span><span class="s2">&quot;low&quot;</span><span class="p">]</span>
                <span class="n">param_high</span> <span class="o">=</span> <span class="n">param_constraints</span><span class="p">[</span><span class="s2">&quot;high&quot;</span><span class="p">]</span>
                <span class="n">param_log</span> <span class="o">=</span> <span class="n">param_constraints</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span>
                <span class="n">hyper_params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                    <span class="p">{</span><span class="n">param_name</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span>
                                                     <span class="n">low</span><span class="o">=</span><span class="n">param_low</span><span class="p">,</span>
                                                     <span class="n">high</span><span class="o">=</span><span class="n">param_high</span><span class="p">,</span>
                                                     <span class="n">log</span><span class="o">=</span><span class="n">param_log</span>
                                                     <span class="p">)</span>
                     <span class="p">})</span>

            <span class="k">elif</span> <span class="n">param_type</span> <span class="o">==</span> <span class="s2">&quot;int&quot;</span><span class="p">:</span>
                <span class="n">param_constraints</span> <span class="o">=</span> <span class="n">param_value</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">param_low</span> <span class="o">=</span> <span class="n">param_constraints</span><span class="p">[</span><span class="s2">&quot;low&quot;</span><span class="p">]</span>
                <span class="n">param_high</span> <span class="o">=</span> <span class="n">param_constraints</span><span class="p">[</span><span class="s2">&quot;high&quot;</span><span class="p">]</span>
                <span class="n">param_log</span> <span class="o">=</span> <span class="n">param_constraints</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span>
                <span class="n">hyper_params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                    <span class="p">{</span><span class="n">param_name</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span>
                                                   <span class="n">low</span><span class="o">=</span><span class="n">param_low</span><span class="p">,</span>
                                                   <span class="n">high</span><span class="o">=</span><span class="n">param_high</span><span class="p">,</span>
                                                   <span class="n">log</span><span class="o">=</span><span class="n">param_log</span>
                                                   <span class="p">)</span>
                     <span class="p">})</span>

        <span class="c1"># Add booster if not included in dictionary</span>
        <span class="k">if</span> <span class="s2">&quot;boosting&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">hyper_params</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">hyper_params</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;boosting&quot;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s2">&quot;boosting&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;gbdt&quot;</span><span class="p">])})</span>

        <span class="c1"># Add pruning and early stopping</span>
        <span class="n">pruning_callback</span> <span class="o">=</span> <span class="n">LightGBMPruningCallback</span><span class="p">(</span><span class="n">trial</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">)</span>
        <span class="n">early_stopping_callback</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="n">stopping_rounds</span><span class="o">=</span><span class="n">early_stopping_rounds</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">lgblss_param_tuning</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span><span class="n">hyper_params</span><span class="p">,</span>
                                      <span class="n">train_set</span><span class="p">,</span>
                                      <span class="n">num_boost_round</span><span class="o">=</span><span class="n">num_boost_round</span><span class="p">,</span>
                                      <span class="n">nfold</span><span class="o">=</span><span class="n">nfold</span><span class="p">,</span>
                                      <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">pruning_callback</span><span class="p">,</span> <span class="n">early_stopping_callback</span><span class="p">],</span>
                                      <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
                                      <span class="p">)</span>

        <span class="c1"># Extract the optimal number of boosting rounds</span>
        <span class="n">opt_rounds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">lgblss_param_tuning</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;valid </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">loss_fn</span><span class="si">}</span><span class="s2">-mean&quot;</span><span class="p">]))</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">trial</span><span class="o">.</span><span class="n">set_user_attr</span><span class="p">(</span><span class="s2">&quot;opt_round&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">opt_rounds</span><span class="p">))</span>

        <span class="c1"># Extract the best score</span>
        <span class="n">best_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">lgblss_param_tuning</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;valid </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">loss_fn</span><span class="si">}</span><span class="s2">-mean&quot;</span><span class="p">]))</span>

        <span class="k">return</span> <span class="n">best_score</span>

    <span class="k">if</span> <span class="n">study_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">study_name</span> <span class="o">=</span> <span class="s2">&quot;LightGBMLSS Hyper-Parameter Optimization&quot;</span>

    <span class="k">if</span> <span class="n">silence</span><span class="p">:</span>
        <span class="n">optuna</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="n">optuna</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">WARNING</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">hp_seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sampler</span> <span class="o">=</span> <span class="n">TPESampler</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">hp_seed</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sampler</span> <span class="o">=</span> <span class="n">TPESampler</span><span class="p">()</span>

    <span class="n">pruner</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">pruners</span><span class="o">.</span><span class="n">MedianPruner</span><span class="p">(</span><span class="n">n_startup_trials</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_warmup_steps</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">study</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span> <span class="n">pruner</span><span class="o">=</span><span class="n">pruner</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s2">&quot;minimize&quot;</span><span class="p">,</span> <span class="n">study_name</span><span class="o">=</span><span class="n">study_name</span><span class="p">)</span>
    <span class="n">study</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="n">n_trials</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">60</span> <span class="o">*</span> <span class="n">max_minutes</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Hyper-Parameter Optimization successfully finished.&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Number of finished trials: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">study</span><span class="o">.</span><span class="n">trials</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Best trial:&quot;</span><span class="p">)</span>
    <span class="n">opt_param</span> <span class="o">=</span> <span class="n">study</span><span class="o">.</span><span class="n">best_trial</span>

    <span class="c1"># Add optimal stopping round</span>
    <span class="n">opt_param</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;opt_rounds&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">study</span><span class="o">.</span><span class="n">trials_dataframe</span><span class="p">()[</span><span class="s2">&quot;user_attrs_opt_round&quot;</span><span class="p">][</span>
        <span class="n">study</span><span class="o">.</span><span class="n">trials_dataframe</span><span class="p">()[</span><span class="s2">&quot;value&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()]</span>
    <span class="n">opt_param</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;opt_rounds&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">opt_param</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;opt_rounds&quot;</span><span class="p">])</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    Value: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">opt_param</span><span class="o">.</span><span class="n">value</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    Params: &quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">opt_param</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    </span><span class="si">{}</span><span class="s2">: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">opt_param</span><span class="o">.</span><span class="n">params</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.model.LightGBMLSS.load_model" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">load_model</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h4>


    <div class="doc doc-contents ">

        <p>Load the model from a file.</p>
<h6 id="lightgbmlss.model.LightGBMLSS.load_model--parameters">Parameters</h6>
<p>model_path : str
    The path to the saved model.</p>
<h6 id="lightgbmlss.model.LightGBMLSS.load_model--returns">Returns</h6>
<p>The loaded model.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_model</span><span class="p">(</span><span class="n">model_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load the model from a file.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    model_path : str</span>
<span class="sd">        The path to the saved model.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    The loaded model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.model.LightGBMLSS.plot" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">feature</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">parameter</span><span class="o">=</span><span class="s1">&#39;loc&#39;</span><span class="p">,</span> <span class="n">max_display</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">plot_type</span><span class="o">=</span><span class="s1">&#39;Partial_Dependence&#39;</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>LightGBMLSS SHap plotting function.</p>
<h6 id="lightgbmlss.model.LightGBMLSS.plot--arguments">Arguments:</h6>
<p>X: pd.DataFrame
    Train/Test Data
feature: str
    Specifies which feature is to be plotted.
parameter: str
    Specifies which distributional parameter is to be plotted.
max_display: int
    Specifies the maximum number of features to be displayed.
plot_type: str
    Specifies the type of plot:
        "Partial_Dependence" plots the partial dependence of the parameter on the feature.
        "Feature_Importance" plots the feature importance of the parameter.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
         <span class="n">X</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
         <span class="n">feature</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span>
         <span class="n">parameter</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;loc&quot;</span><span class="p">,</span>
         <span class="n">max_display</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span>
         <span class="n">plot_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;Partial_Dependence&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    LightGBMLSS SHap plotting function.</span>

<span class="sd">    Arguments:</span>
<span class="sd">    ---------</span>
<span class="sd">    X: pd.DataFrame</span>
<span class="sd">        Train/Test Data</span>
<span class="sd">    feature: str</span>
<span class="sd">        Specifies which feature is to be plotted.</span>
<span class="sd">    parameter: str</span>
<span class="sd">        Specifies which distributional parameter is to be plotted.</span>
<span class="sd">    max_display: int</span>
<span class="sd">        Specifies the maximum number of features to be displayed.</span>
<span class="sd">    plot_type: str</span>
<span class="sd">        Specifies the type of plot:</span>
<span class="sd">            &quot;Partial_Dependence&quot; plots the partial dependence of the parameter on the feature.</span>
<span class="sd">            &quot;Feature_Importance&quot; plots the feature importance of the parameter.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;LightGBMLSS.plot requires &#39;shap&#39; &quot;</span>
        <span class="s2">&quot;to be installed. Please install the package to use this feature. &quot;</span>
        <span class="s2">&quot;Installing via pip install lightgbmlss[all_extras] also installs &quot;</span>
        <span class="s2">&quot;the required dependencies.&quot;</span>
    <span class="p">)</span>
    <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;shap&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

    <span class="kn">import</span><span class="w"> </span><span class="nn">shap</span>

    <span class="n">shap</span><span class="o">.</span><span class="n">initjs</span><span class="p">()</span>
    <span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">booster</span><span class="p">)</span>
    <span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="n">param_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">parameter</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">plot_type</span> <span class="o">==</span> <span class="s2">&quot;Partial_Dependence&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_dist_param</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">][:,</span> <span class="n">param_pos</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">][:,</span> <span class="n">param_pos</span><span class="p">])</span>
    <span class="k">elif</span> <span class="n">plot_type</span> <span class="o">==</span> <span class="s2">&quot;Feature_Importance&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_dist_param</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span> <span class="n">max_display</span><span class="o">=</span><span class="n">max_display</span> <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">max_display</span> <span class="k">else</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span>
                <span class="n">shap_values</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">param_pos</span><span class="p">],</span> <span class="n">max_display</span><span class="o">=</span><span class="n">max_display</span> <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">max_display</span> <span class="k">else</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.model.LightGBMLSS.predict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">pred_type</span><span class="o">=</span><span class="s1">&#39;parameters&#39;</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">quantiles</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function that predicts from the trained model.</p>
<h6 id="lightgbmlss.model.LightGBMLSS.predict--arguments">Arguments</h6>
<p>data : pd.DataFrame
    Data to predict from.
pred_type : str
    Type of prediction:
    - "samples" draws n_samples from the predicted distribution.
    - "quantiles" calculates the quantiles from the predicted distribution.
    - "parameters" returns the predicted distributional parameters.
    - "expectiles" returns the predicted expectiles.
n_samples : int
    Number of samples to draw from the predicted distribution.
quantiles : List[float]
    List of quantiles to calculate from the predicted distribution.
seed : int
    Seed for random number generator used to draw samples from the predicted distribution.</p>
<h6 id="lightgbmlss.model.LightGBMLSS.predict--returns">Returns</h6>
<p>predt_df : pd.DataFrame
    Predictions.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">data</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
            <span class="n">pred_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;parameters&quot;</span><span class="p">,</span>
            <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
            <span class="n">quantiles</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
            <span class="n">seed</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="mi">123</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that predicts from the trained model.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    data : pd.DataFrame</span>
<span class="sd">        Data to predict from.</span>
<span class="sd">    pred_type : str</span>
<span class="sd">        Type of prediction:</span>
<span class="sd">        - &quot;samples&quot; draws n_samples from the predicted distribution.</span>
<span class="sd">        - &quot;quantiles&quot; calculates the quantiles from the predicted distribution.</span>
<span class="sd">        - &quot;parameters&quot; returns the predicted distributional parameters.</span>
<span class="sd">        - &quot;expectiles&quot; returns the predicted expectiles.</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples to draw from the predicted distribution.</span>
<span class="sd">    quantiles : List[float]</span>
<span class="sd">        List of quantiles to calculate from the predicted distribution.</span>
<span class="sd">    seed : int</span>
<span class="sd">        Seed for random number generator used to draw samples from the predicted distribution.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt_df : pd.DataFrame</span>
<span class="sd">        Predictions.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Predict</span>
    <span class="n">predt_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">predict_dist</span><span class="p">(</span><span class="n">booster</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">booster</span><span class="p">,</span>
                                      <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
                                      <span class="n">start_values</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">start_values</span><span class="p">,</span>
                                      <span class="n">pred_type</span><span class="o">=</span><span class="n">pred_type</span><span class="p">,</span>
                                      <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                      <span class="n">quantiles</span><span class="o">=</span><span class="n">quantiles</span><span class="p">,</span>
                                      <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.model.LightGBMLSS.save_model" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">save_model</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Save the model to a file.</p>
<h6 id="lightgbmlss.model.LightGBMLSS.save_model--parameters">Parameters</h6>
<p>model_path : str
    The path to save the model.</p>
<h6 id="lightgbmlss.model.LightGBMLSS.save_model--returns">Returns</h6>
<p>None</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">save_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">model_path</span><span class="p">:</span> <span class="nb">str</span>
               <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the model to a file.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    model_path : str</span>
<span class="sd">        The path to save the model.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.model.LightGBMLSS.set_init_score" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">set_init_score</span><span class="p">(</span><span class="n">dmatrix</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Set init_score for distributions.</p>
<h6 id="lightgbmlss.model.LightGBMLSS.set_init_score--arguments">Arguments</h6>
<p>dmatrix : Dataset
    Dataset to set base margin for.</p>
<h6 id="lightgbmlss.model.LightGBMLSS.set_init_score--returns">Returns</h6>
<p>None</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">set_init_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dmatrix</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set init_score for distributions.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    dmatrix : Dataset</span>
<span class="sd">        Dataset to set base margin for.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_values</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">initialize</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">dmatrix</span><span class="o">.</span><span class="n">get_label</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">start_values</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_dist_param</span>
    <span class="n">init_score</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">dmatrix</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_values</span>
    <span class="n">dmatrix</span><span class="o">.</span><span class="n">set_init_score</span><span class="p">(</span><span class="n">init_score</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="s2">&quot;F&quot;</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.model.LightGBMLSS.set_params" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">set_params</span><span class="p">(</span><span class="n">params</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Set parameters for distributional model.</p>
<h6 id="lightgbmlss.model.LightGBMLSS.set_params--arguments">Arguments</h6>
<p>params : Dict[str, Any]
    Parameters for model.</p>
<h6 id="lightgbmlss.model.LightGBMLSS.set_params--returns">Returns</h6>
<p>params : Dict[str, Any]
    Updated Parameters for model.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set parameters for distributional model.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    params : Dict[str, Any]</span>
<span class="sd">        Parameters for model.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    params : Dict[str, Any]</span>
<span class="sd">        Updated Parameters for model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">params_adj</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;num_class&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">,</span>
                  <span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;objective&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">objective_fn</span><span class="p">,</span>
                  <span class="s2">&quot;random_seed&quot;</span><span class="p">:</span> <span class="mi">123</span><span class="p">,</span>
                  <span class="s2">&quot;verbose&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span>
                  <span class="p">}</span>
    <span class="n">params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">params_adj</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">params</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.model.LightGBMLSS.set_valid_margin" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">set_valid_margin</span><span class="p">(</span><span class="n">valid_sets</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function that sets the base margin for the validation set.</p>
<h6 id="lightgbmlss.model.LightGBMLSS.set_valid_margin--arguments">Arguments</h6>
<p>valid_sets : list
    List of tuples containing the evaluation set(s).</p>
<h6 id="lightgbmlss.model.LightGBMLSS.set_valid_margin--returns">Returns</h6>
<p>valid_sets : list
    List of tuples containing the evaluation set(s).</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">set_valid_margin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">valid_sets</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
                     <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that sets the base margin for the validation set.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    valid_sets : list</span>
<span class="sd">        List of tuples containing the evaluation set(s).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    valid_sets : list</span>
<span class="sd">        List of tuples containing the evaluation set(s).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">valid_set</span> <span class="ow">in</span> <span class="n">valid_sets</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_init_score</span><span class="p">(</span><span class="n">valid_set</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">valid_sets</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="lightgbmlss.model.LightGBMLSS.train" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">train</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">train_set</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">valid_sets</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">valid_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keep_training_booster</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function to perform the training of a LightGBMLSS model with given parameters.</p>
<h6 id="lightgbmlss.model.LightGBMLSS.train--parameters">Parameters</h6>
<p>params : dict
    Parameters for training. Values passed through <code>params</code> take precedence over those
    supplied via arguments.
train_set : Dataset
    Data to be trained on.
num_boost_round : int, optional (default=100)
    Number of boosting iterations.
valid_sets : list of Dataset, or None, optional (default=None)
    List of data to be evaluated on during training.
valid_names : list of str, or None, optional (default=None)
    Names of <code>valid_sets</code>.
init_model : str, pathlib.Path, Booster or None, optional (default=None)
    Filename of LightGBM model or Booster instance used for continue training.
keep_training_booster : bool, optional (default=False)
    Whether the returned Booster will be used to keep training.
    If False, the returned value will be converted into _InnerPredictor before returning.
    This means you won't be able to use <code>eval</code>, <code>eval_train</code> or <code>eval_valid</code> methods of the returned Booster.
    When your model is very large and cause the memory error,
    you can try to set this param to <code>True</code> to avoid the model conversion performed during the internal call of <code>model_to_string</code>.
    You can still use _InnerPredictor as <code>init_model</code> for future continue training.
callbacks : list of callable, or None, optional (default=None)
    List of callback functions that are applied at each iteration.
    See Callbacks in Python API for more information.</p>
<h6 id="lightgbmlss.model.LightGBMLSS.train--returns">Returns</h6>
<p>booster : Booster
    The trained Booster model.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
          <span class="n">params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
          <span class="n">train_set</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">,</span>
          <span class="n">num_boost_round</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
          <span class="n">valid_sets</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dataset</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
          <span class="n">valid_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
          <span class="n">init_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Path</span><span class="p">,</span> <span class="n">Booster</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
          <span class="n">keep_training_booster</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
          <span class="n">callbacks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Callable</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
          <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Booster</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Function to perform the training of a LightGBMLSS model with given parameters.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    params : dict</span>
<span class="sd">        Parameters for training. Values passed through ``params`` take precedence over those</span>
<span class="sd">        supplied via arguments.</span>
<span class="sd">    train_set : Dataset</span>
<span class="sd">        Data to be trained on.</span>
<span class="sd">    num_boost_round : int, optional (default=100)</span>
<span class="sd">        Number of boosting iterations.</span>
<span class="sd">    valid_sets : list of Dataset, or None, optional (default=None)</span>
<span class="sd">        List of data to be evaluated on during training.</span>
<span class="sd">    valid_names : list of str, or None, optional (default=None)</span>
<span class="sd">        Names of ``valid_sets``.</span>
<span class="sd">    init_model : str, pathlib.Path, Booster or None, optional (default=None)</span>
<span class="sd">        Filename of LightGBM model or Booster instance used for continue training.</span>
<span class="sd">    keep_training_booster : bool, optional (default=False)</span>
<span class="sd">        Whether the returned Booster will be used to keep training.</span>
<span class="sd">        If False, the returned value will be converted into _InnerPredictor before returning.</span>
<span class="sd">        This means you won&#39;t be able to use ``eval``, ``eval_train`` or ``eval_valid`` methods of the returned Booster.</span>
<span class="sd">        When your model is very large and cause the memory error,</span>
<span class="sd">        you can try to set this param to ``True`` to avoid the model conversion performed during the internal call of ``model_to_string``.</span>
<span class="sd">        You can still use _InnerPredictor as ``init_model`` for future continue training.</span>
<span class="sd">    callbacks : list of callable, or None, optional (default=None)</span>
<span class="sd">        List of callback functions that are applied at each iteration.</span>
<span class="sd">        See Callbacks in Python API for more information.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    booster : Booster</span>
<span class="sd">        The trained Booster model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">set_init_score</span><span class="p">(</span><span class="n">train_set</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">valid_sets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">valid_sets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_valid_margin</span><span class="p">(</span><span class="n">valid_sets</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">booster</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">params</span><span class="p">,</span>
                             <span class="n">train_set</span><span class="p">,</span>
                             <span class="n">num_boost_round</span><span class="o">=</span><span class="n">num_boost_round</span><span class="p">,</span>
                             <span class="n">feval</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">metric_fn</span><span class="p">,</span>
                             <span class="n">valid_sets</span><span class="o">=</span><span class="n">valid_sets</span><span class="p">,</span>
                             <span class="n">valid_names</span><span class="o">=</span><span class="n">valid_names</span><span class="p">,</span>
                             <span class="n">init_model</span><span class="o">=</span><span class="n">init_model</span><span class="p">,</span>
                             <span class="n">keep_training_booster</span><span class="o">=</span><span class="n">keep_training_booster</span><span class="p">,</span>
                             <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h3 id="lightgbmlss.model.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h5 id="lightgbmlss.model.exp_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="lightgbmlss.model.exp_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="lightgbmlss.model.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h5 id="lightgbmlss.model.exp_fn_df--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="lightgbmlss.model.exp_fn_df--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="lightgbmlss.model.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h5 id="lightgbmlss.model.gumbel_softmax_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h5 id="lightgbmlss.model.gumbel_softmax_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="lightgbmlss.model.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h5 id="lightgbmlss.model.identity_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="lightgbmlss.model.identity_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="lightgbmlss.model.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h5 id="lightgbmlss.model.nan_to_num--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="lightgbmlss.model.nan_to_num--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="lightgbmlss.model.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h5 id="lightgbmlss.model.relu_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="lightgbmlss.model.relu_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="lightgbmlss.model.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h5 id="lightgbmlss.model.sigmoid_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="lightgbmlss.model.sigmoid_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="lightgbmlss.model.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h5 id="lightgbmlss.model.softmax_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="lightgbmlss.model.softmax_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="lightgbmlss.model.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h5 id="lightgbmlss.model.softplus_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="lightgbmlss.model.softplus_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="lightgbmlss.model.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h5 id="lightgbmlss.model.softplus_fn_df--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="lightgbmlss.model.softplus_fn_df--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="lightgbmlss.utils" class="doc doc-heading">
            <code>utils</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="lightgbmlss.utils.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h5 id="lightgbmlss.utils.exp_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="lightgbmlss.utils.exp_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="lightgbmlss.utils.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h5 id="lightgbmlss.utils.exp_fn_df--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="lightgbmlss.utils.exp_fn_df--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="lightgbmlss.utils.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h5 id="lightgbmlss.utils.gumbel_softmax_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h5 id="lightgbmlss.utils.gumbel_softmax_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="lightgbmlss.utils.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h5 id="lightgbmlss.utils.identity_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="lightgbmlss.utils.identity_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="lightgbmlss.utils.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h5 id="lightgbmlss.utils.nan_to_num--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="lightgbmlss.utils.nan_to_num--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="lightgbmlss.utils.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h5 id="lightgbmlss.utils.relu_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="lightgbmlss.utils.relu_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="lightgbmlss.utils.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h5 id="lightgbmlss.utils.sigmoid_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="lightgbmlss.utils.sigmoid_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="lightgbmlss.utils.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h5 id="lightgbmlss.utils.softmax_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="lightgbmlss.utils.softmax_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="lightgbmlss.utils.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h5 id="lightgbmlss.utils.softplus_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="lightgbmlss.utils.softplus_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="lightgbmlss.utils.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h5 id="lightgbmlss.utils.softplus_fn_df--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="lightgbmlss.utils.softplus_fn_df--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>lightgbmlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


  </div>

    </div>

</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../examples/ZAGamma_Regression/" class="btn btn-neutral float-left" title="Zero-Adjusted Gamma Regression"><span class="icon icon-circle-arrow-left"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/StatMixedML/LightGBMLSS" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href="../examples/ZAGamma_Regression/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../javascripts/mathjax.js"></script>
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
